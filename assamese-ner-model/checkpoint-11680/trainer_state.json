{
  "best_global_step": 7008,
  "best_metric": 0.6322787404060364,
  "best_model_checkpoint": "./assamese-ner-model\\checkpoint-7008",
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 11680,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004280821917808219,
      "grad_norm": 9.143527030944824,
      "learning_rate": 2.9976883561643835e-05,
      "loss": 3.2571,
      "step": 10
    },
    {
      "epoch": 0.008561643835616438,
      "grad_norm": 7.429162979125977,
      "learning_rate": 2.9951198630136988e-05,
      "loss": 2.773,
      "step": 20
    },
    {
      "epoch": 0.012842465753424657,
      "grad_norm": 9.300058364868164,
      "learning_rate": 2.9925513698630138e-05,
      "loss": 2.5513,
      "step": 30
    },
    {
      "epoch": 0.017123287671232876,
      "grad_norm": 11.095643043518066,
      "learning_rate": 2.9899828767123288e-05,
      "loss": 2.2438,
      "step": 40
    },
    {
      "epoch": 0.021404109589041095,
      "grad_norm": 5.4018874168396,
      "learning_rate": 2.9874143835616437e-05,
      "loss": 2.0131,
      "step": 50
    },
    {
      "epoch": 0.025684931506849314,
      "grad_norm": 6.561296463012695,
      "learning_rate": 2.984845890410959e-05,
      "loss": 1.9834,
      "step": 60
    },
    {
      "epoch": 0.029965753424657533,
      "grad_norm": 9.898076057434082,
      "learning_rate": 2.982277397260274e-05,
      "loss": 1.8944,
      "step": 70
    },
    {
      "epoch": 0.03424657534246575,
      "grad_norm": 5.873605251312256,
      "learning_rate": 2.979708904109589e-05,
      "loss": 2.1073,
      "step": 80
    },
    {
      "epoch": 0.038527397260273974,
      "grad_norm": 9.62494945526123,
      "learning_rate": 2.977140410958904e-05,
      "loss": 1.642,
      "step": 90
    },
    {
      "epoch": 0.04280821917808219,
      "grad_norm": 7.739095687866211,
      "learning_rate": 2.9745719178082193e-05,
      "loss": 1.8328,
      "step": 100
    },
    {
      "epoch": 0.04708904109589041,
      "grad_norm": 10.027473449707031,
      "learning_rate": 2.9720034246575343e-05,
      "loss": 1.5973,
      "step": 110
    },
    {
      "epoch": 0.05136986301369863,
      "grad_norm": 9.248682975769043,
      "learning_rate": 2.9694349315068496e-05,
      "loss": 1.3256,
      "step": 120
    },
    {
      "epoch": 0.05565068493150685,
      "grad_norm": 11.150430679321289,
      "learning_rate": 2.9668664383561642e-05,
      "loss": 1.8203,
      "step": 130
    },
    {
      "epoch": 0.059931506849315065,
      "grad_norm": 7.573376178741455,
      "learning_rate": 2.9642979452054796e-05,
      "loss": 1.7003,
      "step": 140
    },
    {
      "epoch": 0.0642123287671233,
      "grad_norm": 27.757863998413086,
      "learning_rate": 2.9617294520547945e-05,
      "loss": 1.4981,
      "step": 150
    },
    {
      "epoch": 0.0684931506849315,
      "grad_norm": 3.9192657470703125,
      "learning_rate": 2.95916095890411e-05,
      "loss": 1.3755,
      "step": 160
    },
    {
      "epoch": 0.07277397260273973,
      "grad_norm": 15.962122917175293,
      "learning_rate": 2.9565924657534245e-05,
      "loss": 1.3564,
      "step": 170
    },
    {
      "epoch": 0.07705479452054795,
      "grad_norm": 6.152198791503906,
      "learning_rate": 2.9540239726027398e-05,
      "loss": 1.4281,
      "step": 180
    },
    {
      "epoch": 0.08133561643835617,
      "grad_norm": 12.189838409423828,
      "learning_rate": 2.9514554794520548e-05,
      "loss": 1.7825,
      "step": 190
    },
    {
      "epoch": 0.08561643835616438,
      "grad_norm": 15.862617492675781,
      "learning_rate": 2.94888698630137e-05,
      "loss": 1.4193,
      "step": 200
    },
    {
      "epoch": 0.0898972602739726,
      "grad_norm": 10.695959091186523,
      "learning_rate": 2.946318493150685e-05,
      "loss": 1.3413,
      "step": 210
    },
    {
      "epoch": 0.09417808219178082,
      "grad_norm": 8.591190338134766,
      "learning_rate": 2.94375e-05,
      "loss": 1.4734,
      "step": 220
    },
    {
      "epoch": 0.09845890410958905,
      "grad_norm": 23.265018463134766,
      "learning_rate": 2.941181506849315e-05,
      "loss": 1.3237,
      "step": 230
    },
    {
      "epoch": 0.10273972602739725,
      "grad_norm": 12.896937370300293,
      "learning_rate": 2.9386130136986304e-05,
      "loss": 1.281,
      "step": 240
    },
    {
      "epoch": 0.10702054794520548,
      "grad_norm": 7.430508613586426,
      "learning_rate": 2.9360445205479453e-05,
      "loss": 1.5752,
      "step": 250
    },
    {
      "epoch": 0.1113013698630137,
      "grad_norm": 9.0586519241333,
      "learning_rate": 2.9334760273972607e-05,
      "loss": 1.1468,
      "step": 260
    },
    {
      "epoch": 0.11558219178082192,
      "grad_norm": 9.041784286499023,
      "learning_rate": 2.9309075342465753e-05,
      "loss": 1.3621,
      "step": 270
    },
    {
      "epoch": 0.11986301369863013,
      "grad_norm": 17.757610321044922,
      "learning_rate": 2.9283390410958906e-05,
      "loss": 1.3347,
      "step": 280
    },
    {
      "epoch": 0.12414383561643835,
      "grad_norm": 6.512852668762207,
      "learning_rate": 2.9257705479452056e-05,
      "loss": 1.2847,
      "step": 290
    },
    {
      "epoch": 0.1284246575342466,
      "grad_norm": 11.371417999267578,
      "learning_rate": 2.923202054794521e-05,
      "loss": 1.3513,
      "step": 300
    },
    {
      "epoch": 0.1327054794520548,
      "grad_norm": 10.900997161865234,
      "learning_rate": 2.9206335616438355e-05,
      "loss": 1.3872,
      "step": 310
    },
    {
      "epoch": 0.136986301369863,
      "grad_norm": 25.528583526611328,
      "learning_rate": 2.9180650684931505e-05,
      "loss": 0.9956,
      "step": 320
    },
    {
      "epoch": 0.14126712328767124,
      "grad_norm": 11.505348205566406,
      "learning_rate": 2.915496575342466e-05,
      "loss": 1.3819,
      "step": 330
    },
    {
      "epoch": 0.14554794520547945,
      "grad_norm": 11.744062423706055,
      "learning_rate": 2.9129280821917808e-05,
      "loss": 1.1022,
      "step": 340
    },
    {
      "epoch": 0.14982876712328766,
      "grad_norm": 10.921093940734863,
      "learning_rate": 2.910359589041096e-05,
      "loss": 1.2909,
      "step": 350
    },
    {
      "epoch": 0.1541095890410959,
      "grad_norm": 28.27537727355957,
      "learning_rate": 2.9077910958904108e-05,
      "loss": 1.1084,
      "step": 360
    },
    {
      "epoch": 0.1583904109589041,
      "grad_norm": 14.745160102844238,
      "learning_rate": 2.905222602739726e-05,
      "loss": 1.3607,
      "step": 370
    },
    {
      "epoch": 0.16267123287671234,
      "grad_norm": 9.820703506469727,
      "learning_rate": 2.902654109589041e-05,
      "loss": 1.144,
      "step": 380
    },
    {
      "epoch": 0.16695205479452055,
      "grad_norm": 20.382030487060547,
      "learning_rate": 2.9000856164383564e-05,
      "loss": 1.1846,
      "step": 390
    },
    {
      "epoch": 0.17123287671232876,
      "grad_norm": 11.614411354064941,
      "learning_rate": 2.897517123287671e-05,
      "loss": 1.1864,
      "step": 400
    },
    {
      "epoch": 0.175513698630137,
      "grad_norm": 18.76705551147461,
      "learning_rate": 2.8949486301369863e-05,
      "loss": 1.229,
      "step": 410
    },
    {
      "epoch": 0.1797945205479452,
      "grad_norm": 14.084985733032227,
      "learning_rate": 2.8923801369863013e-05,
      "loss": 0.9564,
      "step": 420
    },
    {
      "epoch": 0.1840753424657534,
      "grad_norm": 15.900201797485352,
      "learning_rate": 2.8898116438356166e-05,
      "loss": 1.0985,
      "step": 430
    },
    {
      "epoch": 0.18835616438356165,
      "grad_norm": 13.179973602294922,
      "learning_rate": 2.8872431506849316e-05,
      "loss": 0.9747,
      "step": 440
    },
    {
      "epoch": 0.19263698630136986,
      "grad_norm": 26.49134635925293,
      "learning_rate": 2.8846746575342466e-05,
      "loss": 1.2553,
      "step": 450
    },
    {
      "epoch": 0.1969178082191781,
      "grad_norm": 20.5519962310791,
      "learning_rate": 2.8821061643835616e-05,
      "loss": 0.8543,
      "step": 460
    },
    {
      "epoch": 0.2011986301369863,
      "grad_norm": 21.370920181274414,
      "learning_rate": 2.879537671232877e-05,
      "loss": 1.1098,
      "step": 470
    },
    {
      "epoch": 0.2054794520547945,
      "grad_norm": 15.452706336975098,
      "learning_rate": 2.876969178082192e-05,
      "loss": 1.1207,
      "step": 480
    },
    {
      "epoch": 0.20976027397260275,
      "grad_norm": 41.02529525756836,
      "learning_rate": 2.874400684931507e-05,
      "loss": 0.8128,
      "step": 490
    },
    {
      "epoch": 0.21404109589041095,
      "grad_norm": 23.58036231994629,
      "learning_rate": 2.8718321917808218e-05,
      "loss": 1.0838,
      "step": 500
    },
    {
      "epoch": 0.2183219178082192,
      "grad_norm": 17.155776977539062,
      "learning_rate": 2.869263698630137e-05,
      "loss": 0.8445,
      "step": 510
    },
    {
      "epoch": 0.2226027397260274,
      "grad_norm": 15.116822242736816,
      "learning_rate": 2.866695205479452e-05,
      "loss": 1.2592,
      "step": 520
    },
    {
      "epoch": 0.2268835616438356,
      "grad_norm": 6.698291301727295,
      "learning_rate": 2.8641267123287674e-05,
      "loss": 1.0454,
      "step": 530
    },
    {
      "epoch": 0.23116438356164384,
      "grad_norm": 27.467140197753906,
      "learning_rate": 2.861558219178082e-05,
      "loss": 0.8242,
      "step": 540
    },
    {
      "epoch": 0.23544520547945205,
      "grad_norm": 16.7839412689209,
      "learning_rate": 2.8589897260273974e-05,
      "loss": 1.0546,
      "step": 550
    },
    {
      "epoch": 0.23972602739726026,
      "grad_norm": 8.238468170166016,
      "learning_rate": 2.8564212328767124e-05,
      "loss": 0.988,
      "step": 560
    },
    {
      "epoch": 0.2440068493150685,
      "grad_norm": 22.661073684692383,
      "learning_rate": 2.8538527397260277e-05,
      "loss": 0.9039,
      "step": 570
    },
    {
      "epoch": 0.2482876712328767,
      "grad_norm": 15.265829086303711,
      "learning_rate": 2.8512842465753427e-05,
      "loss": 1.0021,
      "step": 580
    },
    {
      "epoch": 0.2525684931506849,
      "grad_norm": 49.56181335449219,
      "learning_rate": 2.8487157534246576e-05,
      "loss": 1.146,
      "step": 590
    },
    {
      "epoch": 0.2568493150684932,
      "grad_norm": 14.604925155639648,
      "learning_rate": 2.8461472602739726e-05,
      "loss": 0.9398,
      "step": 600
    },
    {
      "epoch": 0.2611301369863014,
      "grad_norm": 8.990811347961426,
      "learning_rate": 2.843578767123288e-05,
      "loss": 0.7949,
      "step": 610
    },
    {
      "epoch": 0.2654109589041096,
      "grad_norm": 24.059226989746094,
      "learning_rate": 2.841010273972603e-05,
      "loss": 1.1607,
      "step": 620
    },
    {
      "epoch": 0.2696917808219178,
      "grad_norm": 8.618311882019043,
      "learning_rate": 2.838441780821918e-05,
      "loss": 1.0555,
      "step": 630
    },
    {
      "epoch": 0.273972602739726,
      "grad_norm": 9.342193603515625,
      "learning_rate": 2.835873287671233e-05,
      "loss": 1.0055,
      "step": 640
    },
    {
      "epoch": 0.2782534246575342,
      "grad_norm": 11.598215103149414,
      "learning_rate": 2.8333047945205482e-05,
      "loss": 1.1484,
      "step": 650
    },
    {
      "epoch": 0.2825342465753425,
      "grad_norm": 25.86655616760254,
      "learning_rate": 2.8307363013698632e-05,
      "loss": 0.9345,
      "step": 660
    },
    {
      "epoch": 0.2868150684931507,
      "grad_norm": 29.468690872192383,
      "learning_rate": 2.828167808219178e-05,
      "loss": 1.0117,
      "step": 670
    },
    {
      "epoch": 0.2910958904109589,
      "grad_norm": 21.54947280883789,
      "learning_rate": 2.825599315068493e-05,
      "loss": 1.032,
      "step": 680
    },
    {
      "epoch": 0.2953767123287671,
      "grad_norm": 13.336138725280762,
      "learning_rate": 2.823030821917808e-05,
      "loss": 1.0047,
      "step": 690
    },
    {
      "epoch": 0.2996575342465753,
      "grad_norm": 36.05213165283203,
      "learning_rate": 2.8204623287671234e-05,
      "loss": 1.0083,
      "step": 700
    },
    {
      "epoch": 0.3039383561643836,
      "grad_norm": 36.2824821472168,
      "learning_rate": 2.8178938356164384e-05,
      "loss": 0.8792,
      "step": 710
    },
    {
      "epoch": 0.3082191780821918,
      "grad_norm": 15.812623977661133,
      "learning_rate": 2.8153253424657534e-05,
      "loss": 0.9901,
      "step": 720
    },
    {
      "epoch": 0.3125,
      "grad_norm": 13.622113227844238,
      "learning_rate": 2.8127568493150684e-05,
      "loss": 1.08,
      "step": 730
    },
    {
      "epoch": 0.3167808219178082,
      "grad_norm": 19.393569946289062,
      "learning_rate": 2.8101883561643837e-05,
      "loss": 0.8416,
      "step": 740
    },
    {
      "epoch": 0.3210616438356164,
      "grad_norm": 9.216547966003418,
      "learning_rate": 2.8076198630136987e-05,
      "loss": 0.9563,
      "step": 750
    },
    {
      "epoch": 0.3253424657534247,
      "grad_norm": 64.56256103515625,
      "learning_rate": 2.805051369863014e-05,
      "loss": 0.9758,
      "step": 760
    },
    {
      "epoch": 0.3296232876712329,
      "grad_norm": 16.644256591796875,
      "learning_rate": 2.8024828767123286e-05,
      "loss": 0.9189,
      "step": 770
    },
    {
      "epoch": 0.3339041095890411,
      "grad_norm": 22.02782440185547,
      "learning_rate": 2.799914383561644e-05,
      "loss": 1.0309,
      "step": 780
    },
    {
      "epoch": 0.3381849315068493,
      "grad_norm": 14.417555809020996,
      "learning_rate": 2.797345890410959e-05,
      "loss": 1.0168,
      "step": 790
    },
    {
      "epoch": 0.3424657534246575,
      "grad_norm": 17.383155822753906,
      "learning_rate": 2.7947773972602742e-05,
      "loss": 0.8795,
      "step": 800
    },
    {
      "epoch": 0.3467465753424658,
      "grad_norm": 16.478063583374023,
      "learning_rate": 2.792208904109589e-05,
      "loss": 0.9981,
      "step": 810
    },
    {
      "epoch": 0.351027397260274,
      "grad_norm": 46.852333068847656,
      "learning_rate": 2.7896404109589042e-05,
      "loss": 0.8396,
      "step": 820
    },
    {
      "epoch": 0.3553082191780822,
      "grad_norm": 16.336999893188477,
      "learning_rate": 2.787071917808219e-05,
      "loss": 0.8869,
      "step": 830
    },
    {
      "epoch": 0.3595890410958904,
      "grad_norm": 18.43584442138672,
      "learning_rate": 2.7845034246575345e-05,
      "loss": 1.0838,
      "step": 840
    },
    {
      "epoch": 0.3638698630136986,
      "grad_norm": 27.185283660888672,
      "learning_rate": 2.7819349315068495e-05,
      "loss": 0.7127,
      "step": 850
    },
    {
      "epoch": 0.3681506849315068,
      "grad_norm": 9.711454391479492,
      "learning_rate": 2.7793664383561644e-05,
      "loss": 0.6737,
      "step": 860
    },
    {
      "epoch": 0.3724315068493151,
      "grad_norm": 18.34222412109375,
      "learning_rate": 2.7767979452054794e-05,
      "loss": 1.108,
      "step": 870
    },
    {
      "epoch": 0.3767123287671233,
      "grad_norm": 9.437512397766113,
      "learning_rate": 2.7742294520547947e-05,
      "loss": 0.9617,
      "step": 880
    },
    {
      "epoch": 0.3809931506849315,
      "grad_norm": 6.7784905433654785,
      "learning_rate": 2.7716609589041097e-05,
      "loss": 0.894,
      "step": 890
    },
    {
      "epoch": 0.3852739726027397,
      "grad_norm": 14.5828218460083,
      "learning_rate": 2.7690924657534247e-05,
      "loss": 0.8916,
      "step": 900
    },
    {
      "epoch": 0.3895547945205479,
      "grad_norm": 13.952213287353516,
      "learning_rate": 2.7665239726027397e-05,
      "loss": 1.2107,
      "step": 910
    },
    {
      "epoch": 0.3938356164383562,
      "grad_norm": 23.894481658935547,
      "learning_rate": 2.763955479452055e-05,
      "loss": 1.0384,
      "step": 920
    },
    {
      "epoch": 0.3981164383561644,
      "grad_norm": 12.81005573272705,
      "learning_rate": 2.76138698630137e-05,
      "loss": 0.8794,
      "step": 930
    },
    {
      "epoch": 0.4023972602739726,
      "grad_norm": 11.605198860168457,
      "learning_rate": 2.7588184931506853e-05,
      "loss": 1.0815,
      "step": 940
    },
    {
      "epoch": 0.4066780821917808,
      "grad_norm": 19.173080444335938,
      "learning_rate": 2.75625e-05,
      "loss": 0.879,
      "step": 950
    },
    {
      "epoch": 0.410958904109589,
      "grad_norm": 16.12763786315918,
      "learning_rate": 2.7536815068493152e-05,
      "loss": 1.2316,
      "step": 960
    },
    {
      "epoch": 0.4152397260273973,
      "grad_norm": 11.04924488067627,
      "learning_rate": 2.7511130136986302e-05,
      "loss": 1.0372,
      "step": 970
    },
    {
      "epoch": 0.4195205479452055,
      "grad_norm": 12.3121337890625,
      "learning_rate": 2.7485445205479455e-05,
      "loss": 0.8087,
      "step": 980
    },
    {
      "epoch": 0.4238013698630137,
      "grad_norm": 31.46891212463379,
      "learning_rate": 2.7459760273972605e-05,
      "loss": 0.9003,
      "step": 990
    },
    {
      "epoch": 0.4280821917808219,
      "grad_norm": 12.546882629394531,
      "learning_rate": 2.7434075342465755e-05,
      "loss": 1.07,
      "step": 1000
    },
    {
      "epoch": 0.4323630136986301,
      "grad_norm": 8.71003532409668,
      "learning_rate": 2.7408390410958905e-05,
      "loss": 0.9106,
      "step": 1010
    },
    {
      "epoch": 0.4366438356164384,
      "grad_norm": 42.478759765625,
      "learning_rate": 2.7382705479452054e-05,
      "loss": 0.8865,
      "step": 1020
    },
    {
      "epoch": 0.4409246575342466,
      "grad_norm": 15.516157150268555,
      "learning_rate": 2.7357020547945208e-05,
      "loss": 1.078,
      "step": 1030
    },
    {
      "epoch": 0.4452054794520548,
      "grad_norm": 20.25251579284668,
      "learning_rate": 2.7331335616438354e-05,
      "loss": 0.9342,
      "step": 1040
    },
    {
      "epoch": 0.449486301369863,
      "grad_norm": 5.450655937194824,
      "learning_rate": 2.7305650684931507e-05,
      "loss": 0.7869,
      "step": 1050
    },
    {
      "epoch": 0.4537671232876712,
      "grad_norm": 26.11355972290039,
      "learning_rate": 2.7279965753424657e-05,
      "loss": 1.1147,
      "step": 1060
    },
    {
      "epoch": 0.4580479452054795,
      "grad_norm": 11.541396141052246,
      "learning_rate": 2.725428082191781e-05,
      "loss": 0.9773,
      "step": 1070
    },
    {
      "epoch": 0.4623287671232877,
      "grad_norm": 9.537392616271973,
      "learning_rate": 2.722859589041096e-05,
      "loss": 0.6934,
      "step": 1080
    },
    {
      "epoch": 0.4666095890410959,
      "grad_norm": 22.873441696166992,
      "learning_rate": 2.720291095890411e-05,
      "loss": 0.9617,
      "step": 1090
    },
    {
      "epoch": 0.4708904109589041,
      "grad_norm": 10.116670608520508,
      "learning_rate": 2.717722602739726e-05,
      "loss": 0.9711,
      "step": 1100
    },
    {
      "epoch": 0.4751712328767123,
      "grad_norm": 20.031970977783203,
      "learning_rate": 2.7151541095890413e-05,
      "loss": 1.0581,
      "step": 1110
    },
    {
      "epoch": 0.4794520547945205,
      "grad_norm": 19.42740821838379,
      "learning_rate": 2.7125856164383562e-05,
      "loss": 0.8627,
      "step": 1120
    },
    {
      "epoch": 0.4837328767123288,
      "grad_norm": 12.646263122558594,
      "learning_rate": 2.7100171232876712e-05,
      "loss": 0.8751,
      "step": 1130
    },
    {
      "epoch": 0.488013698630137,
      "grad_norm": 4.523874759674072,
      "learning_rate": 2.7074486301369862e-05,
      "loss": 0.8219,
      "step": 1140
    },
    {
      "epoch": 0.4922945205479452,
      "grad_norm": 13.249631881713867,
      "learning_rate": 2.7048801369863015e-05,
      "loss": 0.7833,
      "step": 1150
    },
    {
      "epoch": 0.4965753424657534,
      "grad_norm": 7.592014789581299,
      "learning_rate": 2.7023116438356165e-05,
      "loss": 0.8157,
      "step": 1160
    },
    {
      "epoch": 0.5008561643835616,
      "grad_norm": 34.3780403137207,
      "learning_rate": 2.6997431506849318e-05,
      "loss": 0.9072,
      "step": 1170
    },
    {
      "epoch": 0.5051369863013698,
      "grad_norm": 30.691129684448242,
      "learning_rate": 2.6971746575342464e-05,
      "loss": 0.6879,
      "step": 1180
    },
    {
      "epoch": 0.509417808219178,
      "grad_norm": 6.798182487487793,
      "learning_rate": 2.6946061643835618e-05,
      "loss": 0.8668,
      "step": 1190
    },
    {
      "epoch": 0.5136986301369864,
      "grad_norm": 39.035675048828125,
      "learning_rate": 2.6920376712328767e-05,
      "loss": 0.8607,
      "step": 1200
    },
    {
      "epoch": 0.5179794520547946,
      "grad_norm": 14.542919158935547,
      "learning_rate": 2.689469178082192e-05,
      "loss": 0.811,
      "step": 1210
    },
    {
      "epoch": 0.5222602739726028,
      "grad_norm": 8.363486289978027,
      "learning_rate": 2.6869006849315067e-05,
      "loss": 0.6759,
      "step": 1220
    },
    {
      "epoch": 0.526541095890411,
      "grad_norm": 6.7858123779296875,
      "learning_rate": 2.684332191780822e-05,
      "loss": 0.6917,
      "step": 1230
    },
    {
      "epoch": 0.5308219178082192,
      "grad_norm": 32.5490837097168,
      "learning_rate": 2.681763698630137e-05,
      "loss": 0.9429,
      "step": 1240
    },
    {
      "epoch": 0.5351027397260274,
      "grad_norm": 6.6626458168029785,
      "learning_rate": 2.6791952054794523e-05,
      "loss": 0.8783,
      "step": 1250
    },
    {
      "epoch": 0.5393835616438356,
      "grad_norm": 14.975225448608398,
      "learning_rate": 2.6766267123287673e-05,
      "loss": 0.7047,
      "step": 1260
    },
    {
      "epoch": 0.5436643835616438,
      "grad_norm": 13.297146797180176,
      "learning_rate": 2.6740582191780823e-05,
      "loss": 0.8957,
      "step": 1270
    },
    {
      "epoch": 0.547945205479452,
      "grad_norm": 12.45203685760498,
      "learning_rate": 2.6714897260273972e-05,
      "loss": 0.88,
      "step": 1280
    },
    {
      "epoch": 0.5522260273972602,
      "grad_norm": 4.13766622543335,
      "learning_rate": 2.6689212328767126e-05,
      "loss": 0.6168,
      "step": 1290
    },
    {
      "epoch": 0.5565068493150684,
      "grad_norm": 45.81571578979492,
      "learning_rate": 2.6663527397260275e-05,
      "loss": 1.0284,
      "step": 1300
    },
    {
      "epoch": 0.5607876712328768,
      "grad_norm": 8.625859260559082,
      "learning_rate": 2.663784246575343e-05,
      "loss": 0.5634,
      "step": 1310
    },
    {
      "epoch": 0.565068493150685,
      "grad_norm": 20.825454711914062,
      "learning_rate": 2.6612157534246575e-05,
      "loss": 0.9129,
      "step": 1320
    },
    {
      "epoch": 0.5693493150684932,
      "grad_norm": 12.970701217651367,
      "learning_rate": 2.6586472602739728e-05,
      "loss": 0.686,
      "step": 1330
    },
    {
      "epoch": 0.5736301369863014,
      "grad_norm": 21.17759895324707,
      "learning_rate": 2.6560787671232878e-05,
      "loss": 1.027,
      "step": 1340
    },
    {
      "epoch": 0.5779109589041096,
      "grad_norm": 4.656286716461182,
      "learning_rate": 2.6535102739726028e-05,
      "loss": 0.7063,
      "step": 1350
    },
    {
      "epoch": 0.5821917808219178,
      "grad_norm": 11.171497344970703,
      "learning_rate": 2.6509417808219177e-05,
      "loss": 0.8304,
      "step": 1360
    },
    {
      "epoch": 0.586472602739726,
      "grad_norm": 13.476147651672363,
      "learning_rate": 2.6483732876712327e-05,
      "loss": 0.7401,
      "step": 1370
    },
    {
      "epoch": 0.5907534246575342,
      "grad_norm": 11.683135032653809,
      "learning_rate": 2.645804794520548e-05,
      "loss": 0.8161,
      "step": 1380
    },
    {
      "epoch": 0.5950342465753424,
      "grad_norm": 7.547550678253174,
      "learning_rate": 2.643236301369863e-05,
      "loss": 0.8678,
      "step": 1390
    },
    {
      "epoch": 0.5993150684931506,
      "grad_norm": 4.967058181762695,
      "learning_rate": 2.6406678082191783e-05,
      "loss": 0.5675,
      "step": 1400
    },
    {
      "epoch": 0.603595890410959,
      "grad_norm": 8.383954048156738,
      "learning_rate": 2.638099315068493e-05,
      "loss": 0.7366,
      "step": 1410
    },
    {
      "epoch": 0.6078767123287672,
      "grad_norm": 3.407871723175049,
      "learning_rate": 2.6355308219178083e-05,
      "loss": 0.8091,
      "step": 1420
    },
    {
      "epoch": 0.6121575342465754,
      "grad_norm": 3.6211371421813965,
      "learning_rate": 2.6329623287671233e-05,
      "loss": 0.5425,
      "step": 1430
    },
    {
      "epoch": 0.6164383561643836,
      "grad_norm": 8.027620315551758,
      "learning_rate": 2.6303938356164386e-05,
      "loss": 1.2665,
      "step": 1440
    },
    {
      "epoch": 0.6207191780821918,
      "grad_norm": 10.842984199523926,
      "learning_rate": 2.6278253424657532e-05,
      "loss": 0.8933,
      "step": 1450
    },
    {
      "epoch": 0.625,
      "grad_norm": 17.257038116455078,
      "learning_rate": 2.6252568493150685e-05,
      "loss": 0.9975,
      "step": 1460
    },
    {
      "epoch": 0.6292808219178082,
      "grad_norm": 8.613531112670898,
      "learning_rate": 2.6226883561643835e-05,
      "loss": 0.9868,
      "step": 1470
    },
    {
      "epoch": 0.6335616438356164,
      "grad_norm": 12.613505363464355,
      "learning_rate": 2.620119863013699e-05,
      "loss": 0.8849,
      "step": 1480
    },
    {
      "epoch": 0.6378424657534246,
      "grad_norm": 15.471649169921875,
      "learning_rate": 2.6175513698630138e-05,
      "loss": 0.7888,
      "step": 1490
    },
    {
      "epoch": 0.6421232876712328,
      "grad_norm": 9.704048156738281,
      "learning_rate": 2.6149828767123288e-05,
      "loss": 0.6014,
      "step": 1500
    },
    {
      "epoch": 0.646404109589041,
      "grad_norm": 18.230037689208984,
      "learning_rate": 2.6124143835616438e-05,
      "loss": 0.7973,
      "step": 1510
    },
    {
      "epoch": 0.6506849315068494,
      "grad_norm": 13.136951446533203,
      "learning_rate": 2.609845890410959e-05,
      "loss": 0.54,
      "step": 1520
    },
    {
      "epoch": 0.6549657534246576,
      "grad_norm": 37.58016586303711,
      "learning_rate": 2.607277397260274e-05,
      "loss": 0.9804,
      "step": 1530
    },
    {
      "epoch": 0.6592465753424658,
      "grad_norm": 11.665778160095215,
      "learning_rate": 2.604708904109589e-05,
      "loss": 0.7955,
      "step": 1540
    },
    {
      "epoch": 0.663527397260274,
      "grad_norm": 15.988519668579102,
      "learning_rate": 2.602140410958904e-05,
      "loss": 1.2159,
      "step": 1550
    },
    {
      "epoch": 0.6678082191780822,
      "grad_norm": 12.905730247497559,
      "learning_rate": 2.5995719178082193e-05,
      "loss": 0.9387,
      "step": 1560
    },
    {
      "epoch": 0.6720890410958904,
      "grad_norm": 5.530625343322754,
      "learning_rate": 2.5970034246575343e-05,
      "loss": 0.7292,
      "step": 1570
    },
    {
      "epoch": 0.6763698630136986,
      "grad_norm": 24.208515167236328,
      "learning_rate": 2.5944349315068496e-05,
      "loss": 0.7722,
      "step": 1580
    },
    {
      "epoch": 0.6806506849315068,
      "grad_norm": 14.226933479309082,
      "learning_rate": 2.5918664383561643e-05,
      "loss": 0.7271,
      "step": 1590
    },
    {
      "epoch": 0.684931506849315,
      "grad_norm": 15.042016983032227,
      "learning_rate": 2.5892979452054796e-05,
      "loss": 0.7905,
      "step": 1600
    },
    {
      "epoch": 0.6892123287671232,
      "grad_norm": 35.65140914916992,
      "learning_rate": 2.5867294520547946e-05,
      "loss": 0.5681,
      "step": 1610
    },
    {
      "epoch": 0.6934931506849316,
      "grad_norm": 10.235920906066895,
      "learning_rate": 2.58416095890411e-05,
      "loss": 0.8,
      "step": 1620
    },
    {
      "epoch": 0.6977739726027398,
      "grad_norm": 27.296335220336914,
      "learning_rate": 2.5815924657534245e-05,
      "loss": 0.9603,
      "step": 1630
    },
    {
      "epoch": 0.702054794520548,
      "grad_norm": 25.194801330566406,
      "learning_rate": 2.57902397260274e-05,
      "loss": 0.82,
      "step": 1640
    },
    {
      "epoch": 0.7063356164383562,
      "grad_norm": 36.33634567260742,
      "learning_rate": 2.5764554794520548e-05,
      "loss": 0.7717,
      "step": 1650
    },
    {
      "epoch": 0.7106164383561644,
      "grad_norm": 15.009870529174805,
      "learning_rate": 2.57388698630137e-05,
      "loss": 0.7878,
      "step": 1660
    },
    {
      "epoch": 0.7148972602739726,
      "grad_norm": 34.94595718383789,
      "learning_rate": 2.571318493150685e-05,
      "loss": 0.6897,
      "step": 1670
    },
    {
      "epoch": 0.7191780821917808,
      "grad_norm": 3.3885648250579834,
      "learning_rate": 2.56875e-05,
      "loss": 0.6894,
      "step": 1680
    },
    {
      "epoch": 0.723458904109589,
      "grad_norm": 18.04899024963379,
      "learning_rate": 2.566181506849315e-05,
      "loss": 0.6855,
      "step": 1690
    },
    {
      "epoch": 0.7277397260273972,
      "grad_norm": 23.512062072753906,
      "learning_rate": 2.56361301369863e-05,
      "loss": 0.8258,
      "step": 1700
    },
    {
      "epoch": 0.7320205479452054,
      "grad_norm": 13.760478973388672,
      "learning_rate": 2.5610445205479454e-05,
      "loss": 0.8224,
      "step": 1710
    },
    {
      "epoch": 0.7363013698630136,
      "grad_norm": 17.022178649902344,
      "learning_rate": 2.5584760273972603e-05,
      "loss": 0.8214,
      "step": 1720
    },
    {
      "epoch": 0.740582191780822,
      "grad_norm": 93.08991241455078,
      "learning_rate": 2.5559075342465753e-05,
      "loss": 0.6138,
      "step": 1730
    },
    {
      "epoch": 0.7448630136986302,
      "grad_norm": 12.598793983459473,
      "learning_rate": 2.5533390410958903e-05,
      "loss": 0.6085,
      "step": 1740
    },
    {
      "epoch": 0.7491438356164384,
      "grad_norm": 10.264090538024902,
      "learning_rate": 2.5507705479452056e-05,
      "loss": 0.4778,
      "step": 1750
    },
    {
      "epoch": 0.7534246575342466,
      "grad_norm": 16.948760986328125,
      "learning_rate": 2.5482020547945206e-05,
      "loss": 0.7543,
      "step": 1760
    },
    {
      "epoch": 0.7577054794520548,
      "grad_norm": 16.17952537536621,
      "learning_rate": 2.5456335616438356e-05,
      "loss": 0.676,
      "step": 1770
    },
    {
      "epoch": 0.761986301369863,
      "grad_norm": 16.751449584960938,
      "learning_rate": 2.5430650684931506e-05,
      "loss": 0.6894,
      "step": 1780
    },
    {
      "epoch": 0.7662671232876712,
      "grad_norm": 10.731999397277832,
      "learning_rate": 2.540496575342466e-05,
      "loss": 0.6445,
      "step": 1790
    },
    {
      "epoch": 0.7705479452054794,
      "grad_norm": 5.810342788696289,
      "learning_rate": 2.537928082191781e-05,
      "loss": 0.79,
      "step": 1800
    },
    {
      "epoch": 0.7748287671232876,
      "grad_norm": 23.382064819335938,
      "learning_rate": 2.535359589041096e-05,
      "loss": 0.9898,
      "step": 1810
    },
    {
      "epoch": 0.7791095890410958,
      "grad_norm": 10.761639595031738,
      "learning_rate": 2.5327910958904108e-05,
      "loss": 0.5624,
      "step": 1820
    },
    {
      "epoch": 0.7833904109589042,
      "grad_norm": 13.678400993347168,
      "learning_rate": 2.530222602739726e-05,
      "loss": 0.9474,
      "step": 1830
    },
    {
      "epoch": 0.7876712328767124,
      "grad_norm": 13.515304565429688,
      "learning_rate": 2.527654109589041e-05,
      "loss": 0.7889,
      "step": 1840
    },
    {
      "epoch": 0.7919520547945206,
      "grad_norm": 20.154905319213867,
      "learning_rate": 2.5250856164383564e-05,
      "loss": 0.8551,
      "step": 1850
    },
    {
      "epoch": 0.7962328767123288,
      "grad_norm": 16.845243453979492,
      "learning_rate": 2.522517123287671e-05,
      "loss": 0.7068,
      "step": 1860
    },
    {
      "epoch": 0.800513698630137,
      "grad_norm": 34.205780029296875,
      "learning_rate": 2.5199486301369864e-05,
      "loss": 0.805,
      "step": 1870
    },
    {
      "epoch": 0.8047945205479452,
      "grad_norm": 13.016587257385254,
      "learning_rate": 2.5173801369863014e-05,
      "loss": 0.6715,
      "step": 1880
    },
    {
      "epoch": 0.8090753424657534,
      "grad_norm": 17.59514617919922,
      "learning_rate": 2.5148116438356167e-05,
      "loss": 0.7308,
      "step": 1890
    },
    {
      "epoch": 0.8133561643835616,
      "grad_norm": 21.68423843383789,
      "learning_rate": 2.5122431506849317e-05,
      "loss": 0.9619,
      "step": 1900
    },
    {
      "epoch": 0.8176369863013698,
      "grad_norm": 10.85733413696289,
      "learning_rate": 2.5096746575342466e-05,
      "loss": 0.4836,
      "step": 1910
    },
    {
      "epoch": 0.821917808219178,
      "grad_norm": 17.5113468170166,
      "learning_rate": 2.5071061643835616e-05,
      "loss": 0.7225,
      "step": 1920
    },
    {
      "epoch": 0.8261986301369864,
      "grad_norm": 20.78669548034668,
      "learning_rate": 2.504537671232877e-05,
      "loss": 0.7372,
      "step": 1930
    },
    {
      "epoch": 0.8304794520547946,
      "grad_norm": 12.682047843933105,
      "learning_rate": 2.501969178082192e-05,
      "loss": 0.781,
      "step": 1940
    },
    {
      "epoch": 0.8347602739726028,
      "grad_norm": 18.531436920166016,
      "learning_rate": 2.499400684931507e-05,
      "loss": 0.8725,
      "step": 1950
    },
    {
      "epoch": 0.839041095890411,
      "grad_norm": 33.07869338989258,
      "learning_rate": 2.496832191780822e-05,
      "loss": 0.7145,
      "step": 1960
    },
    {
      "epoch": 0.8433219178082192,
      "grad_norm": 56.62294006347656,
      "learning_rate": 2.4942636986301372e-05,
      "loss": 0.5252,
      "step": 1970
    },
    {
      "epoch": 0.8476027397260274,
      "grad_norm": 64.34467315673828,
      "learning_rate": 2.491695205479452e-05,
      "loss": 0.8005,
      "step": 1980
    },
    {
      "epoch": 0.8518835616438356,
      "grad_norm": 12.431035041809082,
      "learning_rate": 2.4891267123287675e-05,
      "loss": 0.5862,
      "step": 1990
    },
    {
      "epoch": 0.8561643835616438,
      "grad_norm": 31.489168167114258,
      "learning_rate": 2.486558219178082e-05,
      "loss": 0.8181,
      "step": 2000
    },
    {
      "epoch": 0.860445205479452,
      "grad_norm": 20.041730880737305,
      "learning_rate": 2.4839897260273974e-05,
      "loss": 0.6091,
      "step": 2010
    },
    {
      "epoch": 0.8647260273972602,
      "grad_norm": 8.799732208251953,
      "learning_rate": 2.4814212328767124e-05,
      "loss": 0.5877,
      "step": 2020
    },
    {
      "epoch": 0.8690068493150684,
      "grad_norm": 13.108522415161133,
      "learning_rate": 2.4788527397260274e-05,
      "loss": 0.6862,
      "step": 2030
    },
    {
      "epoch": 0.8732876712328768,
      "grad_norm": 21.110349655151367,
      "learning_rate": 2.4762842465753427e-05,
      "loss": 0.6593,
      "step": 2040
    },
    {
      "epoch": 0.877568493150685,
      "grad_norm": 4.349354267120361,
      "learning_rate": 2.4737157534246573e-05,
      "loss": 0.68,
      "step": 2050
    },
    {
      "epoch": 0.8818493150684932,
      "grad_norm": 17.407764434814453,
      "learning_rate": 2.4711472602739727e-05,
      "loss": 0.7587,
      "step": 2060
    },
    {
      "epoch": 0.8861301369863014,
      "grad_norm": 10.936676025390625,
      "learning_rate": 2.4685787671232876e-05,
      "loss": 0.7355,
      "step": 2070
    },
    {
      "epoch": 0.8904109589041096,
      "grad_norm": 36.851112365722656,
      "learning_rate": 2.466010273972603e-05,
      "loss": 0.8058,
      "step": 2080
    },
    {
      "epoch": 0.8946917808219178,
      "grad_norm": 26.359825134277344,
      "learning_rate": 2.4634417808219176e-05,
      "loss": 0.8067,
      "step": 2090
    },
    {
      "epoch": 0.898972602739726,
      "grad_norm": 4.808035373687744,
      "learning_rate": 2.460873287671233e-05,
      "loss": 0.7556,
      "step": 2100
    },
    {
      "epoch": 0.9032534246575342,
      "grad_norm": 29.35092544555664,
      "learning_rate": 2.458304794520548e-05,
      "loss": 0.7164,
      "step": 2110
    },
    {
      "epoch": 0.9075342465753424,
      "grad_norm": 19.436264038085938,
      "learning_rate": 2.4557363013698632e-05,
      "loss": 0.6005,
      "step": 2120
    },
    {
      "epoch": 0.9118150684931506,
      "grad_norm": 12.285465240478516,
      "learning_rate": 2.4531678082191782e-05,
      "loss": 0.5932,
      "step": 2130
    },
    {
      "epoch": 0.916095890410959,
      "grad_norm": 25.76729965209961,
      "learning_rate": 2.450599315068493e-05,
      "loss": 0.6441,
      "step": 2140
    },
    {
      "epoch": 0.9203767123287672,
      "grad_norm": 27.039709091186523,
      "learning_rate": 2.448030821917808e-05,
      "loss": 0.8027,
      "step": 2150
    },
    {
      "epoch": 0.9246575342465754,
      "grad_norm": 28.40961265563965,
      "learning_rate": 2.4454623287671235e-05,
      "loss": 0.5723,
      "step": 2160
    },
    {
      "epoch": 0.9289383561643836,
      "grad_norm": 28.89515495300293,
      "learning_rate": 2.4428938356164384e-05,
      "loss": 0.726,
      "step": 2170
    },
    {
      "epoch": 0.9332191780821918,
      "grad_norm": 15.21774959564209,
      "learning_rate": 2.4403253424657534e-05,
      "loss": 0.5119,
      "step": 2180
    },
    {
      "epoch": 0.9375,
      "grad_norm": 31.885351181030273,
      "learning_rate": 2.4377568493150684e-05,
      "loss": 0.6956,
      "step": 2190
    },
    {
      "epoch": 0.9417808219178082,
      "grad_norm": 41.878997802734375,
      "learning_rate": 2.4351883561643837e-05,
      "loss": 0.4734,
      "step": 2200
    },
    {
      "epoch": 0.9460616438356164,
      "grad_norm": 12.264182090759277,
      "learning_rate": 2.4326198630136987e-05,
      "loss": 0.526,
      "step": 2210
    },
    {
      "epoch": 0.9503424657534246,
      "grad_norm": 6.523041248321533,
      "learning_rate": 2.430051369863014e-05,
      "loss": 0.7543,
      "step": 2220
    },
    {
      "epoch": 0.9546232876712328,
      "grad_norm": 19.857051849365234,
      "learning_rate": 2.4274828767123286e-05,
      "loss": 0.4575,
      "step": 2230
    },
    {
      "epoch": 0.958904109589041,
      "grad_norm": 41.966434478759766,
      "learning_rate": 2.424914383561644e-05,
      "loss": 0.71,
      "step": 2240
    },
    {
      "epoch": 0.9631849315068494,
      "grad_norm": 18.047285079956055,
      "learning_rate": 2.422345890410959e-05,
      "loss": 0.4611,
      "step": 2250
    },
    {
      "epoch": 0.9674657534246576,
      "grad_norm": 32.25387191772461,
      "learning_rate": 2.4197773972602743e-05,
      "loss": 0.6311,
      "step": 2260
    },
    {
      "epoch": 0.9717465753424658,
      "grad_norm": 7.983370780944824,
      "learning_rate": 2.417208904109589e-05,
      "loss": 0.6455,
      "step": 2270
    },
    {
      "epoch": 0.976027397260274,
      "grad_norm": 15.092449188232422,
      "learning_rate": 2.4146404109589042e-05,
      "loss": 0.5441,
      "step": 2280
    },
    {
      "epoch": 0.9803082191780822,
      "grad_norm": 4.433277130126953,
      "learning_rate": 2.4120719178082192e-05,
      "loss": 0.8646,
      "step": 2290
    },
    {
      "epoch": 0.9845890410958904,
      "grad_norm": 2.8443455696105957,
      "learning_rate": 2.4095034246575345e-05,
      "loss": 0.653,
      "step": 2300
    },
    {
      "epoch": 0.9888698630136986,
      "grad_norm": 0.9373922944068909,
      "learning_rate": 2.4069349315068495e-05,
      "loss": 0.5245,
      "step": 2310
    },
    {
      "epoch": 0.9931506849315068,
      "grad_norm": 54.9177131652832,
      "learning_rate": 2.4043664383561645e-05,
      "loss": 0.5716,
      "step": 2320
    },
    {
      "epoch": 0.997431506849315,
      "grad_norm": 17.051746368408203,
      "learning_rate": 2.4017979452054794e-05,
      "loss": 0.5881,
      "step": 2330
    },
    {
      "epoch": 1.0,
      "eval_f1": 0.640155605090126,
      "eval_loss": 0.7885168194770813,
      "eval_precision": 0.65989952624744,
      "eval_recall": 0.6545266692496449,
      "eval_runtime": 596.1184,
      "eval_samples_per_second": 7.499,
      "eval_steps_per_second": 0.938,
      "step": 2336
    },
    {
      "epoch": 1.0017123287671232,
      "grad_norm": 14.434311866760254,
      "learning_rate": 2.3992294520547948e-05,
      "loss": 0.5605,
      "step": 2340
    },
    {
      "epoch": 1.0059931506849316,
      "grad_norm": 1.0917059183120728,
      "learning_rate": 2.3966609589041097e-05,
      "loss": 0.7174,
      "step": 2350
    },
    {
      "epoch": 1.0102739726027397,
      "grad_norm": 10.919930458068848,
      "learning_rate": 2.3940924657534247e-05,
      "loss": 0.5105,
      "step": 2360
    },
    {
      "epoch": 1.014554794520548,
      "grad_norm": 28.403804779052734,
      "learning_rate": 2.3915239726027397e-05,
      "loss": 0.6549,
      "step": 2370
    },
    {
      "epoch": 1.018835616438356,
      "grad_norm": 26.01487922668457,
      "learning_rate": 2.3889554794520547e-05,
      "loss": 0.5837,
      "step": 2380
    },
    {
      "epoch": 1.0231164383561644,
      "grad_norm": 8.452924728393555,
      "learning_rate": 2.38638698630137e-05,
      "loss": 0.4745,
      "step": 2390
    },
    {
      "epoch": 1.0273972602739727,
      "grad_norm": 20.746458053588867,
      "learning_rate": 2.383818493150685e-05,
      "loss": 0.4122,
      "step": 2400
    },
    {
      "epoch": 1.0316780821917808,
      "grad_norm": 25.048707962036133,
      "learning_rate": 2.38125e-05,
      "loss": 0.8255,
      "step": 2410
    },
    {
      "epoch": 1.0359589041095891,
      "grad_norm": 6.413450717926025,
      "learning_rate": 2.378681506849315e-05,
      "loss": 0.5403,
      "step": 2420
    },
    {
      "epoch": 1.0402397260273972,
      "grad_norm": 11.969742774963379,
      "learning_rate": 2.3761130136986302e-05,
      "loss": 0.6837,
      "step": 2430
    },
    {
      "epoch": 1.0445205479452055,
      "grad_norm": 12.856775283813477,
      "learning_rate": 2.3735445205479452e-05,
      "loss": 0.508,
      "step": 2440
    },
    {
      "epoch": 1.0488013698630136,
      "grad_norm": 98.75533294677734,
      "learning_rate": 2.3709760273972605e-05,
      "loss": 0.5482,
      "step": 2450
    },
    {
      "epoch": 1.053082191780822,
      "grad_norm": 14.54837417602539,
      "learning_rate": 2.3684075342465752e-05,
      "loss": 0.543,
      "step": 2460
    },
    {
      "epoch": 1.05736301369863,
      "grad_norm": 13.33887004852295,
      "learning_rate": 2.3658390410958905e-05,
      "loss": 0.5926,
      "step": 2470
    },
    {
      "epoch": 1.0616438356164384,
      "grad_norm": 8.95486831665039,
      "learning_rate": 2.3632705479452055e-05,
      "loss": 0.5339,
      "step": 2480
    },
    {
      "epoch": 1.0659246575342465,
      "grad_norm": 14.323339462280273,
      "learning_rate": 2.3607020547945208e-05,
      "loss": 0.4593,
      "step": 2490
    },
    {
      "epoch": 1.0702054794520548,
      "grad_norm": 15.935379028320312,
      "learning_rate": 2.3581335616438354e-05,
      "loss": 0.5269,
      "step": 2500
    },
    {
      "epoch": 1.0744863013698631,
      "grad_norm": 10.73087215423584,
      "learning_rate": 2.3555650684931507e-05,
      "loss": 0.6115,
      "step": 2510
    },
    {
      "epoch": 1.0787671232876712,
      "grad_norm": 5.429958343505859,
      "learning_rate": 2.3529965753424657e-05,
      "loss": 0.3729,
      "step": 2520
    },
    {
      "epoch": 1.0830479452054795,
      "grad_norm": 21.095849990844727,
      "learning_rate": 2.350428082191781e-05,
      "loss": 0.4417,
      "step": 2530
    },
    {
      "epoch": 1.0873287671232876,
      "grad_norm": 11.610197067260742,
      "learning_rate": 2.347859589041096e-05,
      "loss": 0.7186,
      "step": 2540
    },
    {
      "epoch": 1.091609589041096,
      "grad_norm": 7.565066337585449,
      "learning_rate": 2.345291095890411e-05,
      "loss": 0.5585,
      "step": 2550
    },
    {
      "epoch": 1.095890410958904,
      "grad_norm": 8.311152458190918,
      "learning_rate": 2.342722602739726e-05,
      "loss": 0.6581,
      "step": 2560
    },
    {
      "epoch": 1.1001712328767124,
      "grad_norm": 9.93104076385498,
      "learning_rate": 2.3401541095890413e-05,
      "loss": 0.4014,
      "step": 2570
    },
    {
      "epoch": 1.1044520547945205,
      "grad_norm": 12.65024185180664,
      "learning_rate": 2.3375856164383563e-05,
      "loss": 0.5235,
      "step": 2580
    },
    {
      "epoch": 1.1087328767123288,
      "grad_norm": 60.201515197753906,
      "learning_rate": 2.3350171232876712e-05,
      "loss": 0.91,
      "step": 2590
    },
    {
      "epoch": 1.1130136986301369,
      "grad_norm": 9.550028800964355,
      "learning_rate": 2.3324486301369862e-05,
      "loss": 0.4735,
      "step": 2600
    },
    {
      "epoch": 1.1172945205479452,
      "grad_norm": 9.562759399414062,
      "learning_rate": 2.3298801369863015e-05,
      "loss": 0.4877,
      "step": 2610
    },
    {
      "epoch": 1.1215753424657535,
      "grad_norm": 12.886402130126953,
      "learning_rate": 2.3273116438356165e-05,
      "loss": 0.4259,
      "step": 2620
    },
    {
      "epoch": 1.1258561643835616,
      "grad_norm": 7.7619709968566895,
      "learning_rate": 2.324743150684932e-05,
      "loss": 0.502,
      "step": 2630
    },
    {
      "epoch": 1.13013698630137,
      "grad_norm": 22.827817916870117,
      "learning_rate": 2.3221746575342465e-05,
      "loss": 0.5342,
      "step": 2640
    },
    {
      "epoch": 1.134417808219178,
      "grad_norm": 26.439483642578125,
      "learning_rate": 2.3196061643835618e-05,
      "loss": 0.4635,
      "step": 2650
    },
    {
      "epoch": 1.1386986301369864,
      "grad_norm": 27.487112045288086,
      "learning_rate": 2.3170376712328768e-05,
      "loss": 0.4494,
      "step": 2660
    },
    {
      "epoch": 1.1429794520547945,
      "grad_norm": 8.665365219116211,
      "learning_rate": 2.314469178082192e-05,
      "loss": 0.5481,
      "step": 2670
    },
    {
      "epoch": 1.1472602739726028,
      "grad_norm": 35.2126579284668,
      "learning_rate": 2.3119006849315067e-05,
      "loss": 0.7088,
      "step": 2680
    },
    {
      "epoch": 1.1515410958904109,
      "grad_norm": 19.38114356994629,
      "learning_rate": 2.309332191780822e-05,
      "loss": 0.5704,
      "step": 2690
    },
    {
      "epoch": 1.1558219178082192,
      "grad_norm": 0.7317081093788147,
      "learning_rate": 2.306763698630137e-05,
      "loss": 0.4253,
      "step": 2700
    },
    {
      "epoch": 1.1601027397260273,
      "grad_norm": 8.237403869628906,
      "learning_rate": 2.3041952054794523e-05,
      "loss": 0.4897,
      "step": 2710
    },
    {
      "epoch": 1.1643835616438356,
      "grad_norm": 14.927027702331543,
      "learning_rate": 2.3016267123287673e-05,
      "loss": 0.6706,
      "step": 2720
    },
    {
      "epoch": 1.168664383561644,
      "grad_norm": 28.70943832397461,
      "learning_rate": 2.299058219178082e-05,
      "loss": 0.5062,
      "step": 2730
    },
    {
      "epoch": 1.172945205479452,
      "grad_norm": 20.617576599121094,
      "learning_rate": 2.2964897260273973e-05,
      "loss": 0.5652,
      "step": 2740
    },
    {
      "epoch": 1.1772260273972603,
      "grad_norm": 37.53913879394531,
      "learning_rate": 2.2939212328767123e-05,
      "loss": 0.682,
      "step": 2750
    },
    {
      "epoch": 1.1815068493150684,
      "grad_norm": 125.64456176757812,
      "learning_rate": 2.2913527397260276e-05,
      "loss": 0.6095,
      "step": 2760
    },
    {
      "epoch": 1.1857876712328768,
      "grad_norm": 0.6167072057723999,
      "learning_rate": 2.2887842465753425e-05,
      "loss": 0.3489,
      "step": 2770
    },
    {
      "epoch": 1.1900684931506849,
      "grad_norm": 8.875263214111328,
      "learning_rate": 2.2862157534246575e-05,
      "loss": 0.5724,
      "step": 2780
    },
    {
      "epoch": 1.1943493150684932,
      "grad_norm": 29.087247848510742,
      "learning_rate": 2.2836472602739725e-05,
      "loss": 0.5674,
      "step": 2790
    },
    {
      "epoch": 1.1986301369863013,
      "grad_norm": 108.30213165283203,
      "learning_rate": 2.2810787671232878e-05,
      "loss": 0.4056,
      "step": 2800
    },
    {
      "epoch": 1.2029109589041096,
      "grad_norm": 13.343461990356445,
      "learning_rate": 2.2785102739726028e-05,
      "loss": 0.4987,
      "step": 2810
    },
    {
      "epoch": 1.2071917808219177,
      "grad_norm": 0.6047355532646179,
      "learning_rate": 2.2759417808219178e-05,
      "loss": 0.2861,
      "step": 2820
    },
    {
      "epoch": 1.211472602739726,
      "grad_norm": 12.467401504516602,
      "learning_rate": 2.2733732876712328e-05,
      "loss": 0.7408,
      "step": 2830
    },
    {
      "epoch": 1.2157534246575343,
      "grad_norm": 15.868618965148926,
      "learning_rate": 2.270804794520548e-05,
      "loss": 0.4814,
      "step": 2840
    },
    {
      "epoch": 1.2200342465753424,
      "grad_norm": 24.539642333984375,
      "learning_rate": 2.268236301369863e-05,
      "loss": 0.6094,
      "step": 2850
    },
    {
      "epoch": 1.2243150684931507,
      "grad_norm": 15.609148979187012,
      "learning_rate": 2.2656678082191784e-05,
      "loss": 0.6397,
      "step": 2860
    },
    {
      "epoch": 1.2285958904109588,
      "grad_norm": 15.778641700744629,
      "learning_rate": 2.263099315068493e-05,
      "loss": 0.5055,
      "step": 2870
    },
    {
      "epoch": 1.2328767123287672,
      "grad_norm": 20.7979736328125,
      "learning_rate": 2.2605308219178083e-05,
      "loss": 0.3678,
      "step": 2880
    },
    {
      "epoch": 1.2371575342465753,
      "grad_norm": 39.31053924560547,
      "learning_rate": 2.2579623287671233e-05,
      "loss": 0.3605,
      "step": 2890
    },
    {
      "epoch": 1.2414383561643836,
      "grad_norm": 1.9259212017059326,
      "learning_rate": 2.2553938356164386e-05,
      "loss": 0.4457,
      "step": 2900
    },
    {
      "epoch": 1.245719178082192,
      "grad_norm": 39.82622146606445,
      "learning_rate": 2.2528253424657533e-05,
      "loss": 0.5663,
      "step": 2910
    },
    {
      "epoch": 1.25,
      "grad_norm": 18.27713394165039,
      "learning_rate": 2.2502568493150686e-05,
      "loss": 0.559,
      "step": 2920
    },
    {
      "epoch": 1.254280821917808,
      "grad_norm": 19.46271514892578,
      "learning_rate": 2.2476883561643836e-05,
      "loss": 0.5596,
      "step": 2930
    },
    {
      "epoch": 1.2585616438356164,
      "grad_norm": 15.22482681274414,
      "learning_rate": 2.245119863013699e-05,
      "loss": 0.5677,
      "step": 2940
    },
    {
      "epoch": 1.2628424657534247,
      "grad_norm": 6.7652812004089355,
      "learning_rate": 2.242551369863014e-05,
      "loss": 0.6691,
      "step": 2950
    },
    {
      "epoch": 1.2671232876712328,
      "grad_norm": 15.198921203613281,
      "learning_rate": 2.2399828767123288e-05,
      "loss": 0.4918,
      "step": 2960
    },
    {
      "epoch": 1.2714041095890412,
      "grad_norm": 103.82099151611328,
      "learning_rate": 2.2374143835616438e-05,
      "loss": 0.5306,
      "step": 2970
    },
    {
      "epoch": 1.2756849315068493,
      "grad_norm": 4.987117290496826,
      "learning_rate": 2.234845890410959e-05,
      "loss": 0.5766,
      "step": 2980
    },
    {
      "epoch": 1.2799657534246576,
      "grad_norm": 7.564026355743408,
      "learning_rate": 2.232277397260274e-05,
      "loss": 0.5242,
      "step": 2990
    },
    {
      "epoch": 1.2842465753424657,
      "grad_norm": 3.2099010944366455,
      "learning_rate": 2.229708904109589e-05,
      "loss": 0.6231,
      "step": 3000
    },
    {
      "epoch": 1.288527397260274,
      "grad_norm": 31.249996185302734,
      "learning_rate": 2.227140410958904e-05,
      "loss": 0.582,
      "step": 3010
    },
    {
      "epoch": 1.2928082191780823,
      "grad_norm": 24.8253116607666,
      "learning_rate": 2.2245719178082194e-05,
      "loss": 0.7384,
      "step": 3020
    },
    {
      "epoch": 1.2970890410958904,
      "grad_norm": 9.528766632080078,
      "learning_rate": 2.2220034246575344e-05,
      "loss": 0.3158,
      "step": 3030
    },
    {
      "epoch": 1.3013698630136985,
      "grad_norm": 20.124649047851562,
      "learning_rate": 2.2194349315068497e-05,
      "loss": 0.725,
      "step": 3040
    },
    {
      "epoch": 1.3056506849315068,
      "grad_norm": 6.461442470550537,
      "learning_rate": 2.2168664383561643e-05,
      "loss": 0.523,
      "step": 3050
    },
    {
      "epoch": 1.3099315068493151,
      "grad_norm": 18.374300003051758,
      "learning_rate": 2.2142979452054796e-05,
      "loss": 0.4177,
      "step": 3060
    },
    {
      "epoch": 1.3142123287671232,
      "grad_norm": 15.605560302734375,
      "learning_rate": 2.2117294520547946e-05,
      "loss": 0.6334,
      "step": 3070
    },
    {
      "epoch": 1.3184931506849316,
      "grad_norm": 40.33599090576172,
      "learning_rate": 2.2091609589041096e-05,
      "loss": 0.5582,
      "step": 3080
    },
    {
      "epoch": 1.3227739726027397,
      "grad_norm": 18.125089645385742,
      "learning_rate": 2.2065924657534246e-05,
      "loss": 0.5148,
      "step": 3090
    },
    {
      "epoch": 1.327054794520548,
      "grad_norm": 33.55887222290039,
      "learning_rate": 2.2040239726027395e-05,
      "loss": 0.4119,
      "step": 3100
    },
    {
      "epoch": 1.331335616438356,
      "grad_norm": 27.477209091186523,
      "learning_rate": 2.201455479452055e-05,
      "loss": 0.7335,
      "step": 3110
    },
    {
      "epoch": 1.3356164383561644,
      "grad_norm": 28.373653411865234,
      "learning_rate": 2.19888698630137e-05,
      "loss": 0.4652,
      "step": 3120
    },
    {
      "epoch": 1.3398972602739727,
      "grad_norm": 10.41767692565918,
      "learning_rate": 2.196318493150685e-05,
      "loss": 0.4556,
      "step": 3130
    },
    {
      "epoch": 1.3441780821917808,
      "grad_norm": 15.985031127929688,
      "learning_rate": 2.1937499999999998e-05,
      "loss": 0.5208,
      "step": 3140
    },
    {
      "epoch": 1.348458904109589,
      "grad_norm": 9.439993858337402,
      "learning_rate": 2.191181506849315e-05,
      "loss": 0.4854,
      "step": 3150
    },
    {
      "epoch": 1.3527397260273972,
      "grad_norm": 16.428089141845703,
      "learning_rate": 2.18861301369863e-05,
      "loss": 0.5496,
      "step": 3160
    },
    {
      "epoch": 1.3570205479452055,
      "grad_norm": 13.383630752563477,
      "learning_rate": 2.1860445205479454e-05,
      "loss": 0.4427,
      "step": 3170
    },
    {
      "epoch": 1.3613013698630136,
      "grad_norm": 21.770055770874023,
      "learning_rate": 2.1834760273972604e-05,
      "loss": 0.3362,
      "step": 3180
    },
    {
      "epoch": 1.365582191780822,
      "grad_norm": 2.661846876144409,
      "learning_rate": 2.1809075342465754e-05,
      "loss": 0.4305,
      "step": 3190
    },
    {
      "epoch": 1.36986301369863,
      "grad_norm": 9.57947826385498,
      "learning_rate": 2.1783390410958903e-05,
      "loss": 0.6307,
      "step": 3200
    },
    {
      "epoch": 1.3741438356164384,
      "grad_norm": 7.182124137878418,
      "learning_rate": 2.1757705479452057e-05,
      "loss": 0.7003,
      "step": 3210
    },
    {
      "epoch": 1.3784246575342465,
      "grad_norm": 12.542230606079102,
      "learning_rate": 2.1732020547945206e-05,
      "loss": 0.7924,
      "step": 3220
    },
    {
      "epoch": 1.3827054794520548,
      "grad_norm": 11.852202415466309,
      "learning_rate": 2.1706335616438356e-05,
      "loss": 0.5024,
      "step": 3230
    },
    {
      "epoch": 1.3869863013698631,
      "grad_norm": 22.10004997253418,
      "learning_rate": 2.1680650684931506e-05,
      "loss": 0.5869,
      "step": 3240
    },
    {
      "epoch": 1.3912671232876712,
      "grad_norm": 9.61673641204834,
      "learning_rate": 2.165496575342466e-05,
      "loss": 0.5511,
      "step": 3250
    },
    {
      "epoch": 1.3955479452054795,
      "grad_norm": 3.5772383213043213,
      "learning_rate": 2.162928082191781e-05,
      "loss": 0.5378,
      "step": 3260
    },
    {
      "epoch": 1.3998287671232876,
      "grad_norm": 14.454607009887695,
      "learning_rate": 2.1603595890410962e-05,
      "loss": 0.7021,
      "step": 3270
    },
    {
      "epoch": 1.404109589041096,
      "grad_norm": 12.301724433898926,
      "learning_rate": 2.157791095890411e-05,
      "loss": 0.7471,
      "step": 3280
    },
    {
      "epoch": 1.408390410958904,
      "grad_norm": 4.101442813873291,
      "learning_rate": 2.155222602739726e-05,
      "loss": 0.355,
      "step": 3290
    },
    {
      "epoch": 1.4126712328767124,
      "grad_norm": 39.615867614746094,
      "learning_rate": 2.152654109589041e-05,
      "loss": 0.6048,
      "step": 3300
    },
    {
      "epoch": 1.4169520547945205,
      "grad_norm": 24.081043243408203,
      "learning_rate": 2.1500856164383565e-05,
      "loss": 0.7089,
      "step": 3310
    },
    {
      "epoch": 1.4212328767123288,
      "grad_norm": 6.137239933013916,
      "learning_rate": 2.147517123287671e-05,
      "loss": 0.4565,
      "step": 3320
    },
    {
      "epoch": 1.4255136986301369,
      "grad_norm": 11.220376014709473,
      "learning_rate": 2.1449486301369864e-05,
      "loss": 0.4277,
      "step": 3330
    },
    {
      "epoch": 1.4297945205479452,
      "grad_norm": 49.353233337402344,
      "learning_rate": 2.1423801369863014e-05,
      "loss": 0.3421,
      "step": 3340
    },
    {
      "epoch": 1.4340753424657535,
      "grad_norm": 21.286075592041016,
      "learning_rate": 2.1398116438356167e-05,
      "loss": 0.4536,
      "step": 3350
    },
    {
      "epoch": 1.4383561643835616,
      "grad_norm": 10.80482006072998,
      "learning_rate": 2.1372431506849317e-05,
      "loss": 0.6355,
      "step": 3360
    },
    {
      "epoch": 1.44263698630137,
      "grad_norm": 22.60628318786621,
      "learning_rate": 2.1346746575342467e-05,
      "loss": 0.5357,
      "step": 3370
    },
    {
      "epoch": 1.446917808219178,
      "grad_norm": 7.038317680358887,
      "learning_rate": 2.1321061643835616e-05,
      "loss": 0.5848,
      "step": 3380
    },
    {
      "epoch": 1.4511986301369864,
      "grad_norm": 3.6277172565460205,
      "learning_rate": 2.129537671232877e-05,
      "loss": 0.3811,
      "step": 3390
    },
    {
      "epoch": 1.4554794520547945,
      "grad_norm": 141.23995971679688,
      "learning_rate": 2.126969178082192e-05,
      "loss": 0.3749,
      "step": 3400
    },
    {
      "epoch": 1.4597602739726028,
      "grad_norm": 9.505330085754395,
      "learning_rate": 2.1244006849315066e-05,
      "loss": 0.5169,
      "step": 3410
    },
    {
      "epoch": 1.464041095890411,
      "grad_norm": 31.260398864746094,
      "learning_rate": 2.121832191780822e-05,
      "loss": 0.6798,
      "step": 3420
    },
    {
      "epoch": 1.4683219178082192,
      "grad_norm": 14.576324462890625,
      "learning_rate": 2.119263698630137e-05,
      "loss": 0.3887,
      "step": 3430
    },
    {
      "epoch": 1.4726027397260273,
      "grad_norm": 17.221073150634766,
      "learning_rate": 2.1166952054794522e-05,
      "loss": 0.3907,
      "step": 3440
    },
    {
      "epoch": 1.4768835616438356,
      "grad_norm": 0.8417624235153198,
      "learning_rate": 2.114126712328767e-05,
      "loss": 0.3861,
      "step": 3450
    },
    {
      "epoch": 1.481164383561644,
      "grad_norm": 1.0525720119476318,
      "learning_rate": 2.111558219178082e-05,
      "loss": 0.5549,
      "step": 3460
    },
    {
      "epoch": 1.485445205479452,
      "grad_norm": 8.386690139770508,
      "learning_rate": 2.108989726027397e-05,
      "loss": 0.4114,
      "step": 3470
    },
    {
      "epoch": 1.4897260273972603,
      "grad_norm": 4.5233283042907715,
      "learning_rate": 2.1064212328767124e-05,
      "loss": 0.4464,
      "step": 3480
    },
    {
      "epoch": 1.4940068493150684,
      "grad_norm": 59.0789794921875,
      "learning_rate": 2.1038527397260274e-05,
      "loss": 0.682,
      "step": 3490
    },
    {
      "epoch": 1.4982876712328768,
      "grad_norm": 23.49764633178711,
      "learning_rate": 2.1012842465753427e-05,
      "loss": 0.4015,
      "step": 3500
    },
    {
      "epoch": 1.5025684931506849,
      "grad_norm": 26.084774017333984,
      "learning_rate": 2.0987157534246574e-05,
      "loss": 0.7139,
      "step": 3510
    },
    {
      "epoch": 1.5068493150684932,
      "grad_norm": 7.794407367706299,
      "learning_rate": 2.0961472602739727e-05,
      "loss": 0.4606,
      "step": 3520
    },
    {
      "epoch": 1.5111301369863015,
      "grad_norm": 11.288233757019043,
      "learning_rate": 2.0935787671232877e-05,
      "loss": 0.451,
      "step": 3530
    },
    {
      "epoch": 1.5154109589041096,
      "grad_norm": 2.306042194366455,
      "learning_rate": 2.091010273972603e-05,
      "loss": 0.3553,
      "step": 3540
    },
    {
      "epoch": 1.5196917808219177,
      "grad_norm": 11.157553672790527,
      "learning_rate": 2.0884417808219176e-05,
      "loss": 0.6895,
      "step": 3550
    },
    {
      "epoch": 1.523972602739726,
      "grad_norm": 11.598366737365723,
      "learning_rate": 2.085873287671233e-05,
      "loss": 0.5611,
      "step": 3560
    },
    {
      "epoch": 1.5282534246575343,
      "grad_norm": 16.561546325683594,
      "learning_rate": 2.083304794520548e-05,
      "loss": 0.5075,
      "step": 3570
    },
    {
      "epoch": 1.5325342465753424,
      "grad_norm": 0.8690369129180908,
      "learning_rate": 2.0807363013698632e-05,
      "loss": 0.6442,
      "step": 3580
    },
    {
      "epoch": 1.5368150684931505,
      "grad_norm": 19.73202896118164,
      "learning_rate": 2.0781678082191782e-05,
      "loss": 0.5141,
      "step": 3590
    },
    {
      "epoch": 1.541095890410959,
      "grad_norm": 14.27385425567627,
      "learning_rate": 2.0755993150684932e-05,
      "loss": 0.4149,
      "step": 3600
    },
    {
      "epoch": 1.5453767123287672,
      "grad_norm": 2.2302627563476562,
      "learning_rate": 2.0730308219178082e-05,
      "loss": 0.758,
      "step": 3610
    },
    {
      "epoch": 1.5496575342465753,
      "grad_norm": 4.972026824951172,
      "learning_rate": 2.0704623287671235e-05,
      "loss": 0.4684,
      "step": 3620
    },
    {
      "epoch": 1.5539383561643836,
      "grad_norm": 4.984468460083008,
      "learning_rate": 2.0678938356164385e-05,
      "loss": 0.4058,
      "step": 3630
    },
    {
      "epoch": 1.558219178082192,
      "grad_norm": 58.44465255737305,
      "learning_rate": 2.0653253424657534e-05,
      "loss": 0.6543,
      "step": 3640
    },
    {
      "epoch": 1.5625,
      "grad_norm": 10.453229904174805,
      "learning_rate": 2.0627568493150684e-05,
      "loss": 0.6567,
      "step": 3650
    },
    {
      "epoch": 1.566780821917808,
      "grad_norm": 32.872833251953125,
      "learning_rate": 2.0601883561643837e-05,
      "loss": 0.5757,
      "step": 3660
    },
    {
      "epoch": 1.5710616438356164,
      "grad_norm": 26.98045539855957,
      "learning_rate": 2.0576198630136987e-05,
      "loss": 0.6303,
      "step": 3670
    },
    {
      "epoch": 1.5753424657534247,
      "grad_norm": 19.083444595336914,
      "learning_rate": 2.055051369863014e-05,
      "loss": 0.4122,
      "step": 3680
    },
    {
      "epoch": 1.5796232876712328,
      "grad_norm": 23.704484939575195,
      "learning_rate": 2.0524828767123287e-05,
      "loss": 0.8762,
      "step": 3690
    },
    {
      "epoch": 1.583904109589041,
      "grad_norm": 10.016107559204102,
      "learning_rate": 2.049914383561644e-05,
      "loss": 0.6037,
      "step": 3700
    },
    {
      "epoch": 1.5881849315068495,
      "grad_norm": 11.916234016418457,
      "learning_rate": 2.047345890410959e-05,
      "loss": 0.4509,
      "step": 3710
    },
    {
      "epoch": 1.5924657534246576,
      "grad_norm": 12.279425621032715,
      "learning_rate": 2.0447773972602743e-05,
      "loss": 0.477,
      "step": 3720
    },
    {
      "epoch": 1.5967465753424657,
      "grad_norm": 41.168392181396484,
      "learning_rate": 2.042208904109589e-05,
      "loss": 0.3373,
      "step": 3730
    },
    {
      "epoch": 1.601027397260274,
      "grad_norm": 13.21113395690918,
      "learning_rate": 2.0396404109589042e-05,
      "loss": 0.6568,
      "step": 3740
    },
    {
      "epoch": 1.6053082191780823,
      "grad_norm": 55.25428009033203,
      "learning_rate": 2.0370719178082192e-05,
      "loss": 0.5181,
      "step": 3750
    },
    {
      "epoch": 1.6095890410958904,
      "grad_norm": 4.288461208343506,
      "learning_rate": 2.0345034246575342e-05,
      "loss": 0.4256,
      "step": 3760
    },
    {
      "epoch": 1.6138698630136985,
      "grad_norm": 9.131546020507812,
      "learning_rate": 2.0319349315068495e-05,
      "loss": 0.4068,
      "step": 3770
    },
    {
      "epoch": 1.6181506849315068,
      "grad_norm": 14.117582321166992,
      "learning_rate": 2.029366438356164e-05,
      "loss": 0.5419,
      "step": 3780
    },
    {
      "epoch": 1.6224315068493151,
      "grad_norm": 12.835063934326172,
      "learning_rate": 2.0267979452054795e-05,
      "loss": 0.3837,
      "step": 3790
    },
    {
      "epoch": 1.6267123287671232,
      "grad_norm": 3.802135944366455,
      "learning_rate": 2.0242294520547945e-05,
      "loss": 0.453,
      "step": 3800
    },
    {
      "epoch": 1.6309931506849316,
      "grad_norm": 21.60682487487793,
      "learning_rate": 2.0216609589041098e-05,
      "loss": 0.4163,
      "step": 3810
    },
    {
      "epoch": 1.6352739726027399,
      "grad_norm": 8.55812931060791,
      "learning_rate": 2.0190924657534244e-05,
      "loss": 0.6263,
      "step": 3820
    },
    {
      "epoch": 1.639554794520548,
      "grad_norm": 12.189432144165039,
      "learning_rate": 2.0165239726027397e-05,
      "loss": 0.5857,
      "step": 3830
    },
    {
      "epoch": 1.643835616438356,
      "grad_norm": 21.914569854736328,
      "learning_rate": 2.0139554794520547e-05,
      "loss": 0.6283,
      "step": 3840
    },
    {
      "epoch": 1.6481164383561644,
      "grad_norm": 6.071353912353516,
      "learning_rate": 2.01138698630137e-05,
      "loss": 0.5488,
      "step": 3850
    },
    {
      "epoch": 1.6523972602739727,
      "grad_norm": 2.474181652069092,
      "learning_rate": 2.008818493150685e-05,
      "loss": 0.7891,
      "step": 3860
    },
    {
      "epoch": 1.6566780821917808,
      "grad_norm": 5.496734142303467,
      "learning_rate": 2.00625e-05,
      "loss": 0.4428,
      "step": 3870
    },
    {
      "epoch": 1.660958904109589,
      "grad_norm": 5.611483573913574,
      "learning_rate": 2.003681506849315e-05,
      "loss": 0.5238,
      "step": 3880
    },
    {
      "epoch": 1.6652397260273972,
      "grad_norm": 18.853336334228516,
      "learning_rate": 2.0011130136986303e-05,
      "loss": 0.5035,
      "step": 3890
    },
    {
      "epoch": 1.6695205479452055,
      "grad_norm": 12.679913520812988,
      "learning_rate": 1.9985445205479453e-05,
      "loss": 0.4014,
      "step": 3900
    },
    {
      "epoch": 1.6738013698630136,
      "grad_norm": 7.984349250793457,
      "learning_rate": 1.9959760273972606e-05,
      "loss": 0.2865,
      "step": 3910
    },
    {
      "epoch": 1.678082191780822,
      "grad_norm": 27.35059356689453,
      "learning_rate": 1.9934075342465752e-05,
      "loss": 0.6572,
      "step": 3920
    },
    {
      "epoch": 1.6823630136986303,
      "grad_norm": 17.193315505981445,
      "learning_rate": 1.9908390410958905e-05,
      "loss": 0.4457,
      "step": 3930
    },
    {
      "epoch": 1.6866438356164384,
      "grad_norm": 17.416433334350586,
      "learning_rate": 1.9882705479452055e-05,
      "loss": 0.7941,
      "step": 3940
    },
    {
      "epoch": 1.6909246575342465,
      "grad_norm": 16.681852340698242,
      "learning_rate": 1.9857020547945208e-05,
      "loss": 0.412,
      "step": 3950
    },
    {
      "epoch": 1.6952054794520548,
      "grad_norm": 3.40037202835083,
      "learning_rate": 1.9831335616438355e-05,
      "loss": 0.4159,
      "step": 3960
    },
    {
      "epoch": 1.6994863013698631,
      "grad_norm": 5.397609233856201,
      "learning_rate": 1.9805650684931508e-05,
      "loss": 0.4583,
      "step": 3970
    },
    {
      "epoch": 1.7037671232876712,
      "grad_norm": 28.7585391998291,
      "learning_rate": 1.9779965753424658e-05,
      "loss": 0.6251,
      "step": 3980
    },
    {
      "epoch": 1.7080479452054793,
      "grad_norm": 16.76787567138672,
      "learning_rate": 1.975428082191781e-05,
      "loss": 0.4039,
      "step": 3990
    },
    {
      "epoch": 1.7123287671232876,
      "grad_norm": 18.519248962402344,
      "learning_rate": 1.972859589041096e-05,
      "loss": 0.5611,
      "step": 4000
    },
    {
      "epoch": 1.716609589041096,
      "grad_norm": 3.6796257495880127,
      "learning_rate": 1.970291095890411e-05,
      "loss": 0.61,
      "step": 4010
    },
    {
      "epoch": 1.720890410958904,
      "grad_norm": 2.5599374771118164,
      "learning_rate": 1.967722602739726e-05,
      "loss": 0.51,
      "step": 4020
    },
    {
      "epoch": 1.7251712328767124,
      "grad_norm": 4.009096622467041,
      "learning_rate": 1.9651541095890413e-05,
      "loss": 0.5236,
      "step": 4030
    },
    {
      "epoch": 1.7294520547945207,
      "grad_norm": 61.896385192871094,
      "learning_rate": 1.9625856164383563e-05,
      "loss": 0.4505,
      "step": 4040
    },
    {
      "epoch": 1.7337328767123288,
      "grad_norm": 1.6276532411575317,
      "learning_rate": 1.9600171232876713e-05,
      "loss": 0.3222,
      "step": 4050
    },
    {
      "epoch": 1.7380136986301369,
      "grad_norm": 23.2451114654541,
      "learning_rate": 1.9574486301369863e-05,
      "loss": 0.5861,
      "step": 4060
    },
    {
      "epoch": 1.7422945205479452,
      "grad_norm": 19.793611526489258,
      "learning_rate": 1.9548801369863016e-05,
      "loss": 0.4213,
      "step": 4070
    },
    {
      "epoch": 1.7465753424657535,
      "grad_norm": 24.211828231811523,
      "learning_rate": 1.9523116438356166e-05,
      "loss": 0.6524,
      "step": 4080
    },
    {
      "epoch": 1.7508561643835616,
      "grad_norm": 76.02991485595703,
      "learning_rate": 1.9497431506849315e-05,
      "loss": 0.4516,
      "step": 4090
    },
    {
      "epoch": 1.7551369863013697,
      "grad_norm": 31.41939353942871,
      "learning_rate": 1.9471746575342465e-05,
      "loss": 0.7432,
      "step": 4100
    },
    {
      "epoch": 1.759417808219178,
      "grad_norm": 19.59832000732422,
      "learning_rate": 1.9446061643835615e-05,
      "loss": 0.3654,
      "step": 4110
    },
    {
      "epoch": 1.7636986301369864,
      "grad_norm": 5.274127006530762,
      "learning_rate": 1.9420376712328768e-05,
      "loss": 0.5074,
      "step": 4120
    },
    {
      "epoch": 1.7679794520547945,
      "grad_norm": 14.360557556152344,
      "learning_rate": 1.9394691780821918e-05,
      "loss": 0.3533,
      "step": 4130
    },
    {
      "epoch": 1.7722602739726028,
      "grad_norm": 31.25728416442871,
      "learning_rate": 1.9369006849315068e-05,
      "loss": 0.7591,
      "step": 4140
    },
    {
      "epoch": 1.776541095890411,
      "grad_norm": 13.008493423461914,
      "learning_rate": 1.9343321917808217e-05,
      "loss": 0.6009,
      "step": 4150
    },
    {
      "epoch": 1.7808219178082192,
      "grad_norm": 11.199233055114746,
      "learning_rate": 1.931763698630137e-05,
      "loss": 0.4796,
      "step": 4160
    },
    {
      "epoch": 1.7851027397260273,
      "grad_norm": 7.007798671722412,
      "learning_rate": 1.929195205479452e-05,
      "loss": 0.359,
      "step": 4170
    },
    {
      "epoch": 1.7893835616438356,
      "grad_norm": 7.08510160446167,
      "learning_rate": 1.9266267123287674e-05,
      "loss": 0.3583,
      "step": 4180
    },
    {
      "epoch": 1.793664383561644,
      "grad_norm": 22.454998016357422,
      "learning_rate": 1.924058219178082e-05,
      "loss": 0.663,
      "step": 4190
    },
    {
      "epoch": 1.797945205479452,
      "grad_norm": 1.7827061414718628,
      "learning_rate": 1.9214897260273973e-05,
      "loss": 0.581,
      "step": 4200
    },
    {
      "epoch": 1.8022260273972601,
      "grad_norm": 9.737443923950195,
      "learning_rate": 1.9189212328767123e-05,
      "loss": 0.5445,
      "step": 4210
    },
    {
      "epoch": 1.8065068493150684,
      "grad_norm": 18.056055068969727,
      "learning_rate": 1.9163527397260276e-05,
      "loss": 0.3796,
      "step": 4220
    },
    {
      "epoch": 1.8107876712328768,
      "grad_norm": 22.148479461669922,
      "learning_rate": 1.9137842465753426e-05,
      "loss": 0.4609,
      "step": 4230
    },
    {
      "epoch": 1.8150684931506849,
      "grad_norm": 6.404452323913574,
      "learning_rate": 1.9112157534246576e-05,
      "loss": 0.3963,
      "step": 4240
    },
    {
      "epoch": 1.8193493150684932,
      "grad_norm": 17.864734649658203,
      "learning_rate": 1.9086472602739725e-05,
      "loss": 0.4972,
      "step": 4250
    },
    {
      "epoch": 1.8236301369863015,
      "grad_norm": 14.38877010345459,
      "learning_rate": 1.906078767123288e-05,
      "loss": 0.5134,
      "step": 4260
    },
    {
      "epoch": 1.8279109589041096,
      "grad_norm": 12.86507797241211,
      "learning_rate": 1.903510273972603e-05,
      "loss": 0.267,
      "step": 4270
    },
    {
      "epoch": 1.8321917808219177,
      "grad_norm": 30.7558536529541,
      "learning_rate": 1.9009417808219178e-05,
      "loss": 0.3332,
      "step": 4280
    },
    {
      "epoch": 1.836472602739726,
      "grad_norm": 46.09252166748047,
      "learning_rate": 1.8983732876712328e-05,
      "loss": 0.6937,
      "step": 4290
    },
    {
      "epoch": 1.8407534246575343,
      "grad_norm": 20.406261444091797,
      "learning_rate": 1.895804794520548e-05,
      "loss": 0.3059,
      "step": 4300
    },
    {
      "epoch": 1.8450342465753424,
      "grad_norm": 2.7680084705352783,
      "learning_rate": 1.893236301369863e-05,
      "loss": 0.5121,
      "step": 4310
    },
    {
      "epoch": 1.8493150684931505,
      "grad_norm": 12.978144645690918,
      "learning_rate": 1.8906678082191784e-05,
      "loss": 0.5167,
      "step": 4320
    },
    {
      "epoch": 1.853595890410959,
      "grad_norm": 12.749186515808105,
      "learning_rate": 1.888099315068493e-05,
      "loss": 0.4093,
      "step": 4330
    },
    {
      "epoch": 1.8578767123287672,
      "grad_norm": 4.961120128631592,
      "learning_rate": 1.8855308219178084e-05,
      "loss": 0.6228,
      "step": 4340
    },
    {
      "epoch": 1.8621575342465753,
      "grad_norm": 11.497303009033203,
      "learning_rate": 1.8829623287671233e-05,
      "loss": 0.4085,
      "step": 4350
    },
    {
      "epoch": 1.8664383561643836,
      "grad_norm": 9.498723030090332,
      "learning_rate": 1.8803938356164387e-05,
      "loss": 0.6306,
      "step": 4360
    },
    {
      "epoch": 1.870719178082192,
      "grad_norm": 18.589962005615234,
      "learning_rate": 1.8778253424657533e-05,
      "loss": 0.5089,
      "step": 4370
    },
    {
      "epoch": 1.875,
      "grad_norm": 13.742088317871094,
      "learning_rate": 1.8752568493150686e-05,
      "loss": 0.4835,
      "step": 4380
    },
    {
      "epoch": 1.879280821917808,
      "grad_norm": 13.139420509338379,
      "learning_rate": 1.8726883561643836e-05,
      "loss": 0.5298,
      "step": 4390
    },
    {
      "epoch": 1.8835616438356164,
      "grad_norm": 12.52042007446289,
      "learning_rate": 1.870119863013699e-05,
      "loss": 0.4707,
      "step": 4400
    },
    {
      "epoch": 1.8878424657534247,
      "grad_norm": 46.593074798583984,
      "learning_rate": 1.867551369863014e-05,
      "loss": 0.6026,
      "step": 4410
    },
    {
      "epoch": 1.8921232876712328,
      "grad_norm": 20.09745979309082,
      "learning_rate": 1.864982876712329e-05,
      "loss": 0.4009,
      "step": 4420
    },
    {
      "epoch": 1.896404109589041,
      "grad_norm": 21.251699447631836,
      "learning_rate": 1.862414383561644e-05,
      "loss": 0.5198,
      "step": 4430
    },
    {
      "epoch": 1.9006849315068495,
      "grad_norm": 36.72938537597656,
      "learning_rate": 1.8598458904109588e-05,
      "loss": 0.5082,
      "step": 4440
    },
    {
      "epoch": 1.9049657534246576,
      "grad_norm": 7.994814872741699,
      "learning_rate": 1.857277397260274e-05,
      "loss": 0.421,
      "step": 4450
    },
    {
      "epoch": 1.9092465753424657,
      "grad_norm": 7.301170825958252,
      "learning_rate": 1.8547089041095888e-05,
      "loss": 0.6448,
      "step": 4460
    },
    {
      "epoch": 1.913527397260274,
      "grad_norm": 6.295528888702393,
      "learning_rate": 1.852140410958904e-05,
      "loss": 0.4079,
      "step": 4470
    },
    {
      "epoch": 1.9178082191780823,
      "grad_norm": 25.070934295654297,
      "learning_rate": 1.849571917808219e-05,
      "loss": 0.6458,
      "step": 4480
    },
    {
      "epoch": 1.9220890410958904,
      "grad_norm": 15.986374855041504,
      "learning_rate": 1.8470034246575344e-05,
      "loss": 0.5191,
      "step": 4490
    },
    {
      "epoch": 1.9263698630136985,
      "grad_norm": 10.834296226501465,
      "learning_rate": 1.8444349315068494e-05,
      "loss": 0.4371,
      "step": 4500
    },
    {
      "epoch": 1.9306506849315068,
      "grad_norm": 44.80891036987305,
      "learning_rate": 1.8418664383561643e-05,
      "loss": 0.5873,
      "step": 4510
    },
    {
      "epoch": 1.9349315068493151,
      "grad_norm": 11.328543663024902,
      "learning_rate": 1.8392979452054793e-05,
      "loss": 0.4742,
      "step": 4520
    },
    {
      "epoch": 1.9392123287671232,
      "grad_norm": 2.0611612796783447,
      "learning_rate": 1.8367294520547946e-05,
      "loss": 0.57,
      "step": 4530
    },
    {
      "epoch": 1.9434931506849316,
      "grad_norm": 8.943866729736328,
      "learning_rate": 1.8341609589041096e-05,
      "loss": 0.5446,
      "step": 4540
    },
    {
      "epoch": 1.9477739726027399,
      "grad_norm": 21.342111587524414,
      "learning_rate": 1.8315924657534246e-05,
      "loss": 0.6471,
      "step": 4550
    },
    {
      "epoch": 1.952054794520548,
      "grad_norm": 6.3900604248046875,
      "learning_rate": 1.8290239726027396e-05,
      "loss": 0.6117,
      "step": 4560
    },
    {
      "epoch": 1.956335616438356,
      "grad_norm": 6.169938087463379,
      "learning_rate": 1.826455479452055e-05,
      "loss": 0.5876,
      "step": 4570
    },
    {
      "epoch": 1.9606164383561644,
      "grad_norm": 10.066176414489746,
      "learning_rate": 1.82388698630137e-05,
      "loss": 0.3932,
      "step": 4580
    },
    {
      "epoch": 1.9648972602739727,
      "grad_norm": 41.185943603515625,
      "learning_rate": 1.8213184931506852e-05,
      "loss": 0.518,
      "step": 4590
    },
    {
      "epoch": 1.9691780821917808,
      "grad_norm": 21.49726104736328,
      "learning_rate": 1.8187499999999998e-05,
      "loss": 0.4224,
      "step": 4600
    },
    {
      "epoch": 1.973458904109589,
      "grad_norm": 5.734244346618652,
      "learning_rate": 1.816181506849315e-05,
      "loss": 0.3754,
      "step": 4610
    },
    {
      "epoch": 1.9777397260273972,
      "grad_norm": 37.46115493774414,
      "learning_rate": 1.81361301369863e-05,
      "loss": 0.5325,
      "step": 4620
    },
    {
      "epoch": 1.9820205479452055,
      "grad_norm": 28.129777908325195,
      "learning_rate": 1.8110445205479454e-05,
      "loss": 0.5172,
      "step": 4630
    },
    {
      "epoch": 1.9863013698630136,
      "grad_norm": 13.365239143371582,
      "learning_rate": 1.8084760273972604e-05,
      "loss": 0.3772,
      "step": 4640
    },
    {
      "epoch": 1.990582191780822,
      "grad_norm": 8.639898300170898,
      "learning_rate": 1.8059075342465754e-05,
      "loss": 0.3761,
      "step": 4650
    },
    {
      "epoch": 1.9948630136986303,
      "grad_norm": 13.050405502319336,
      "learning_rate": 1.8033390410958904e-05,
      "loss": 0.3278,
      "step": 4660
    },
    {
      "epoch": 1.9991438356164384,
      "grad_norm": 21.96680450439453,
      "learning_rate": 1.8007705479452057e-05,
      "loss": 0.5551,
      "step": 4670
    },
    {
      "epoch": 2.0,
      "eval_f1": 0.6594364809181281,
      "eval_loss": 0.7407808303833008,
      "eval_precision": 0.690955407939604,
      "eval_recall": 0.6671832623014335,
      "eval_runtime": 539.0773,
      "eval_samples_per_second": 8.292,
      "eval_steps_per_second": 1.037,
      "step": 4672
    },
    {
      "epoch": 2.0034246575342465,
      "grad_norm": 7.5784077644348145,
      "learning_rate": 1.7982020547945207e-05,
      "loss": 0.3331,
      "step": 4680
    },
    {
      "epoch": 2.0077054794520546,
      "grad_norm": 21.390098571777344,
      "learning_rate": 1.7956335616438356e-05,
      "loss": 0.4134,
      "step": 4690
    },
    {
      "epoch": 2.011986301369863,
      "grad_norm": 18.07249641418457,
      "learning_rate": 1.7930650684931506e-05,
      "loss": 0.2384,
      "step": 4700
    },
    {
      "epoch": 2.016267123287671,
      "grad_norm": 19.398225784301758,
      "learning_rate": 1.790496575342466e-05,
      "loss": 0.6319,
      "step": 4710
    },
    {
      "epoch": 2.0205479452054793,
      "grad_norm": 5.301888465881348,
      "learning_rate": 1.787928082191781e-05,
      "loss": 0.2799,
      "step": 4720
    },
    {
      "epoch": 2.024828767123288,
      "grad_norm": 4.345090866088867,
      "learning_rate": 1.7853595890410962e-05,
      "loss": 0.418,
      "step": 4730
    },
    {
      "epoch": 2.029109589041096,
      "grad_norm": 37.557518005371094,
      "learning_rate": 1.782791095890411e-05,
      "loss": 0.3438,
      "step": 4740
    },
    {
      "epoch": 2.033390410958904,
      "grad_norm": 5.074457168579102,
      "learning_rate": 1.7802226027397262e-05,
      "loss": 0.4904,
      "step": 4750
    },
    {
      "epoch": 2.037671232876712,
      "grad_norm": 3.9058921337127686,
      "learning_rate": 1.777654109589041e-05,
      "loss": 0.453,
      "step": 4760
    },
    {
      "epoch": 2.0419520547945207,
      "grad_norm": 2.5020668506622314,
      "learning_rate": 1.7750856164383565e-05,
      "loss": 0.2404,
      "step": 4770
    },
    {
      "epoch": 2.046232876712329,
      "grad_norm": 8.928509712219238,
      "learning_rate": 1.772517123287671e-05,
      "loss": 0.4391,
      "step": 4780
    },
    {
      "epoch": 2.050513698630137,
      "grad_norm": 14.929970741271973,
      "learning_rate": 1.769948630136986e-05,
      "loss": 0.4441,
      "step": 4790
    },
    {
      "epoch": 2.0547945205479454,
      "grad_norm": 7.411099433898926,
      "learning_rate": 1.7673801369863014e-05,
      "loss": 0.3208,
      "step": 4800
    },
    {
      "epoch": 2.0590753424657535,
      "grad_norm": 8.026456832885742,
      "learning_rate": 1.7648116438356164e-05,
      "loss": 0.4963,
      "step": 4810
    },
    {
      "epoch": 2.0633561643835616,
      "grad_norm": 8.132319450378418,
      "learning_rate": 1.7622431506849317e-05,
      "loss": 0.2727,
      "step": 4820
    },
    {
      "epoch": 2.0676369863013697,
      "grad_norm": 18.716650009155273,
      "learning_rate": 1.7596746575342464e-05,
      "loss": 0.572,
      "step": 4830
    },
    {
      "epoch": 2.0719178082191783,
      "grad_norm": 24.137731552124023,
      "learning_rate": 1.7571061643835617e-05,
      "loss": 0.2845,
      "step": 4840
    },
    {
      "epoch": 2.0761986301369864,
      "grad_norm": 4.676906585693359,
      "learning_rate": 1.7545376712328767e-05,
      "loss": 0.4307,
      "step": 4850
    },
    {
      "epoch": 2.0804794520547945,
      "grad_norm": 17.069551467895508,
      "learning_rate": 1.751969178082192e-05,
      "loss": 0.3838,
      "step": 4860
    },
    {
      "epoch": 2.0847602739726026,
      "grad_norm": 7.364201068878174,
      "learning_rate": 1.7494006849315066e-05,
      "loss": 0.3437,
      "step": 4870
    },
    {
      "epoch": 2.089041095890411,
      "grad_norm": 2.5601813793182373,
      "learning_rate": 1.746832191780822e-05,
      "loss": 0.382,
      "step": 4880
    },
    {
      "epoch": 2.093321917808219,
      "grad_norm": 16.39969253540039,
      "learning_rate": 1.744263698630137e-05,
      "loss": 0.4344,
      "step": 4890
    },
    {
      "epoch": 2.0976027397260273,
      "grad_norm": 5.234245777130127,
      "learning_rate": 1.7416952054794522e-05,
      "loss": 0.302,
      "step": 4900
    },
    {
      "epoch": 2.101883561643836,
      "grad_norm": 4.944858551025391,
      "learning_rate": 1.7391267123287672e-05,
      "loss": 0.4896,
      "step": 4910
    },
    {
      "epoch": 2.106164383561644,
      "grad_norm": 15.660709381103516,
      "learning_rate": 1.7365582191780822e-05,
      "loss": 0.4176,
      "step": 4920
    },
    {
      "epoch": 2.110445205479452,
      "grad_norm": 3.9154975414276123,
      "learning_rate": 1.733989726027397e-05,
      "loss": 0.3128,
      "step": 4930
    },
    {
      "epoch": 2.11472602739726,
      "grad_norm": 27.22719955444336,
      "learning_rate": 1.7314212328767125e-05,
      "loss": 0.3307,
      "step": 4940
    },
    {
      "epoch": 2.1190068493150687,
      "grad_norm": 24.816665649414062,
      "learning_rate": 1.7288527397260274e-05,
      "loss": 0.5286,
      "step": 4950
    },
    {
      "epoch": 2.1232876712328768,
      "grad_norm": 28.056711196899414,
      "learning_rate": 1.7262842465753428e-05,
      "loss": 0.5091,
      "step": 4960
    },
    {
      "epoch": 2.127568493150685,
      "grad_norm": 36.482337951660156,
      "learning_rate": 1.7237157534246574e-05,
      "loss": 0.3768,
      "step": 4970
    },
    {
      "epoch": 2.131849315068493,
      "grad_norm": 32.92665481567383,
      "learning_rate": 1.7211472602739727e-05,
      "loss": 0.4938,
      "step": 4980
    },
    {
      "epoch": 2.1361301369863015,
      "grad_norm": 5.585390090942383,
      "learning_rate": 1.7185787671232877e-05,
      "loss": 0.4322,
      "step": 4990
    },
    {
      "epoch": 2.1404109589041096,
      "grad_norm": 17.453750610351562,
      "learning_rate": 1.716010273972603e-05,
      "loss": 0.4746,
      "step": 5000
    },
    {
      "epoch": 2.1446917808219177,
      "grad_norm": 5.330987930297852,
      "learning_rate": 1.7134417808219177e-05,
      "loss": 0.3506,
      "step": 5010
    },
    {
      "epoch": 2.1489726027397262,
      "grad_norm": 13.89369010925293,
      "learning_rate": 1.710873287671233e-05,
      "loss": 0.3438,
      "step": 5020
    },
    {
      "epoch": 2.1532534246575343,
      "grad_norm": 28.46163558959961,
      "learning_rate": 1.708304794520548e-05,
      "loss": 0.4291,
      "step": 5030
    },
    {
      "epoch": 2.1575342465753424,
      "grad_norm": 9.328664779663086,
      "learning_rate": 1.7057363013698633e-05,
      "loss": 0.2962,
      "step": 5040
    },
    {
      "epoch": 2.1618150684931505,
      "grad_norm": 26.54800796508789,
      "learning_rate": 1.7031678082191782e-05,
      "loss": 0.3664,
      "step": 5050
    },
    {
      "epoch": 2.166095890410959,
      "grad_norm": 10.387811660766602,
      "learning_rate": 1.7005993150684932e-05,
      "loss": 0.4343,
      "step": 5060
    },
    {
      "epoch": 2.170376712328767,
      "grad_norm": 7.560271739959717,
      "learning_rate": 1.6980308219178082e-05,
      "loss": 0.6425,
      "step": 5070
    },
    {
      "epoch": 2.1746575342465753,
      "grad_norm": 5.551652431488037,
      "learning_rate": 1.6954623287671235e-05,
      "loss": 0.3556,
      "step": 5080
    },
    {
      "epoch": 2.1789383561643834,
      "grad_norm": 26.02391242980957,
      "learning_rate": 1.6928938356164385e-05,
      "loss": 0.5694,
      "step": 5090
    },
    {
      "epoch": 2.183219178082192,
      "grad_norm": 6.021932125091553,
      "learning_rate": 1.6903253424657535e-05,
      "loss": 0.4364,
      "step": 5100
    },
    {
      "epoch": 2.1875,
      "grad_norm": 5.446930885314941,
      "learning_rate": 1.6877568493150685e-05,
      "loss": 0.245,
      "step": 5110
    },
    {
      "epoch": 2.191780821917808,
      "grad_norm": 82.65406799316406,
      "learning_rate": 1.6851883561643834e-05,
      "loss": 0.3438,
      "step": 5120
    },
    {
      "epoch": 2.1960616438356166,
      "grad_norm": 1.4810196161270142,
      "learning_rate": 1.6826198630136988e-05,
      "loss": 0.3721,
      "step": 5130
    },
    {
      "epoch": 2.2003424657534247,
      "grad_norm": 4.872475624084473,
      "learning_rate": 1.6800513698630137e-05,
      "loss": 0.2618,
      "step": 5140
    },
    {
      "epoch": 2.204623287671233,
      "grad_norm": 7.877708911895752,
      "learning_rate": 1.6774828767123287e-05,
      "loss": 0.3209,
      "step": 5150
    },
    {
      "epoch": 2.208904109589041,
      "grad_norm": 35.26039505004883,
      "learning_rate": 1.6749143835616437e-05,
      "loss": 0.3139,
      "step": 5160
    },
    {
      "epoch": 2.2131849315068495,
      "grad_norm": 17.693119049072266,
      "learning_rate": 1.672345890410959e-05,
      "loss": 0.5413,
      "step": 5170
    },
    {
      "epoch": 2.2174657534246576,
      "grad_norm": 19.4815616607666,
      "learning_rate": 1.669777397260274e-05,
      "loss": 0.2726,
      "step": 5180
    },
    {
      "epoch": 2.2217465753424657,
      "grad_norm": 28.45952796936035,
      "learning_rate": 1.667208904109589e-05,
      "loss": 0.4977,
      "step": 5190
    },
    {
      "epoch": 2.2260273972602738,
      "grad_norm": 27.157264709472656,
      "learning_rate": 1.664640410958904e-05,
      "loss": 0.555,
      "step": 5200
    },
    {
      "epoch": 2.2303082191780823,
      "grad_norm": 19.181812286376953,
      "learning_rate": 1.6620719178082193e-05,
      "loss": 0.1182,
      "step": 5210
    },
    {
      "epoch": 2.2345890410958904,
      "grad_norm": 19.080657958984375,
      "learning_rate": 1.6595034246575342e-05,
      "loss": 0.3881,
      "step": 5220
    },
    {
      "epoch": 2.2388698630136985,
      "grad_norm": 2.213844060897827,
      "learning_rate": 1.6569349315068495e-05,
      "loss": 0.457,
      "step": 5230
    },
    {
      "epoch": 2.243150684931507,
      "grad_norm": 41.012664794921875,
      "learning_rate": 1.6543664383561642e-05,
      "loss": 0.436,
      "step": 5240
    },
    {
      "epoch": 2.247431506849315,
      "grad_norm": 3.807694435119629,
      "learning_rate": 1.6517979452054795e-05,
      "loss": 0.7137,
      "step": 5250
    },
    {
      "epoch": 2.2517123287671232,
      "grad_norm": 8.440142631530762,
      "learning_rate": 1.6492294520547945e-05,
      "loss": 0.3145,
      "step": 5260
    },
    {
      "epoch": 2.2559931506849313,
      "grad_norm": 3.7810192108154297,
      "learning_rate": 1.6466609589041098e-05,
      "loss": 0.3953,
      "step": 5270
    },
    {
      "epoch": 2.26027397260274,
      "grad_norm": 9.430413246154785,
      "learning_rate": 1.6440924657534244e-05,
      "loss": 0.4492,
      "step": 5280
    },
    {
      "epoch": 2.264554794520548,
      "grad_norm": 32.50558853149414,
      "learning_rate": 1.6415239726027398e-05,
      "loss": 0.3626,
      "step": 5290
    },
    {
      "epoch": 2.268835616438356,
      "grad_norm": 20.190345764160156,
      "learning_rate": 1.6389554794520547e-05,
      "loss": 0.458,
      "step": 5300
    },
    {
      "epoch": 2.2731164383561646,
      "grad_norm": 14.516268730163574,
      "learning_rate": 1.63638698630137e-05,
      "loss": 0.6646,
      "step": 5310
    },
    {
      "epoch": 2.2773972602739727,
      "grad_norm": 41.566795349121094,
      "learning_rate": 1.633818493150685e-05,
      "loss": 0.2481,
      "step": 5320
    },
    {
      "epoch": 2.281678082191781,
      "grad_norm": 6.321224212646484,
      "learning_rate": 1.63125e-05,
      "loss": 0.2749,
      "step": 5330
    },
    {
      "epoch": 2.285958904109589,
      "grad_norm": 25.822124481201172,
      "learning_rate": 1.628681506849315e-05,
      "loss": 0.427,
      "step": 5340
    },
    {
      "epoch": 2.2902397260273974,
      "grad_norm": 7.016406059265137,
      "learning_rate": 1.6261130136986303e-05,
      "loss": 0.2023,
      "step": 5350
    },
    {
      "epoch": 2.2945205479452055,
      "grad_norm": 9.202388763427734,
      "learning_rate": 1.6235445205479453e-05,
      "loss": 0.3631,
      "step": 5360
    },
    {
      "epoch": 2.2988013698630136,
      "grad_norm": 68.24703216552734,
      "learning_rate": 1.6209760273972606e-05,
      "loss": 0.5583,
      "step": 5370
    },
    {
      "epoch": 2.3030821917808217,
      "grad_norm": 16.365711212158203,
      "learning_rate": 1.6184075342465752e-05,
      "loss": 0.4531,
      "step": 5380
    },
    {
      "epoch": 2.3073630136986303,
      "grad_norm": 26.823516845703125,
      "learning_rate": 1.6158390410958906e-05,
      "loss": 0.3281,
      "step": 5390
    },
    {
      "epoch": 2.3116438356164384,
      "grad_norm": 18.592607498168945,
      "learning_rate": 1.6132705479452055e-05,
      "loss": 0.3683,
      "step": 5400
    },
    {
      "epoch": 2.3159246575342465,
      "grad_norm": 25.722455978393555,
      "learning_rate": 1.610702054794521e-05,
      "loss": 0.3496,
      "step": 5410
    },
    {
      "epoch": 2.3202054794520546,
      "grad_norm": 18.97013282775879,
      "learning_rate": 1.6081335616438355e-05,
      "loss": 0.516,
      "step": 5420
    },
    {
      "epoch": 2.324486301369863,
      "grad_norm": 17.858768463134766,
      "learning_rate": 1.6055650684931508e-05,
      "loss": 0.5239,
      "step": 5430
    },
    {
      "epoch": 2.328767123287671,
      "grad_norm": 0.8647858500480652,
      "learning_rate": 1.6029965753424658e-05,
      "loss": 0.1986,
      "step": 5440
    },
    {
      "epoch": 2.3330479452054793,
      "grad_norm": 18.001590728759766,
      "learning_rate": 1.600428082191781e-05,
      "loss": 0.3065,
      "step": 5450
    },
    {
      "epoch": 2.337328767123288,
      "grad_norm": 16.14793586730957,
      "learning_rate": 1.597859589041096e-05,
      "loss": 0.3663,
      "step": 5460
    },
    {
      "epoch": 2.341609589041096,
      "grad_norm": 29.626916885375977,
      "learning_rate": 1.5952910958904107e-05,
      "loss": 0.3091,
      "step": 5470
    },
    {
      "epoch": 2.345890410958904,
      "grad_norm": 8.745993614196777,
      "learning_rate": 1.592722602739726e-05,
      "loss": 0.3453,
      "step": 5480
    },
    {
      "epoch": 2.350171232876712,
      "grad_norm": 18.141637802124023,
      "learning_rate": 1.590154109589041e-05,
      "loss": 0.4909,
      "step": 5490
    },
    {
      "epoch": 2.3544520547945207,
      "grad_norm": 7.329688549041748,
      "learning_rate": 1.5875856164383563e-05,
      "loss": 0.5541,
      "step": 5500
    },
    {
      "epoch": 2.358732876712329,
      "grad_norm": 22.19893455505371,
      "learning_rate": 1.585017123287671e-05,
      "loss": 0.3543,
      "step": 5510
    },
    {
      "epoch": 2.363013698630137,
      "grad_norm": 27.797569274902344,
      "learning_rate": 1.5824486301369863e-05,
      "loss": 0.2833,
      "step": 5520
    },
    {
      "epoch": 2.3672945205479454,
      "grad_norm": 25.593875885009766,
      "learning_rate": 1.5798801369863013e-05,
      "loss": 0.3358,
      "step": 5530
    },
    {
      "epoch": 2.3715753424657535,
      "grad_norm": 12.76595401763916,
      "learning_rate": 1.5773116438356166e-05,
      "loss": 0.4432,
      "step": 5540
    },
    {
      "epoch": 2.3758561643835616,
      "grad_norm": 12.162564277648926,
      "learning_rate": 1.5747431506849316e-05,
      "loss": 0.4548,
      "step": 5550
    },
    {
      "epoch": 2.3801369863013697,
      "grad_norm": 4.608762264251709,
      "learning_rate": 1.5721746575342465e-05,
      "loss": 0.399,
      "step": 5560
    },
    {
      "epoch": 2.3844178082191783,
      "grad_norm": 23.13904571533203,
      "learning_rate": 1.5696061643835615e-05,
      "loss": 0.3768,
      "step": 5570
    },
    {
      "epoch": 2.3886986301369864,
      "grad_norm": 13.040365219116211,
      "learning_rate": 1.567037671232877e-05,
      "loss": 0.5266,
      "step": 5580
    },
    {
      "epoch": 2.3929794520547945,
      "grad_norm": 27.029071807861328,
      "learning_rate": 1.5644691780821918e-05,
      "loss": 0.4631,
      "step": 5590
    },
    {
      "epoch": 2.3972602739726026,
      "grad_norm": 12.686609268188477,
      "learning_rate": 1.5619006849315068e-05,
      "loss": 0.3717,
      "step": 5600
    },
    {
      "epoch": 2.401541095890411,
      "grad_norm": 34.209197998046875,
      "learning_rate": 1.5593321917808218e-05,
      "loss": 0.3817,
      "step": 5610
    },
    {
      "epoch": 2.405821917808219,
      "grad_norm": 10.254599571228027,
      "learning_rate": 1.556763698630137e-05,
      "loss": 0.4157,
      "step": 5620
    },
    {
      "epoch": 2.4101027397260273,
      "grad_norm": 16.622798919677734,
      "learning_rate": 1.554195205479452e-05,
      "loss": 0.4799,
      "step": 5630
    },
    {
      "epoch": 2.4143835616438354,
      "grad_norm": 18.332128524780273,
      "learning_rate": 1.5516267123287674e-05,
      "loss": 0.3347,
      "step": 5640
    },
    {
      "epoch": 2.418664383561644,
      "grad_norm": 25.412059783935547,
      "learning_rate": 1.549058219178082e-05,
      "loss": 0.3954,
      "step": 5650
    },
    {
      "epoch": 2.422945205479452,
      "grad_norm": 12.598109245300293,
      "learning_rate": 1.5464897260273973e-05,
      "loss": 0.5304,
      "step": 5660
    },
    {
      "epoch": 2.42722602739726,
      "grad_norm": 5.205364227294922,
      "learning_rate": 1.5439212328767123e-05,
      "loss": 0.2844,
      "step": 5670
    },
    {
      "epoch": 2.4315068493150687,
      "grad_norm": 14.736977577209473,
      "learning_rate": 1.5413527397260276e-05,
      "loss": 0.2891,
      "step": 5680
    },
    {
      "epoch": 2.4357876712328768,
      "grad_norm": 16.482112884521484,
      "learning_rate": 1.5387842465753426e-05,
      "loss": 0.3232,
      "step": 5690
    },
    {
      "epoch": 2.440068493150685,
      "grad_norm": 4.986355781555176,
      "learning_rate": 1.5362157534246576e-05,
      "loss": 0.2102,
      "step": 5700
    },
    {
      "epoch": 2.444349315068493,
      "grad_norm": 16.099008560180664,
      "learning_rate": 1.5336472602739726e-05,
      "loss": 0.4282,
      "step": 5710
    },
    {
      "epoch": 2.4486301369863015,
      "grad_norm": 8.582435607910156,
      "learning_rate": 1.531078767123288e-05,
      "loss": 0.3354,
      "step": 5720
    },
    {
      "epoch": 2.4529109589041096,
      "grad_norm": 16.83907699584961,
      "learning_rate": 1.528510273972603e-05,
      "loss": 0.348,
      "step": 5730
    },
    {
      "epoch": 2.4571917808219177,
      "grad_norm": 18.713483810424805,
      "learning_rate": 1.525941780821918e-05,
      "loss": 0.5037,
      "step": 5740
    },
    {
      "epoch": 2.4614726027397262,
      "grad_norm": 8.611288070678711,
      "learning_rate": 1.5233732876712328e-05,
      "loss": 0.3006,
      "step": 5750
    },
    {
      "epoch": 2.4657534246575343,
      "grad_norm": 40.95690155029297,
      "learning_rate": 1.520804794520548e-05,
      "loss": 0.2929,
      "step": 5760
    },
    {
      "epoch": 2.4700342465753424,
      "grad_norm": 11.442882537841797,
      "learning_rate": 1.5182363013698631e-05,
      "loss": 0.4211,
      "step": 5770
    },
    {
      "epoch": 2.4743150684931505,
      "grad_norm": 9.837360382080078,
      "learning_rate": 1.5156678082191783e-05,
      "loss": 0.317,
      "step": 5780
    },
    {
      "epoch": 2.478595890410959,
      "grad_norm": 3.857771635055542,
      "learning_rate": 1.513099315068493e-05,
      "loss": 0.3037,
      "step": 5790
    },
    {
      "epoch": 2.482876712328767,
      "grad_norm": 3.8799350261688232,
      "learning_rate": 1.5105308219178082e-05,
      "loss": 0.2907,
      "step": 5800
    },
    {
      "epoch": 2.4871575342465753,
      "grad_norm": 8.690640449523926,
      "learning_rate": 1.5079623287671234e-05,
      "loss": 0.3324,
      "step": 5810
    },
    {
      "epoch": 2.491438356164384,
      "grad_norm": 16.528512954711914,
      "learning_rate": 1.5053938356164385e-05,
      "loss": 0.2389,
      "step": 5820
    },
    {
      "epoch": 2.495719178082192,
      "grad_norm": 26.928821563720703,
      "learning_rate": 1.5028253424657533e-05,
      "loss": 0.4505,
      "step": 5830
    },
    {
      "epoch": 2.5,
      "grad_norm": 14.557092666625977,
      "learning_rate": 1.5002568493150685e-05,
      "loss": 0.2683,
      "step": 5840
    },
    {
      "epoch": 2.504280821917808,
      "grad_norm": 6.253172397613525,
      "learning_rate": 1.4976883561643836e-05,
      "loss": 0.3873,
      "step": 5850
    },
    {
      "epoch": 2.508561643835616,
      "grad_norm": 2.911450147628784,
      "learning_rate": 1.4951198630136988e-05,
      "loss": 0.4035,
      "step": 5860
    },
    {
      "epoch": 2.5128424657534247,
      "grad_norm": 4.000398635864258,
      "learning_rate": 1.4925513698630137e-05,
      "loss": 0.4091,
      "step": 5870
    },
    {
      "epoch": 2.517123287671233,
      "grad_norm": 26.130159378051758,
      "learning_rate": 1.4899828767123289e-05,
      "loss": 0.4736,
      "step": 5880
    },
    {
      "epoch": 2.521404109589041,
      "grad_norm": 3.722693920135498,
      "learning_rate": 1.4874143835616439e-05,
      "loss": 0.2163,
      "step": 5890
    },
    {
      "epoch": 2.5256849315068495,
      "grad_norm": 29.399431228637695,
      "learning_rate": 1.484845890410959e-05,
      "loss": 0.3091,
      "step": 5900
    },
    {
      "epoch": 2.5299657534246576,
      "grad_norm": 17.68366241455078,
      "learning_rate": 1.482277397260274e-05,
      "loss": 0.4119,
      "step": 5910
    },
    {
      "epoch": 2.5342465753424657,
      "grad_norm": 41.45615768432617,
      "learning_rate": 1.4797089041095891e-05,
      "loss": 0.4262,
      "step": 5920
    },
    {
      "epoch": 2.5385273972602738,
      "grad_norm": 7.572283744812012,
      "learning_rate": 1.4771404109589041e-05,
      "loss": 0.5072,
      "step": 5930
    },
    {
      "epoch": 2.5428082191780823,
      "grad_norm": 1.1365571022033691,
      "learning_rate": 1.4745719178082193e-05,
      "loss": 0.2365,
      "step": 5940
    },
    {
      "epoch": 2.5470890410958904,
      "grad_norm": 5.3911967277526855,
      "learning_rate": 1.4720034246575344e-05,
      "loss": 0.5228,
      "step": 5950
    },
    {
      "epoch": 2.5513698630136985,
      "grad_norm": 15.704383850097656,
      "learning_rate": 1.4694349315068494e-05,
      "loss": 0.4204,
      "step": 5960
    },
    {
      "epoch": 2.555650684931507,
      "grad_norm": 17.785507202148438,
      "learning_rate": 1.4668664383561645e-05,
      "loss": 0.4702,
      "step": 5970
    },
    {
      "epoch": 2.559931506849315,
      "grad_norm": 54.5627555847168,
      "learning_rate": 1.4642979452054795e-05,
      "loss": 0.5656,
      "step": 5980
    },
    {
      "epoch": 2.5642123287671232,
      "grad_norm": 0.4259118437767029,
      "learning_rate": 1.4617294520547945e-05,
      "loss": 0.3317,
      "step": 5990
    },
    {
      "epoch": 2.5684931506849313,
      "grad_norm": 14.865763664245605,
      "learning_rate": 1.4591609589041095e-05,
      "loss": 0.3364,
      "step": 6000
    },
    {
      "epoch": 2.57277397260274,
      "grad_norm": 71.84607696533203,
      "learning_rate": 1.4565924657534246e-05,
      "loss": 0.4389,
      "step": 6010
    },
    {
      "epoch": 2.577054794520548,
      "grad_norm": 3.5749399662017822,
      "learning_rate": 1.4540239726027398e-05,
      "loss": 0.4435,
      "step": 6020
    },
    {
      "epoch": 2.581335616438356,
      "grad_norm": 6.95957088470459,
      "learning_rate": 1.4514554794520548e-05,
      "loss": 0.4064,
      "step": 6030
    },
    {
      "epoch": 2.5856164383561646,
      "grad_norm": 32.337677001953125,
      "learning_rate": 1.4488869863013699e-05,
      "loss": 0.4884,
      "step": 6040
    },
    {
      "epoch": 2.5898972602739727,
      "grad_norm": 14.259390830993652,
      "learning_rate": 1.4463184931506849e-05,
      "loss": 0.315,
      "step": 6050
    },
    {
      "epoch": 2.594178082191781,
      "grad_norm": 15.365755081176758,
      "learning_rate": 1.44375e-05,
      "loss": 0.3082,
      "step": 6060
    },
    {
      "epoch": 2.598458904109589,
      "grad_norm": 14.756214141845703,
      "learning_rate": 1.441181506849315e-05,
      "loss": 0.3758,
      "step": 6070
    },
    {
      "epoch": 2.602739726027397,
      "grad_norm": 9.131820678710938,
      "learning_rate": 1.4386130136986302e-05,
      "loss": 0.2125,
      "step": 6080
    },
    {
      "epoch": 2.6070205479452055,
      "grad_norm": 8.63611888885498,
      "learning_rate": 1.4360445205479451e-05,
      "loss": 0.5943,
      "step": 6090
    },
    {
      "epoch": 2.6113013698630136,
      "grad_norm": 0.5434450507164001,
      "learning_rate": 1.4334760273972603e-05,
      "loss": 0.265,
      "step": 6100
    },
    {
      "epoch": 2.615582191780822,
      "grad_norm": 48.881927490234375,
      "learning_rate": 1.4309075342465754e-05,
      "loss": 0.7705,
      "step": 6110
    },
    {
      "epoch": 2.6198630136986303,
      "grad_norm": 30.735191345214844,
      "learning_rate": 1.4283390410958904e-05,
      "loss": 0.3856,
      "step": 6120
    },
    {
      "epoch": 2.6241438356164384,
      "grad_norm": 7.842372894287109,
      "learning_rate": 1.4257705479452056e-05,
      "loss": 0.5184,
      "step": 6130
    },
    {
      "epoch": 2.6284246575342465,
      "grad_norm": 5.729582786560059,
      "learning_rate": 1.4232020547945205e-05,
      "loss": 0.311,
      "step": 6140
    },
    {
      "epoch": 2.6327054794520546,
      "grad_norm": 58.029170989990234,
      "learning_rate": 1.4206335616438357e-05,
      "loss": 0.2896,
      "step": 6150
    },
    {
      "epoch": 2.636986301369863,
      "grad_norm": 68.16889953613281,
      "learning_rate": 1.4180650684931507e-05,
      "loss": 0.4067,
      "step": 6160
    },
    {
      "epoch": 2.641267123287671,
      "grad_norm": 14.337040901184082,
      "learning_rate": 1.4154965753424658e-05,
      "loss": 0.2654,
      "step": 6170
    },
    {
      "epoch": 2.6455479452054793,
      "grad_norm": 22.358224868774414,
      "learning_rate": 1.412928082191781e-05,
      "loss": 0.4134,
      "step": 6180
    },
    {
      "epoch": 2.649828767123288,
      "grad_norm": 3.1232869625091553,
      "learning_rate": 1.410359589041096e-05,
      "loss": 0.2272,
      "step": 6190
    },
    {
      "epoch": 2.654109589041096,
      "grad_norm": 15.371111869812012,
      "learning_rate": 1.407791095890411e-05,
      "loss": 0.2948,
      "step": 6200
    },
    {
      "epoch": 2.658390410958904,
      "grad_norm": 6.1461873054504395,
      "learning_rate": 1.405222602739726e-05,
      "loss": 0.3548,
      "step": 6210
    },
    {
      "epoch": 2.662671232876712,
      "grad_norm": 12.958230972290039,
      "learning_rate": 1.4026541095890412e-05,
      "loss": 0.2452,
      "step": 6220
    },
    {
      "epoch": 2.6669520547945207,
      "grad_norm": 13.883332252502441,
      "learning_rate": 1.4000856164383562e-05,
      "loss": 0.3661,
      "step": 6230
    },
    {
      "epoch": 2.671232876712329,
      "grad_norm": 18.730741500854492,
      "learning_rate": 1.3975171232876713e-05,
      "loss": 0.4045,
      "step": 6240
    },
    {
      "epoch": 2.675513698630137,
      "grad_norm": 22.66792106628418,
      "learning_rate": 1.3949486301369863e-05,
      "loss": 0.7604,
      "step": 6250
    },
    {
      "epoch": 2.6797945205479454,
      "grad_norm": 12.541153907775879,
      "learning_rate": 1.3923801369863015e-05,
      "loss": 0.4365,
      "step": 6260
    },
    {
      "epoch": 2.6840753424657535,
      "grad_norm": 33.599945068359375,
      "learning_rate": 1.3898116438356166e-05,
      "loss": 0.3368,
      "step": 6270
    },
    {
      "epoch": 2.6883561643835616,
      "grad_norm": 7.811888217926025,
      "learning_rate": 1.3872431506849316e-05,
      "loss": 0.3929,
      "step": 6280
    },
    {
      "epoch": 2.6926369863013697,
      "grad_norm": 12.050020217895508,
      "learning_rate": 1.3846746575342467e-05,
      "loss": 0.2433,
      "step": 6290
    },
    {
      "epoch": 2.696917808219178,
      "grad_norm": 6.235482215881348,
      "learning_rate": 1.3821061643835617e-05,
      "loss": 0.3246,
      "step": 6300
    },
    {
      "epoch": 2.7011986301369864,
      "grad_norm": 2.5552730560302734,
      "learning_rate": 1.3795376712328769e-05,
      "loss": 0.2813,
      "step": 6310
    },
    {
      "epoch": 2.7054794520547945,
      "grad_norm": 34.8233757019043,
      "learning_rate": 1.3769691780821918e-05,
      "loss": 0.4047,
      "step": 6320
    },
    {
      "epoch": 2.709760273972603,
      "grad_norm": 2.2522239685058594,
      "learning_rate": 1.3744006849315068e-05,
      "loss": 0.2541,
      "step": 6330
    },
    {
      "epoch": 2.714041095890411,
      "grad_norm": 73.57962036132812,
      "learning_rate": 1.371832191780822e-05,
      "loss": 0.3109,
      "step": 6340
    },
    {
      "epoch": 2.718321917808219,
      "grad_norm": 5.327383041381836,
      "learning_rate": 1.369263698630137e-05,
      "loss": 0.4063,
      "step": 6350
    },
    {
      "epoch": 2.7226027397260273,
      "grad_norm": 47.393096923828125,
      "learning_rate": 1.366695205479452e-05,
      "loss": 0.2141,
      "step": 6360
    },
    {
      "epoch": 2.7268835616438354,
      "grad_norm": 95.91473388671875,
      "learning_rate": 1.364126712328767e-05,
      "loss": 0.3217,
      "step": 6370
    },
    {
      "epoch": 2.731164383561644,
      "grad_norm": 4.98040246963501,
      "learning_rate": 1.3615582191780822e-05,
      "loss": 0.4745,
      "step": 6380
    },
    {
      "epoch": 2.735445205479452,
      "grad_norm": 4.033075332641602,
      "learning_rate": 1.3589897260273972e-05,
      "loss": 0.4663,
      "step": 6390
    },
    {
      "epoch": 2.73972602739726,
      "grad_norm": 59.075706481933594,
      "learning_rate": 1.3564212328767123e-05,
      "loss": 0.4925,
      "step": 6400
    },
    {
      "epoch": 2.7440068493150687,
      "grad_norm": 2.0034759044647217,
      "learning_rate": 1.3538527397260273e-05,
      "loss": 0.3554,
      "step": 6410
    },
    {
      "epoch": 2.7482876712328768,
      "grad_norm": 14.203645706176758,
      "learning_rate": 1.3512842465753425e-05,
      "loss": 0.3135,
      "step": 6420
    },
    {
      "epoch": 2.752568493150685,
      "grad_norm": 28.168960571289062,
      "learning_rate": 1.3487157534246576e-05,
      "loss": 0.3593,
      "step": 6430
    },
    {
      "epoch": 2.756849315068493,
      "grad_norm": 8.687153816223145,
      "learning_rate": 1.3461472602739726e-05,
      "loss": 0.2939,
      "step": 6440
    },
    {
      "epoch": 2.7611301369863015,
      "grad_norm": 178.3568878173828,
      "learning_rate": 1.3435787671232877e-05,
      "loss": 0.2925,
      "step": 6450
    },
    {
      "epoch": 2.7654109589041096,
      "grad_norm": 10.218193054199219,
      "learning_rate": 1.3410102739726027e-05,
      "loss": 0.2169,
      "step": 6460
    },
    {
      "epoch": 2.7696917808219177,
      "grad_norm": 9.835856437683105,
      "learning_rate": 1.3384417808219179e-05,
      "loss": 0.3026,
      "step": 6470
    },
    {
      "epoch": 2.7739726027397262,
      "grad_norm": 10.289078712463379,
      "learning_rate": 1.3358732876712328e-05,
      "loss": 0.3038,
      "step": 6480
    },
    {
      "epoch": 2.7782534246575343,
      "grad_norm": 14.575322151184082,
      "learning_rate": 1.333304794520548e-05,
      "loss": 0.4191,
      "step": 6490
    },
    {
      "epoch": 2.7825342465753424,
      "grad_norm": 18.783327102661133,
      "learning_rate": 1.3307363013698631e-05,
      "loss": 0.3779,
      "step": 6500
    },
    {
      "epoch": 2.7868150684931505,
      "grad_norm": 9.678206443786621,
      "learning_rate": 1.3281678082191781e-05,
      "loss": 0.4244,
      "step": 6510
    },
    {
      "epoch": 2.791095890410959,
      "grad_norm": 10.929518699645996,
      "learning_rate": 1.3255993150684933e-05,
      "loss": 0.276,
      "step": 6520
    },
    {
      "epoch": 2.795376712328767,
      "grad_norm": 13.22543716430664,
      "learning_rate": 1.3230308219178082e-05,
      "loss": 0.3198,
      "step": 6530
    },
    {
      "epoch": 2.7996575342465753,
      "grad_norm": 11.663180351257324,
      "learning_rate": 1.3204623287671234e-05,
      "loss": 0.3545,
      "step": 6540
    },
    {
      "epoch": 2.803938356164384,
      "grad_norm": 20.666854858398438,
      "learning_rate": 1.3178938356164384e-05,
      "loss": 0.3197,
      "step": 6550
    },
    {
      "epoch": 2.808219178082192,
      "grad_norm": 52.60772705078125,
      "learning_rate": 1.3153253424657535e-05,
      "loss": 0.3719,
      "step": 6560
    },
    {
      "epoch": 2.8125,
      "grad_norm": 4.957720756530762,
      "learning_rate": 1.3127568493150685e-05,
      "loss": 0.3335,
      "step": 6570
    },
    {
      "epoch": 2.816780821917808,
      "grad_norm": 16.60493278503418,
      "learning_rate": 1.3101883561643836e-05,
      "loss": 0.2479,
      "step": 6580
    },
    {
      "epoch": 2.821061643835616,
      "grad_norm": 66.47907257080078,
      "learning_rate": 1.3076198630136988e-05,
      "loss": 0.6352,
      "step": 6590
    },
    {
      "epoch": 2.8253424657534247,
      "grad_norm": 39.71760559082031,
      "learning_rate": 1.3050513698630138e-05,
      "loss": 0.2677,
      "step": 6600
    },
    {
      "epoch": 2.829623287671233,
      "grad_norm": 1.683408260345459,
      "learning_rate": 1.3024828767123289e-05,
      "loss": 0.3094,
      "step": 6610
    },
    {
      "epoch": 2.833904109589041,
      "grad_norm": 39.07011032104492,
      "learning_rate": 1.2999143835616439e-05,
      "loss": 0.406,
      "step": 6620
    },
    {
      "epoch": 2.8381849315068495,
      "grad_norm": 19.41545295715332,
      "learning_rate": 1.297345890410959e-05,
      "loss": 0.3937,
      "step": 6630
    },
    {
      "epoch": 2.8424657534246576,
      "grad_norm": 17.39725112915039,
      "learning_rate": 1.294777397260274e-05,
      "loss": 0.7544,
      "step": 6640
    },
    {
      "epoch": 2.8467465753424657,
      "grad_norm": 12.125307083129883,
      "learning_rate": 1.2922089041095892e-05,
      "loss": 0.4436,
      "step": 6650
    },
    {
      "epoch": 2.8510273972602738,
      "grad_norm": 16.698118209838867,
      "learning_rate": 1.2896404109589041e-05,
      "loss": 0.3357,
      "step": 6660
    },
    {
      "epoch": 2.8553082191780823,
      "grad_norm": 17.360824584960938,
      "learning_rate": 1.2870719178082191e-05,
      "loss": 0.5837,
      "step": 6670
    },
    {
      "epoch": 2.8595890410958904,
      "grad_norm": 1.2292284965515137,
      "learning_rate": 1.2845034246575343e-05,
      "loss": 0.4396,
      "step": 6680
    },
    {
      "epoch": 2.8638698630136985,
      "grad_norm": 11.362168312072754,
      "learning_rate": 1.2819349315068492e-05,
      "loss": 0.3662,
      "step": 6690
    },
    {
      "epoch": 2.868150684931507,
      "grad_norm": 9.516593933105469,
      "learning_rate": 1.2793664383561644e-05,
      "loss": 0.2031,
      "step": 6700
    },
    {
      "epoch": 2.872431506849315,
      "grad_norm": 13.51259994506836,
      "learning_rate": 1.2767979452054794e-05,
      "loss": 0.2392,
      "step": 6710
    },
    {
      "epoch": 2.8767123287671232,
      "grad_norm": 1.1329867839813232,
      "learning_rate": 1.2742294520547945e-05,
      "loss": 0.3997,
      "step": 6720
    },
    {
      "epoch": 2.8809931506849313,
      "grad_norm": 6.9017653465271,
      "learning_rate": 1.2716609589041095e-05,
      "loss": 0.2701,
      "step": 6730
    },
    {
      "epoch": 2.88527397260274,
      "grad_norm": 4.315032005310059,
      "learning_rate": 1.2690924657534246e-05,
      "loss": 0.3739,
      "step": 6740
    },
    {
      "epoch": 2.889554794520548,
      "grad_norm": 7.0909600257873535,
      "learning_rate": 1.2665239726027398e-05,
      "loss": 0.2293,
      "step": 6750
    },
    {
      "epoch": 2.893835616438356,
      "grad_norm": 7.25237512588501,
      "learning_rate": 1.2639554794520548e-05,
      "loss": 0.3554,
      "step": 6760
    },
    {
      "epoch": 2.8981164383561646,
      "grad_norm": 5.144253730773926,
      "learning_rate": 1.26138698630137e-05,
      "loss": 0.3319,
      "step": 6770
    },
    {
      "epoch": 2.9023972602739727,
      "grad_norm": 10.541754722595215,
      "learning_rate": 1.2588184931506849e-05,
      "loss": 0.2834,
      "step": 6780
    },
    {
      "epoch": 2.906678082191781,
      "grad_norm": 349.6491394042969,
      "learning_rate": 1.25625e-05,
      "loss": 0.3183,
      "step": 6790
    },
    {
      "epoch": 2.910958904109589,
      "grad_norm": 23.919036865234375,
      "learning_rate": 1.253681506849315e-05,
      "loss": 0.5665,
      "step": 6800
    },
    {
      "epoch": 2.915239726027397,
      "grad_norm": 30.442419052124023,
      "learning_rate": 1.2511130136986302e-05,
      "loss": 0.3193,
      "step": 6810
    },
    {
      "epoch": 2.9195205479452055,
      "grad_norm": 4.340581893920898,
      "learning_rate": 1.2485445205479451e-05,
      "loss": 0.4151,
      "step": 6820
    },
    {
      "epoch": 2.9238013698630136,
      "grad_norm": 25.994104385375977,
      "learning_rate": 1.2459760273972603e-05,
      "loss": 0.4564,
      "step": 6830
    },
    {
      "epoch": 2.928082191780822,
      "grad_norm": 64.38519287109375,
      "learning_rate": 1.2434075342465754e-05,
      "loss": 0.4864,
      "step": 6840
    },
    {
      "epoch": 2.9323630136986303,
      "grad_norm": 17.231943130493164,
      "learning_rate": 1.2408390410958904e-05,
      "loss": 0.4898,
      "step": 6850
    },
    {
      "epoch": 2.9366438356164384,
      "grad_norm": 3.990678310394287,
      "learning_rate": 1.2382705479452056e-05,
      "loss": 0.2025,
      "step": 6860
    },
    {
      "epoch": 2.9409246575342465,
      "grad_norm": 7.416327953338623,
      "learning_rate": 1.2357020547945205e-05,
      "loss": 0.3813,
      "step": 6870
    },
    {
      "epoch": 2.9452054794520546,
      "grad_norm": 16.415189743041992,
      "learning_rate": 1.2331335616438357e-05,
      "loss": 0.5509,
      "step": 6880
    },
    {
      "epoch": 2.949486301369863,
      "grad_norm": 0.2791307270526886,
      "learning_rate": 1.2305650684931507e-05,
      "loss": 0.3133,
      "step": 6890
    },
    {
      "epoch": 2.953767123287671,
      "grad_norm": 9.656376838684082,
      "learning_rate": 1.2279965753424658e-05,
      "loss": 0.5342,
      "step": 6900
    },
    {
      "epoch": 2.9580479452054793,
      "grad_norm": 13.278460502624512,
      "learning_rate": 1.225428082191781e-05,
      "loss": 0.3831,
      "step": 6910
    },
    {
      "epoch": 2.962328767123288,
      "grad_norm": 1.9387692213058472,
      "learning_rate": 1.222859589041096e-05,
      "loss": 0.3702,
      "step": 6920
    },
    {
      "epoch": 2.966609589041096,
      "grad_norm": 0.4600253701210022,
      "learning_rate": 1.2202910958904111e-05,
      "loss": 0.2236,
      "step": 6930
    },
    {
      "epoch": 2.970890410958904,
      "grad_norm": 11.301372528076172,
      "learning_rate": 1.217722602739726e-05,
      "loss": 0.2348,
      "step": 6940
    },
    {
      "epoch": 2.975171232876712,
      "grad_norm": 17.38274383544922,
      "learning_rate": 1.2151541095890412e-05,
      "loss": 0.4951,
      "step": 6950
    },
    {
      "epoch": 2.9794520547945207,
      "grad_norm": 4.354271411895752,
      "learning_rate": 1.2125856164383562e-05,
      "loss": 0.6209,
      "step": 6960
    },
    {
      "epoch": 2.983732876712329,
      "grad_norm": 11.591288566589355,
      "learning_rate": 1.2100171232876713e-05,
      "loss": 0.2922,
      "step": 6970
    },
    {
      "epoch": 2.988013698630137,
      "grad_norm": 6.630304336547852,
      "learning_rate": 1.2074486301369863e-05,
      "loss": 0.46,
      "step": 6980
    },
    {
      "epoch": 2.9922945205479454,
      "grad_norm": 10.017501831054688,
      "learning_rate": 1.2048801369863015e-05,
      "loss": 0.4073,
      "step": 6990
    },
    {
      "epoch": 2.9965753424657535,
      "grad_norm": 8.010895729064941,
      "learning_rate": 1.2023116438356166e-05,
      "loss": 0.2658,
      "step": 7000
    },
    {
      "epoch": 3.0,
      "eval_f1": 0.691335357376561,
      "eval_loss": 0.6322787404060364,
      "eval_precision": 0.689940389645045,
      "eval_recall": 0.702699212191657,
      "eval_runtime": 536.5879,
      "eval_samples_per_second": 8.33,
      "eval_steps_per_second": 1.042,
      "step": 7008
    },
    {
      "epoch": 3.0008561643835616,
      "grad_norm": 7.107393264770508,
      "learning_rate": 1.1997431506849314e-05,
      "loss": 0.4602,
      "step": 7010
    },
    {
      "epoch": 3.0051369863013697,
      "grad_norm": 15.877123832702637,
      "learning_rate": 1.1971746575342466e-05,
      "loss": 0.3676,
      "step": 7020
    },
    {
      "epoch": 3.0094178082191783,
      "grad_norm": 6.789012432098389,
      "learning_rate": 1.1946061643835616e-05,
      "loss": 0.258,
      "step": 7030
    },
    {
      "epoch": 3.0136986301369864,
      "grad_norm": 3.0907251834869385,
      "learning_rate": 1.1920376712328767e-05,
      "loss": 0.4291,
      "step": 7040
    },
    {
      "epoch": 3.0179794520547945,
      "grad_norm": 10.118813514709473,
      "learning_rate": 1.1894691780821917e-05,
      "loss": 0.1185,
      "step": 7050
    },
    {
      "epoch": 3.0222602739726026,
      "grad_norm": 44.35822677612305,
      "learning_rate": 1.1869006849315068e-05,
      "loss": 0.2515,
      "step": 7060
    },
    {
      "epoch": 3.026541095890411,
      "grad_norm": 7.587951183319092,
      "learning_rate": 1.184332191780822e-05,
      "loss": 0.364,
      "step": 7070
    },
    {
      "epoch": 3.030821917808219,
      "grad_norm": 0.6930009126663208,
      "learning_rate": 1.181763698630137e-05,
      "loss": 0.3527,
      "step": 7080
    },
    {
      "epoch": 3.0351027397260273,
      "grad_norm": 3.604750871658325,
      "learning_rate": 1.1791952054794521e-05,
      "loss": 0.119,
      "step": 7090
    },
    {
      "epoch": 3.039383561643836,
      "grad_norm": 8.773877143859863,
      "learning_rate": 1.176626712328767e-05,
      "loss": 0.2829,
      "step": 7100
    },
    {
      "epoch": 3.043664383561644,
      "grad_norm": 4.335702419281006,
      "learning_rate": 1.1740582191780822e-05,
      "loss": 0.3034,
      "step": 7110
    },
    {
      "epoch": 3.047945205479452,
      "grad_norm": 13.531649589538574,
      "learning_rate": 1.1714897260273972e-05,
      "loss": 0.229,
      "step": 7120
    },
    {
      "epoch": 3.05222602739726,
      "grad_norm": 42.93304443359375,
      "learning_rate": 1.1689212328767124e-05,
      "loss": 0.3274,
      "step": 7130
    },
    {
      "epoch": 3.0565068493150687,
      "grad_norm": 26.934123992919922,
      "learning_rate": 1.1663527397260273e-05,
      "loss": 0.4067,
      "step": 7140
    },
    {
      "epoch": 3.0607876712328768,
      "grad_norm": 20.19399070739746,
      "learning_rate": 1.1637842465753425e-05,
      "loss": 0.3622,
      "step": 7150
    },
    {
      "epoch": 3.065068493150685,
      "grad_norm": 0.692182183265686,
      "learning_rate": 1.1612157534246576e-05,
      "loss": 0.4016,
      "step": 7160
    },
    {
      "epoch": 3.069349315068493,
      "grad_norm": 16.932876586914062,
      "learning_rate": 1.1586472602739726e-05,
      "loss": 0.2211,
      "step": 7170
    },
    {
      "epoch": 3.0736301369863015,
      "grad_norm": 5.115691184997559,
      "learning_rate": 1.1560787671232878e-05,
      "loss": 0.3513,
      "step": 7180
    },
    {
      "epoch": 3.0779109589041096,
      "grad_norm": 15.148873329162598,
      "learning_rate": 1.1535102739726027e-05,
      "loss": 0.3692,
      "step": 7190
    },
    {
      "epoch": 3.0821917808219177,
      "grad_norm": 12.05111312866211,
      "learning_rate": 1.1509417808219179e-05,
      "loss": 0.3021,
      "step": 7200
    },
    {
      "epoch": 3.0864726027397262,
      "grad_norm": 26.534669876098633,
      "learning_rate": 1.1483732876712329e-05,
      "loss": 0.3263,
      "step": 7210
    },
    {
      "epoch": 3.0907534246575343,
      "grad_norm": 5.85041618347168,
      "learning_rate": 1.145804794520548e-05,
      "loss": 0.2395,
      "step": 7220
    },
    {
      "epoch": 3.0950342465753424,
      "grad_norm": 2.9019341468811035,
      "learning_rate": 1.1432363013698631e-05,
      "loss": 0.2463,
      "step": 7230
    },
    {
      "epoch": 3.0993150684931505,
      "grad_norm": 21.326887130737305,
      "learning_rate": 1.1406678082191781e-05,
      "loss": 0.2597,
      "step": 7240
    },
    {
      "epoch": 3.103595890410959,
      "grad_norm": 1.2641385793685913,
      "learning_rate": 1.1380993150684933e-05,
      "loss": 0.2005,
      "step": 7250
    },
    {
      "epoch": 3.107876712328767,
      "grad_norm": 15.008341789245605,
      "learning_rate": 1.1355308219178083e-05,
      "loss": 0.2482,
      "step": 7260
    },
    {
      "epoch": 3.1121575342465753,
      "grad_norm": 4.2595062255859375,
      "learning_rate": 1.1329623287671234e-05,
      "loss": 0.2605,
      "step": 7270
    },
    {
      "epoch": 3.1164383561643834,
      "grad_norm": 7.89963960647583,
      "learning_rate": 1.1303938356164384e-05,
      "loss": 0.4661,
      "step": 7280
    },
    {
      "epoch": 3.120719178082192,
      "grad_norm": 5.801093101501465,
      "learning_rate": 1.1278253424657535e-05,
      "loss": 0.351,
      "step": 7290
    },
    {
      "epoch": 3.125,
      "grad_norm": 3.259659767150879,
      "learning_rate": 1.1252568493150685e-05,
      "loss": 0.3098,
      "step": 7300
    },
    {
      "epoch": 3.129280821917808,
      "grad_norm": 8.621288299560547,
      "learning_rate": 1.1226883561643837e-05,
      "loss": 0.1927,
      "step": 7310
    },
    {
      "epoch": 3.1335616438356166,
      "grad_norm": 14.369351387023926,
      "learning_rate": 1.1201198630136988e-05,
      "loss": 0.4466,
      "step": 7320
    },
    {
      "epoch": 3.1378424657534247,
      "grad_norm": 31.71902847290039,
      "learning_rate": 1.1175513698630138e-05,
      "loss": 0.4271,
      "step": 7330
    },
    {
      "epoch": 3.142123287671233,
      "grad_norm": 12.180694580078125,
      "learning_rate": 1.114982876712329e-05,
      "loss": 0.4397,
      "step": 7340
    },
    {
      "epoch": 3.146404109589041,
      "grad_norm": 3.724388599395752,
      "learning_rate": 1.1124143835616439e-05,
      "loss": 0.2239,
      "step": 7350
    },
    {
      "epoch": 3.1506849315068495,
      "grad_norm": 8.645956993103027,
      "learning_rate": 1.1098458904109589e-05,
      "loss": 0.2115,
      "step": 7360
    },
    {
      "epoch": 3.1549657534246576,
      "grad_norm": 2.8188278675079346,
      "learning_rate": 1.1072773972602739e-05,
      "loss": 0.1467,
      "step": 7370
    },
    {
      "epoch": 3.1592465753424657,
      "grad_norm": 16.52374267578125,
      "learning_rate": 1.104708904109589e-05,
      "loss": 0.2261,
      "step": 7380
    },
    {
      "epoch": 3.1635273972602738,
      "grad_norm": 6.666365146636963,
      "learning_rate": 1.102140410958904e-05,
      "loss": 0.2501,
      "step": 7390
    },
    {
      "epoch": 3.1678082191780823,
      "grad_norm": 92.22974395751953,
      "learning_rate": 1.0995719178082191e-05,
      "loss": 0.2264,
      "step": 7400
    },
    {
      "epoch": 3.1720890410958904,
      "grad_norm": 38.882362365722656,
      "learning_rate": 1.0970034246575343e-05,
      "loss": 0.4654,
      "step": 7410
    },
    {
      "epoch": 3.1763698630136985,
      "grad_norm": 8.898421287536621,
      "learning_rate": 1.0944349315068493e-05,
      "loss": 0.3028,
      "step": 7420
    },
    {
      "epoch": 3.180650684931507,
      "grad_norm": 5.811822414398193,
      "learning_rate": 1.0918664383561644e-05,
      "loss": 0.2957,
      "step": 7430
    },
    {
      "epoch": 3.184931506849315,
      "grad_norm": 18.61747932434082,
      "learning_rate": 1.0892979452054794e-05,
      "loss": 0.3112,
      "step": 7440
    },
    {
      "epoch": 3.1892123287671232,
      "grad_norm": 11.638489723205566,
      "learning_rate": 1.0867294520547945e-05,
      "loss": 0.1755,
      "step": 7450
    },
    {
      "epoch": 3.1934931506849313,
      "grad_norm": 9.74038028717041,
      "learning_rate": 1.0841609589041095e-05,
      "loss": 0.202,
      "step": 7460
    },
    {
      "epoch": 3.19777397260274,
      "grad_norm": 11.804908752441406,
      "learning_rate": 1.0815924657534247e-05,
      "loss": 0.2544,
      "step": 7470
    },
    {
      "epoch": 3.202054794520548,
      "grad_norm": 12.588708877563477,
      "learning_rate": 1.0790239726027398e-05,
      "loss": 0.353,
      "step": 7480
    },
    {
      "epoch": 3.206335616438356,
      "grad_norm": 1.1729371547698975,
      "learning_rate": 1.0764554794520548e-05,
      "loss": 0.1694,
      "step": 7490
    },
    {
      "epoch": 3.2106164383561646,
      "grad_norm": 31.956037521362305,
      "learning_rate": 1.07388698630137e-05,
      "loss": 0.1155,
      "step": 7500
    },
    {
      "epoch": 3.2148972602739727,
      "grad_norm": 29.97380828857422,
      "learning_rate": 1.0713184931506849e-05,
      "loss": 0.3226,
      "step": 7510
    },
    {
      "epoch": 3.219178082191781,
      "grad_norm": 16.509052276611328,
      "learning_rate": 1.06875e-05,
      "loss": 0.2799,
      "step": 7520
    },
    {
      "epoch": 3.223458904109589,
      "grad_norm": 65.94884490966797,
      "learning_rate": 1.066181506849315e-05,
      "loss": 0.3002,
      "step": 7530
    },
    {
      "epoch": 3.2277397260273974,
      "grad_norm": 11.29791259765625,
      "learning_rate": 1.0636130136986302e-05,
      "loss": 0.14,
      "step": 7540
    },
    {
      "epoch": 3.2320205479452055,
      "grad_norm": 9.676562309265137,
      "learning_rate": 1.0610445205479452e-05,
      "loss": 0.1474,
      "step": 7550
    },
    {
      "epoch": 3.2363013698630136,
      "grad_norm": 32.383384704589844,
      "learning_rate": 1.0584760273972603e-05,
      "loss": 0.2542,
      "step": 7560
    },
    {
      "epoch": 3.2405821917808217,
      "grad_norm": 3.545203685760498,
      "learning_rate": 1.0559075342465755e-05,
      "loss": 0.2499,
      "step": 7570
    },
    {
      "epoch": 3.2448630136986303,
      "grad_norm": 1.278502345085144,
      "learning_rate": 1.0533390410958904e-05,
      "loss": 0.2696,
      "step": 7580
    },
    {
      "epoch": 3.2491438356164384,
      "grad_norm": 0.48693403601646423,
      "learning_rate": 1.0507705479452056e-05,
      "loss": 0.2303,
      "step": 7590
    },
    {
      "epoch": 3.2534246575342465,
      "grad_norm": 0.3899189829826355,
      "learning_rate": 1.0482020547945206e-05,
      "loss": 0.2412,
      "step": 7600
    },
    {
      "epoch": 3.2577054794520546,
      "grad_norm": 45.24286651611328,
      "learning_rate": 1.0456335616438357e-05,
      "loss": 0.3799,
      "step": 7610
    },
    {
      "epoch": 3.261986301369863,
      "grad_norm": 1.2162933349609375,
      "learning_rate": 1.0430650684931507e-05,
      "loss": 0.3154,
      "step": 7620
    },
    {
      "epoch": 3.266267123287671,
      "grad_norm": 38.216453552246094,
      "learning_rate": 1.0404965753424658e-05,
      "loss": 0.3185,
      "step": 7630
    },
    {
      "epoch": 3.2705479452054793,
      "grad_norm": 1.193117380142212,
      "learning_rate": 1.037928082191781e-05,
      "loss": 0.3294,
      "step": 7640
    },
    {
      "epoch": 3.274828767123288,
      "grad_norm": 8.07707405090332,
      "learning_rate": 1.035359589041096e-05,
      "loss": 0.4048,
      "step": 7650
    },
    {
      "epoch": 3.279109589041096,
      "grad_norm": 5.338712692260742,
      "learning_rate": 1.0327910958904111e-05,
      "loss": 0.4375,
      "step": 7660
    },
    {
      "epoch": 3.283390410958904,
      "grad_norm": 54.6923713684082,
      "learning_rate": 1.0302226027397261e-05,
      "loss": 0.2273,
      "step": 7670
    },
    {
      "epoch": 3.287671232876712,
      "grad_norm": 20.747461318969727,
      "learning_rate": 1.0276541095890412e-05,
      "loss": 0.3662,
      "step": 7680
    },
    {
      "epoch": 3.2919520547945207,
      "grad_norm": 23.671314239501953,
      "learning_rate": 1.0250856164383562e-05,
      "loss": 0.413,
      "step": 7690
    },
    {
      "epoch": 3.296232876712329,
      "grad_norm": 34.659339904785156,
      "learning_rate": 1.0225171232876712e-05,
      "loss": 0.1763,
      "step": 7700
    },
    {
      "epoch": 3.300513698630137,
      "grad_norm": 65.43441009521484,
      "learning_rate": 1.0199486301369862e-05,
      "loss": 0.2297,
      "step": 7710
    },
    {
      "epoch": 3.3047945205479454,
      "grad_norm": 20.500192642211914,
      "learning_rate": 1.0173801369863013e-05,
      "loss": 0.4518,
      "step": 7720
    },
    {
      "epoch": 3.3090753424657535,
      "grad_norm": 6.227715492248535,
      "learning_rate": 1.0148116438356165e-05,
      "loss": 0.373,
      "step": 7730
    },
    {
      "epoch": 3.3133561643835616,
      "grad_norm": 3.9391250610351562,
      "learning_rate": 1.0122431506849314e-05,
      "loss": 0.2778,
      "step": 7740
    },
    {
      "epoch": 3.3176369863013697,
      "grad_norm": 27.359256744384766,
      "learning_rate": 1.0096746575342466e-05,
      "loss": 0.3674,
      "step": 7750
    },
    {
      "epoch": 3.3219178082191783,
      "grad_norm": 9.592819213867188,
      "learning_rate": 1.0071061643835616e-05,
      "loss": 0.1252,
      "step": 7760
    },
    {
      "epoch": 3.3261986301369864,
      "grad_norm": 20.95033836364746,
      "learning_rate": 1.0045376712328767e-05,
      "loss": 0.5756,
      "step": 7770
    },
    {
      "epoch": 3.3304794520547945,
      "grad_norm": 4.710381507873535,
      "learning_rate": 1.0019691780821917e-05,
      "loss": 0.2141,
      "step": 7780
    },
    {
      "epoch": 3.3347602739726026,
      "grad_norm": 12.658058166503906,
      "learning_rate": 9.994006849315068e-06,
      "loss": 0.2429,
      "step": 7790
    },
    {
      "epoch": 3.339041095890411,
      "grad_norm": 14.327632904052734,
      "learning_rate": 9.96832191780822e-06,
      "loss": 0.2644,
      "step": 7800
    },
    {
      "epoch": 3.343321917808219,
      "grad_norm": 12.171268463134766,
      "learning_rate": 9.94263698630137e-06,
      "loss": 0.1541,
      "step": 7810
    },
    {
      "epoch": 3.3476027397260273,
      "grad_norm": 9.421029090881348,
      "learning_rate": 9.916952054794521e-06,
      "loss": 0.4702,
      "step": 7820
    },
    {
      "epoch": 3.3518835616438354,
      "grad_norm": 6.440403938293457,
      "learning_rate": 9.891267123287671e-06,
      "loss": 0.1606,
      "step": 7830
    },
    {
      "epoch": 3.356164383561644,
      "grad_norm": 5.179156303405762,
      "learning_rate": 9.865582191780822e-06,
      "loss": 0.5463,
      "step": 7840
    },
    {
      "epoch": 3.360445205479452,
      "grad_norm": 5.020229339599609,
      "learning_rate": 9.839897260273972e-06,
      "loss": 0.2176,
      "step": 7850
    },
    {
      "epoch": 3.36472602739726,
      "grad_norm": 5.4852294921875,
      "learning_rate": 9.814212328767124e-06,
      "loss": 0.2476,
      "step": 7860
    },
    {
      "epoch": 3.3690068493150687,
      "grad_norm": 18.22146224975586,
      "learning_rate": 9.788527397260273e-06,
      "loss": 0.3027,
      "step": 7870
    },
    {
      "epoch": 3.3732876712328768,
      "grad_norm": 37.59115219116211,
      "learning_rate": 9.762842465753425e-06,
      "loss": 0.2633,
      "step": 7880
    },
    {
      "epoch": 3.377568493150685,
      "grad_norm": 13.462567329406738,
      "learning_rate": 9.737157534246576e-06,
      "loss": 0.273,
      "step": 7890
    },
    {
      "epoch": 3.381849315068493,
      "grad_norm": 22.006746292114258,
      "learning_rate": 9.711472602739726e-06,
      "loss": 0.2389,
      "step": 7900
    },
    {
      "epoch": 3.3861301369863015,
      "grad_norm": 12.479241371154785,
      "learning_rate": 9.685787671232878e-06,
      "loss": 0.1209,
      "step": 7910
    },
    {
      "epoch": 3.3904109589041096,
      "grad_norm": 152.48788452148438,
      "learning_rate": 9.660102739726027e-06,
      "loss": 0.1734,
      "step": 7920
    },
    {
      "epoch": 3.3946917808219177,
      "grad_norm": 13.324239730834961,
      "learning_rate": 9.634417808219179e-06,
      "loss": 0.3093,
      "step": 7930
    },
    {
      "epoch": 3.3989726027397262,
      "grad_norm": 20.294904708862305,
      "learning_rate": 9.608732876712329e-06,
      "loss": 0.3161,
      "step": 7940
    },
    {
      "epoch": 3.4032534246575343,
      "grad_norm": 12.497081756591797,
      "learning_rate": 9.58304794520548e-06,
      "loss": 0.5785,
      "step": 7950
    },
    {
      "epoch": 3.4075342465753424,
      "grad_norm": 0.7821828722953796,
      "learning_rate": 9.557363013698632e-06,
      "loss": 0.2902,
      "step": 7960
    },
    {
      "epoch": 3.4118150684931505,
      "grad_norm": 0.4167367219924927,
      "learning_rate": 9.531678082191781e-06,
      "loss": 0.1929,
      "step": 7970
    },
    {
      "epoch": 3.416095890410959,
      "grad_norm": 17.12537384033203,
      "learning_rate": 9.505993150684933e-06,
      "loss": 0.2245,
      "step": 7980
    },
    {
      "epoch": 3.420376712328767,
      "grad_norm": 13.147619247436523,
      "learning_rate": 9.480308219178083e-06,
      "loss": 0.1634,
      "step": 7990
    },
    {
      "epoch": 3.4246575342465753,
      "grad_norm": 11.846938133239746,
      "learning_rate": 9.454623287671234e-06,
      "loss": 0.3772,
      "step": 8000
    },
    {
      "epoch": 3.428938356164384,
      "grad_norm": 3.0257091522216797,
      "learning_rate": 9.428938356164384e-06,
      "loss": 0.2086,
      "step": 8010
    },
    {
      "epoch": 3.433219178082192,
      "grad_norm": 2.5437653064727783,
      "learning_rate": 9.403253424657535e-06,
      "loss": 0.255,
      "step": 8020
    },
    {
      "epoch": 3.4375,
      "grad_norm": 28.776121139526367,
      "learning_rate": 9.377568493150685e-06,
      "loss": 0.3507,
      "step": 8030
    },
    {
      "epoch": 3.441780821917808,
      "grad_norm": 0.3830064833164215,
      "learning_rate": 9.351883561643835e-06,
      "loss": 0.2417,
      "step": 8040
    },
    {
      "epoch": 3.446061643835616,
      "grad_norm": 12.966840744018555,
      "learning_rate": 9.326198630136986e-06,
      "loss": 0.2751,
      "step": 8050
    },
    {
      "epoch": 3.4503424657534247,
      "grad_norm": 6.816314697265625,
      "learning_rate": 9.300513698630136e-06,
      "loss": 0.4124,
      "step": 8060
    },
    {
      "epoch": 3.454623287671233,
      "grad_norm": 51.883758544921875,
      "learning_rate": 9.274828767123288e-06,
      "loss": 0.3565,
      "step": 8070
    },
    {
      "epoch": 3.458904109589041,
      "grad_norm": 0.5356184244155884,
      "learning_rate": 9.249143835616438e-06,
      "loss": 0.3085,
      "step": 8080
    },
    {
      "epoch": 3.4631849315068495,
      "grad_norm": 2.3621835708618164,
      "learning_rate": 9.223458904109589e-06,
      "loss": 0.346,
      "step": 8090
    },
    {
      "epoch": 3.4674657534246576,
      "grad_norm": 48.15780258178711,
      "learning_rate": 9.197773972602739e-06,
      "loss": 0.1855,
      "step": 8100
    },
    {
      "epoch": 3.4717465753424657,
      "grad_norm": 8.611063957214355,
      "learning_rate": 9.17208904109589e-06,
      "loss": 0.2737,
      "step": 8110
    },
    {
      "epoch": 3.4760273972602738,
      "grad_norm": 23.53715705871582,
      "learning_rate": 9.14640410958904e-06,
      "loss": 0.1896,
      "step": 8120
    },
    {
      "epoch": 3.4803082191780823,
      "grad_norm": 6.161003112792969,
      "learning_rate": 9.120719178082192e-06,
      "loss": 0.4102,
      "step": 8130
    },
    {
      "epoch": 3.4845890410958904,
      "grad_norm": 9.082889556884766,
      "learning_rate": 9.095034246575343e-06,
      "loss": 0.372,
      "step": 8140
    },
    {
      "epoch": 3.4888698630136985,
      "grad_norm": 12.153048515319824,
      "learning_rate": 9.069349315068493e-06,
      "loss": 0.334,
      "step": 8150
    },
    {
      "epoch": 3.493150684931507,
      "grad_norm": 2.001887559890747,
      "learning_rate": 9.043664383561644e-06,
      "loss": 0.3201,
      "step": 8160
    },
    {
      "epoch": 3.497431506849315,
      "grad_norm": 116.3150405883789,
      "learning_rate": 9.017979452054794e-06,
      "loss": 0.3589,
      "step": 8170
    },
    {
      "epoch": 3.5017123287671232,
      "grad_norm": 52.29755783081055,
      "learning_rate": 8.992294520547946e-06,
      "loss": 0.4155,
      "step": 8180
    },
    {
      "epoch": 3.5059931506849313,
      "grad_norm": 11.023022651672363,
      "learning_rate": 8.966609589041095e-06,
      "loss": 0.5234,
      "step": 8190
    },
    {
      "epoch": 3.51027397260274,
      "grad_norm": 0.25601813197135925,
      "learning_rate": 8.940924657534247e-06,
      "loss": 0.2006,
      "step": 8200
    },
    {
      "epoch": 3.514554794520548,
      "grad_norm": 0.36965659260749817,
      "learning_rate": 8.915239726027398e-06,
      "loss": 0.3039,
      "step": 8210
    },
    {
      "epoch": 3.518835616438356,
      "grad_norm": 2.2415034770965576,
      "learning_rate": 8.889554794520548e-06,
      "loss": 0.5059,
      "step": 8220
    },
    {
      "epoch": 3.5231164383561646,
      "grad_norm": 26.397769927978516,
      "learning_rate": 8.8638698630137e-06,
      "loss": 0.2169,
      "step": 8230
    },
    {
      "epoch": 3.5273972602739727,
      "grad_norm": 9.27592658996582,
      "learning_rate": 8.83818493150685e-06,
      "loss": 0.2426,
      "step": 8240
    },
    {
      "epoch": 3.531678082191781,
      "grad_norm": 11.107216835021973,
      "learning_rate": 8.8125e-06,
      "loss": 0.2185,
      "step": 8250
    },
    {
      "epoch": 3.535958904109589,
      "grad_norm": 16.632110595703125,
      "learning_rate": 8.78681506849315e-06,
      "loss": 0.3594,
      "step": 8260
    },
    {
      "epoch": 3.540239726027397,
      "grad_norm": 15.386643409729004,
      "learning_rate": 8.761130136986302e-06,
      "loss": 0.3542,
      "step": 8270
    },
    {
      "epoch": 3.5445205479452055,
      "grad_norm": 27.16507339477539,
      "learning_rate": 8.735445205479452e-06,
      "loss": 0.2457,
      "step": 8280
    },
    {
      "epoch": 3.5488013698630136,
      "grad_norm": 12.696066856384277,
      "learning_rate": 8.709760273972603e-06,
      "loss": 0.5331,
      "step": 8290
    },
    {
      "epoch": 3.553082191780822,
      "grad_norm": 12.512483596801758,
      "learning_rate": 8.684075342465755e-06,
      "loss": 0.2506,
      "step": 8300
    },
    {
      "epoch": 3.5573630136986303,
      "grad_norm": 17.614002227783203,
      "learning_rate": 8.658390410958905e-06,
      "loss": 0.194,
      "step": 8310
    },
    {
      "epoch": 3.5616438356164384,
      "grad_norm": 1.469028115272522,
      "learning_rate": 8.632705479452056e-06,
      "loss": 0.2502,
      "step": 8320
    },
    {
      "epoch": 3.5659246575342465,
      "grad_norm": 68.42003631591797,
      "learning_rate": 8.607020547945206e-06,
      "loss": 0.303,
      "step": 8330
    },
    {
      "epoch": 3.5702054794520546,
      "grad_norm": 11.750349044799805,
      "learning_rate": 8.581335616438357e-06,
      "loss": 0.4077,
      "step": 8340
    },
    {
      "epoch": 3.574486301369863,
      "grad_norm": 24.20992088317871,
      "learning_rate": 8.555650684931507e-06,
      "loss": 0.3598,
      "step": 8350
    },
    {
      "epoch": 3.578767123287671,
      "grad_norm": 7.49694299697876,
      "learning_rate": 8.529965753424659e-06,
      "loss": 0.2469,
      "step": 8360
    },
    {
      "epoch": 3.5830479452054793,
      "grad_norm": 7.4035515785217285,
      "learning_rate": 8.50428082191781e-06,
      "loss": 0.2645,
      "step": 8370
    },
    {
      "epoch": 3.587328767123288,
      "grad_norm": 0.9278361797332764,
      "learning_rate": 8.47859589041096e-06,
      "loss": 0.1795,
      "step": 8380
    },
    {
      "epoch": 3.591609589041096,
      "grad_norm": 4.147633075714111,
      "learning_rate": 8.45291095890411e-06,
      "loss": 0.2197,
      "step": 8390
    },
    {
      "epoch": 3.595890410958904,
      "grad_norm": 25.21886444091797,
      "learning_rate": 8.42722602739726e-06,
      "loss": 0.5148,
      "step": 8400
    },
    {
      "epoch": 3.600171232876712,
      "grad_norm": 19.8698673248291,
      "learning_rate": 8.40154109589041e-06,
      "loss": 0.3382,
      "step": 8410
    },
    {
      "epoch": 3.6044520547945207,
      "grad_norm": 36.53032684326172,
      "learning_rate": 8.37585616438356e-06,
      "loss": 0.3332,
      "step": 8420
    },
    {
      "epoch": 3.608732876712329,
      "grad_norm": 3.209751844406128,
      "learning_rate": 8.350171232876712e-06,
      "loss": 0.5036,
      "step": 8430
    },
    {
      "epoch": 3.613013698630137,
      "grad_norm": 6.430628776550293,
      "learning_rate": 8.324486301369862e-06,
      "loss": 0.1959,
      "step": 8440
    },
    {
      "epoch": 3.6172945205479454,
      "grad_norm": 13.456216812133789,
      "learning_rate": 8.298801369863013e-06,
      "loss": 0.1274,
      "step": 8450
    },
    {
      "epoch": 3.6215753424657535,
      "grad_norm": 1.2164396047592163,
      "learning_rate": 8.273116438356165e-06,
      "loss": 0.1302,
      "step": 8460
    },
    {
      "epoch": 3.6258561643835616,
      "grad_norm": 45.599151611328125,
      "learning_rate": 8.247431506849315e-06,
      "loss": 0.3542,
      "step": 8470
    },
    {
      "epoch": 3.6301369863013697,
      "grad_norm": 9.211711883544922,
      "learning_rate": 8.221746575342466e-06,
      "loss": 0.4665,
      "step": 8480
    },
    {
      "epoch": 3.634417808219178,
      "grad_norm": 14.82393741607666,
      "learning_rate": 8.196061643835616e-06,
      "loss": 0.2365,
      "step": 8490
    },
    {
      "epoch": 3.6386986301369864,
      "grad_norm": 6.135715007781982,
      "learning_rate": 8.170376712328767e-06,
      "loss": 0.1452,
      "step": 8500
    },
    {
      "epoch": 3.6429794520547945,
      "grad_norm": 9.791316986083984,
      "learning_rate": 8.144691780821917e-06,
      "loss": 0.2363,
      "step": 8510
    },
    {
      "epoch": 3.647260273972603,
      "grad_norm": 15.229523658752441,
      "learning_rate": 8.119006849315069e-06,
      "loss": 0.1048,
      "step": 8520
    },
    {
      "epoch": 3.651541095890411,
      "grad_norm": 11.406367301940918,
      "learning_rate": 8.09332191780822e-06,
      "loss": 0.3187,
      "step": 8530
    },
    {
      "epoch": 3.655821917808219,
      "grad_norm": 9.699492454528809,
      "learning_rate": 8.06763698630137e-06,
      "loss": 0.2805,
      "step": 8540
    },
    {
      "epoch": 3.6601027397260273,
      "grad_norm": 2.1376876831054688,
      "learning_rate": 8.041952054794521e-06,
      "loss": 0.5094,
      "step": 8550
    },
    {
      "epoch": 3.6643835616438354,
      "grad_norm": 14.164148330688477,
      "learning_rate": 8.016267123287671e-06,
      "loss": 0.2981,
      "step": 8560
    },
    {
      "epoch": 3.668664383561644,
      "grad_norm": 0.7013278007507324,
      "learning_rate": 7.990582191780823e-06,
      "loss": 0.1697,
      "step": 8570
    },
    {
      "epoch": 3.672945205479452,
      "grad_norm": 23.742740631103516,
      "learning_rate": 7.964897260273972e-06,
      "loss": 0.2727,
      "step": 8580
    },
    {
      "epoch": 3.67722602739726,
      "grad_norm": 2.900005340576172,
      "learning_rate": 7.939212328767124e-06,
      "loss": 0.1653,
      "step": 8590
    },
    {
      "epoch": 3.6815068493150687,
      "grad_norm": 17.61905860900879,
      "learning_rate": 7.913527397260274e-06,
      "loss": 0.2414,
      "step": 8600
    },
    {
      "epoch": 3.6857876712328768,
      "grad_norm": 20.375072479248047,
      "learning_rate": 7.887842465753425e-06,
      "loss": 0.2783,
      "step": 8610
    },
    {
      "epoch": 3.690068493150685,
      "grad_norm": 4.476799964904785,
      "learning_rate": 7.862157534246577e-06,
      "loss": 0.1962,
      "step": 8620
    },
    {
      "epoch": 3.694349315068493,
      "grad_norm": 9.163458824157715,
      "learning_rate": 7.836472602739726e-06,
      "loss": 0.5042,
      "step": 8630
    },
    {
      "epoch": 3.6986301369863015,
      "grad_norm": 29.27598762512207,
      "learning_rate": 7.810787671232878e-06,
      "loss": 0.1982,
      "step": 8640
    },
    {
      "epoch": 3.7029109589041096,
      "grad_norm": 12.692920684814453,
      "learning_rate": 7.785102739726028e-06,
      "loss": 0.4929,
      "step": 8650
    },
    {
      "epoch": 3.7071917808219177,
      "grad_norm": 4.835925102233887,
      "learning_rate": 7.759417808219179e-06,
      "loss": 0.356,
      "step": 8660
    },
    {
      "epoch": 3.7114726027397262,
      "grad_norm": 24.50899887084961,
      "learning_rate": 7.733732876712329e-06,
      "loss": 0.1587,
      "step": 8670
    },
    {
      "epoch": 3.7157534246575343,
      "grad_norm": 4.5174407958984375,
      "learning_rate": 7.70804794520548e-06,
      "loss": 0.1929,
      "step": 8680
    },
    {
      "epoch": 3.7200342465753424,
      "grad_norm": 8.005946159362793,
      "learning_rate": 7.682363013698632e-06,
      "loss": 0.3259,
      "step": 8690
    },
    {
      "epoch": 3.7243150684931505,
      "grad_norm": 16.85796356201172,
      "learning_rate": 7.656678082191782e-06,
      "loss": 0.3221,
      "step": 8700
    },
    {
      "epoch": 3.728595890410959,
      "grad_norm": 51.40901565551758,
      "learning_rate": 7.630993150684933e-06,
      "loss": 0.2285,
      "step": 8710
    },
    {
      "epoch": 3.732876712328767,
      "grad_norm": 3.8901660442352295,
      "learning_rate": 7.605308219178082e-06,
      "loss": 0.1929,
      "step": 8720
    },
    {
      "epoch": 3.7371575342465753,
      "grad_norm": 70.35617065429688,
      "learning_rate": 7.5796232876712335e-06,
      "loss": 0.2971,
      "step": 8730
    },
    {
      "epoch": 3.741438356164384,
      "grad_norm": 5.6947784423828125,
      "learning_rate": 7.553938356164383e-06,
      "loss": 0.1457,
      "step": 8740
    },
    {
      "epoch": 3.745719178082192,
      "grad_norm": 0.20936478674411774,
      "learning_rate": 7.528253424657535e-06,
      "loss": 0.2409,
      "step": 8750
    },
    {
      "epoch": 3.75,
      "grad_norm": 50.182395935058594,
      "learning_rate": 7.5025684931506845e-06,
      "loss": 0.2995,
      "step": 8760
    },
    {
      "epoch": 3.754280821917808,
      "grad_norm": 29.022953033447266,
      "learning_rate": 7.476883561643836e-06,
      "loss": 0.4079,
      "step": 8770
    },
    {
      "epoch": 3.758561643835616,
      "grad_norm": 10.919440269470215,
      "learning_rate": 7.451198630136987e-06,
      "loss": 0.4291,
      "step": 8780
    },
    {
      "epoch": 3.7628424657534247,
      "grad_norm": 6.352647304534912,
      "learning_rate": 7.425513698630137e-06,
      "loss": 0.2382,
      "step": 8790
    },
    {
      "epoch": 3.767123287671233,
      "grad_norm": 9.78495979309082,
      "learning_rate": 7.399828767123288e-06,
      "loss": 0.3048,
      "step": 8800
    },
    {
      "epoch": 3.771404109589041,
      "grad_norm": 6.0058674812316895,
      "learning_rate": 7.374143835616439e-06,
      "loss": 0.2248,
      "step": 8810
    },
    {
      "epoch": 3.7756849315068495,
      "grad_norm": 6.899167537689209,
      "learning_rate": 7.348458904109589e-06,
      "loss": 0.2982,
      "step": 8820
    },
    {
      "epoch": 3.7799657534246576,
      "grad_norm": 9.263936042785645,
      "learning_rate": 7.32277397260274e-06,
      "loss": 0.2282,
      "step": 8830
    },
    {
      "epoch": 3.7842465753424657,
      "grad_norm": 33.27027130126953,
      "learning_rate": 7.29708904109589e-06,
      "loss": 0.5119,
      "step": 8840
    },
    {
      "epoch": 3.7885273972602738,
      "grad_norm": 2.8730015754699707,
      "learning_rate": 7.271404109589041e-06,
      "loss": 0.1997,
      "step": 8850
    },
    {
      "epoch": 3.7928082191780823,
      "grad_norm": 9.00295352935791,
      "learning_rate": 7.245719178082192e-06,
      "loss": 0.1579,
      "step": 8860
    },
    {
      "epoch": 3.7970890410958904,
      "grad_norm": 9.100250244140625,
      "learning_rate": 7.220034246575342e-06,
      "loss": 0.4164,
      "step": 8870
    },
    {
      "epoch": 3.8013698630136985,
      "grad_norm": 3.027723789215088,
      "learning_rate": 7.194349315068493e-06,
      "loss": 0.3717,
      "step": 8880
    },
    {
      "epoch": 3.805650684931507,
      "grad_norm": 6.124817371368408,
      "learning_rate": 7.1686643835616436e-06,
      "loss": 0.2146,
      "step": 8890
    },
    {
      "epoch": 3.809931506849315,
      "grad_norm": 10.1585111618042,
      "learning_rate": 7.142979452054795e-06,
      "loss": 0.2713,
      "step": 8900
    },
    {
      "epoch": 3.8142123287671232,
      "grad_norm": 6.037136077880859,
      "learning_rate": 7.117294520547946e-06,
      "loss": 0.2764,
      "step": 8910
    },
    {
      "epoch": 3.8184931506849313,
      "grad_norm": 26.277353286743164,
      "learning_rate": 7.091609589041096e-06,
      "loss": 0.1872,
      "step": 8920
    },
    {
      "epoch": 3.82277397260274,
      "grad_norm": 16.247573852539062,
      "learning_rate": 7.065924657534247e-06,
      "loss": 0.4083,
      "step": 8930
    },
    {
      "epoch": 3.827054794520548,
      "grad_norm": 18.19317054748535,
      "learning_rate": 7.0402397260273976e-06,
      "loss": 0.2273,
      "step": 8940
    },
    {
      "epoch": 3.831335616438356,
      "grad_norm": 0.34188663959503174,
      "learning_rate": 7.014554794520548e-06,
      "loss": 0.307,
      "step": 8950
    },
    {
      "epoch": 3.8356164383561646,
      "grad_norm": 26.29466438293457,
      "learning_rate": 6.988869863013699e-06,
      "loss": 0.4038,
      "step": 8960
    },
    {
      "epoch": 3.8398972602739727,
      "grad_norm": 10.891845703125,
      "learning_rate": 6.9631849315068494e-06,
      "loss": 0.2545,
      "step": 8970
    },
    {
      "epoch": 3.844178082191781,
      "grad_norm": 27.62042808532715,
      "learning_rate": 6.937500000000001e-06,
      "loss": 0.3626,
      "step": 8980
    },
    {
      "epoch": 3.848458904109589,
      "grad_norm": 14.880117416381836,
      "learning_rate": 6.911815068493151e-06,
      "loss": 0.1803,
      "step": 8990
    },
    {
      "epoch": 3.852739726027397,
      "grad_norm": 2.8737165927886963,
      "learning_rate": 6.886130136986301e-06,
      "loss": 0.2248,
      "step": 9000
    },
    {
      "epoch": 3.8570205479452055,
      "grad_norm": 4.585152626037598,
      "learning_rate": 6.860445205479452e-06,
      "loss": 0.2883,
      "step": 9010
    },
    {
      "epoch": 3.8613013698630136,
      "grad_norm": 10.190000534057617,
      "learning_rate": 6.834760273972603e-06,
      "loss": 0.2805,
      "step": 9020
    },
    {
      "epoch": 3.865582191780822,
      "grad_norm": 19.219545364379883,
      "learning_rate": 6.809075342465753e-06,
      "loss": 0.126,
      "step": 9030
    },
    {
      "epoch": 3.8698630136986303,
      "grad_norm": 0.668126106262207,
      "learning_rate": 6.783390410958904e-06,
      "loss": 0.1586,
      "step": 9040
    },
    {
      "epoch": 3.8741438356164384,
      "grad_norm": 7.722413063049316,
      "learning_rate": 6.7577054794520545e-06,
      "loss": 0.2885,
      "step": 9050
    },
    {
      "epoch": 3.8784246575342465,
      "grad_norm": 11.343769073486328,
      "learning_rate": 6.732020547945206e-06,
      "loss": 0.2715,
      "step": 9060
    },
    {
      "epoch": 3.8827054794520546,
      "grad_norm": 20.136980056762695,
      "learning_rate": 6.706335616438357e-06,
      "loss": 0.3489,
      "step": 9070
    },
    {
      "epoch": 3.886986301369863,
      "grad_norm": 12.46761703491211,
      "learning_rate": 6.680650684931507e-06,
      "loss": 0.2695,
      "step": 9080
    },
    {
      "epoch": 3.891267123287671,
      "grad_norm": 19.354921340942383,
      "learning_rate": 6.654965753424658e-06,
      "loss": 0.2415,
      "step": 9090
    },
    {
      "epoch": 3.8955479452054793,
      "grad_norm": 12.660961151123047,
      "learning_rate": 6.6292808219178085e-06,
      "loss": 0.2836,
      "step": 9100
    },
    {
      "epoch": 3.899828767123288,
      "grad_norm": 0.460790753364563,
      "learning_rate": 6.603595890410959e-06,
      "loss": 0.3012,
      "step": 9110
    },
    {
      "epoch": 3.904109589041096,
      "grad_norm": 18.414262771606445,
      "learning_rate": 6.57791095890411e-06,
      "loss": 0.2342,
      "step": 9120
    },
    {
      "epoch": 3.908390410958904,
      "grad_norm": 0.2439306378364563,
      "learning_rate": 6.55222602739726e-06,
      "loss": 0.4888,
      "step": 9130
    },
    {
      "epoch": 3.912671232876712,
      "grad_norm": 8.423267364501953,
      "learning_rate": 6.526541095890412e-06,
      "loss": 0.4188,
      "step": 9140
    },
    {
      "epoch": 3.9169520547945207,
      "grad_norm": 10.939461708068848,
      "learning_rate": 6.5008561643835625e-06,
      "loss": 0.272,
      "step": 9150
    },
    {
      "epoch": 3.921232876712329,
      "grad_norm": 32.72812271118164,
      "learning_rate": 6.475171232876712e-06,
      "loss": 0.3769,
      "step": 9160
    },
    {
      "epoch": 3.925513698630137,
      "grad_norm": 5.512265682220459,
      "learning_rate": 6.449486301369863e-06,
      "loss": 0.1973,
      "step": 9170
    },
    {
      "epoch": 3.9297945205479454,
      "grad_norm": 19.151334762573242,
      "learning_rate": 6.4238013698630135e-06,
      "loss": 0.3363,
      "step": 9180
    },
    {
      "epoch": 3.9340753424657535,
      "grad_norm": 2.6266634464263916,
      "learning_rate": 6.398116438356164e-06,
      "loss": 0.2465,
      "step": 9190
    },
    {
      "epoch": 3.9383561643835616,
      "grad_norm": 23.141407012939453,
      "learning_rate": 6.372431506849315e-06,
      "loss": 0.4106,
      "step": 9200
    },
    {
      "epoch": 3.9426369863013697,
      "grad_norm": 24.50872039794922,
      "learning_rate": 6.346746575342465e-06,
      "loss": 0.2143,
      "step": 9210
    },
    {
      "epoch": 3.946917808219178,
      "grad_norm": 16.733015060424805,
      "learning_rate": 6.321061643835617e-06,
      "loss": 0.2455,
      "step": 9220
    },
    {
      "epoch": 3.9511986301369864,
      "grad_norm": 19.49696159362793,
      "learning_rate": 6.2953767123287675e-06,
      "loss": 0.4581,
      "step": 9230
    },
    {
      "epoch": 3.9554794520547945,
      "grad_norm": 25.660322189331055,
      "learning_rate": 6.269691780821918e-06,
      "loss": 0.3799,
      "step": 9240
    },
    {
      "epoch": 3.959760273972603,
      "grad_norm": 0.47356098890304565,
      "learning_rate": 6.244006849315069e-06,
      "loss": 0.2087,
      "step": 9250
    },
    {
      "epoch": 3.964041095890411,
      "grad_norm": 18.898351669311523,
      "learning_rate": 6.218321917808219e-06,
      "loss": 0.3897,
      "step": 9260
    },
    {
      "epoch": 3.968321917808219,
      "grad_norm": 5.86472225189209,
      "learning_rate": 6.19263698630137e-06,
      "loss": 0.1598,
      "step": 9270
    },
    {
      "epoch": 3.9726027397260273,
      "grad_norm": 0.9436332583427429,
      "learning_rate": 6.166952054794521e-06,
      "loss": 0.3029,
      "step": 9280
    },
    {
      "epoch": 3.9768835616438354,
      "grad_norm": 45.263694763183594,
      "learning_rate": 6.141267123287671e-06,
      "loss": 0.3032,
      "step": 9290
    },
    {
      "epoch": 3.981164383561644,
      "grad_norm": 20.323726654052734,
      "learning_rate": 6.115582191780823e-06,
      "loss": 0.2809,
      "step": 9300
    },
    {
      "epoch": 3.985445205479452,
      "grad_norm": 5.8305792808532715,
      "learning_rate": 6.089897260273973e-06,
      "loss": 0.1658,
      "step": 9310
    },
    {
      "epoch": 3.98972602739726,
      "grad_norm": 20.195913314819336,
      "learning_rate": 6.064212328767124e-06,
      "loss": 0.2605,
      "step": 9320
    },
    {
      "epoch": 3.9940068493150687,
      "grad_norm": 16.716651916503906,
      "learning_rate": 6.038527397260274e-06,
      "loss": 0.1708,
      "step": 9330
    },
    {
      "epoch": 3.9982876712328768,
      "grad_norm": 14.937767028808594,
      "learning_rate": 6.012842465753424e-06,
      "loss": 0.1526,
      "step": 9340
    },
    {
      "epoch": 4.0,
      "eval_f1": 0.7049513838453577,
      "eval_loss": 0.7717663049697876,
      "eval_precision": 0.7134840217793632,
      "eval_recall": 0.7067028283611003,
      "eval_runtime": 546.6714,
      "eval_samples_per_second": 8.177,
      "eval_steps_per_second": 1.023,
      "step": 9344
    },
    {
      "epoch": 4.002568493150685,
      "grad_norm": 4.7793426513671875,
      "learning_rate": 5.987157534246575e-06,
      "loss": 0.165,
      "step": 9350
    },
    {
      "epoch": 4.006849315068493,
      "grad_norm": 12.650493621826172,
      "learning_rate": 5.961472602739726e-06,
      "loss": 0.0771,
      "step": 9360
    },
    {
      "epoch": 4.011130136986301,
      "grad_norm": 40.53340148925781,
      "learning_rate": 5.935787671232876e-06,
      "loss": 0.22,
      "step": 9370
    },
    {
      "epoch": 4.015410958904109,
      "grad_norm": 0.5072563290596008,
      "learning_rate": 5.910102739726028e-06,
      "loss": 0.1538,
      "step": 9380
    },
    {
      "epoch": 4.019691780821918,
      "grad_norm": 29.628332138061523,
      "learning_rate": 5.884417808219178e-06,
      "loss": 0.1844,
      "step": 9390
    },
    {
      "epoch": 4.023972602739726,
      "grad_norm": 12.182629585266113,
      "learning_rate": 5.858732876712329e-06,
      "loss": 0.2631,
      "step": 9400
    },
    {
      "epoch": 4.028253424657534,
      "grad_norm": 6.094665050506592,
      "learning_rate": 5.83304794520548e-06,
      "loss": 0.1223,
      "step": 9410
    },
    {
      "epoch": 4.032534246575342,
      "grad_norm": 3.9770591259002686,
      "learning_rate": 5.80736301369863e-06,
      "loss": 0.1011,
      "step": 9420
    },
    {
      "epoch": 4.0368150684931505,
      "grad_norm": 1.8653903007507324,
      "learning_rate": 5.781678082191781e-06,
      "loss": 0.2361,
      "step": 9430
    },
    {
      "epoch": 4.041095890410959,
      "grad_norm": 2.969468832015991,
      "learning_rate": 5.7559931506849316e-06,
      "loss": 0.297,
      "step": 9440
    },
    {
      "epoch": 4.045376712328767,
      "grad_norm": 23.146228790283203,
      "learning_rate": 5.730308219178082e-06,
      "loss": 0.1677,
      "step": 9450
    },
    {
      "epoch": 4.049657534246576,
      "grad_norm": 6.268233776092529,
      "learning_rate": 5.704623287671234e-06,
      "loss": 0.1658,
      "step": 9460
    },
    {
      "epoch": 4.053938356164384,
      "grad_norm": 19.08185386657715,
      "learning_rate": 5.678938356164384e-06,
      "loss": 0.191,
      "step": 9470
    },
    {
      "epoch": 4.058219178082192,
      "grad_norm": 13.43994140625,
      "learning_rate": 5.653253424657535e-06,
      "loss": 0.3268,
      "step": 9480
    },
    {
      "epoch": 4.0625,
      "grad_norm": 0.580450713634491,
      "learning_rate": 5.6275684931506855e-06,
      "loss": 0.1776,
      "step": 9490
    },
    {
      "epoch": 4.066780821917808,
      "grad_norm": 6.753300666809082,
      "learning_rate": 5.601883561643835e-06,
      "loss": 0.1936,
      "step": 9500
    },
    {
      "epoch": 4.071061643835616,
      "grad_norm": 38.25286102294922,
      "learning_rate": 5.576198630136986e-06,
      "loss": 0.2507,
      "step": 9510
    },
    {
      "epoch": 4.075342465753424,
      "grad_norm": 0.6446570158004761,
      "learning_rate": 5.550513698630137e-06,
      "loss": 0.1826,
      "step": 9520
    },
    {
      "epoch": 4.079623287671233,
      "grad_norm": 24.206806182861328,
      "learning_rate": 5.524828767123287e-06,
      "loss": 0.198,
      "step": 9530
    },
    {
      "epoch": 4.083904109589041,
      "grad_norm": 7.342499256134033,
      "learning_rate": 5.499143835616439e-06,
      "loss": 0.0613,
      "step": 9540
    },
    {
      "epoch": 4.0881849315068495,
      "grad_norm": 10.100362777709961,
      "learning_rate": 5.473458904109589e-06,
      "loss": 0.2099,
      "step": 9550
    },
    {
      "epoch": 4.092465753424658,
      "grad_norm": 28.4718017578125,
      "learning_rate": 5.44777397260274e-06,
      "loss": 0.1871,
      "step": 9560
    },
    {
      "epoch": 4.096746575342466,
      "grad_norm": 6.7092742919921875,
      "learning_rate": 5.422089041095891e-06,
      "loss": 0.1196,
      "step": 9570
    },
    {
      "epoch": 4.101027397260274,
      "grad_norm": 15.569928169250488,
      "learning_rate": 5.396404109589041e-06,
      "loss": 0.2284,
      "step": 9580
    },
    {
      "epoch": 4.105308219178082,
      "grad_norm": 15.121779441833496,
      "learning_rate": 5.370719178082192e-06,
      "loss": 0.187,
      "step": 9590
    },
    {
      "epoch": 4.109589041095891,
      "grad_norm": 50.028106689453125,
      "learning_rate": 5.3450342465753425e-06,
      "loss": 0.3552,
      "step": 9600
    },
    {
      "epoch": 4.113869863013699,
      "grad_norm": 30.815244674682617,
      "learning_rate": 5.319349315068493e-06,
      "loss": 0.1735,
      "step": 9610
    },
    {
      "epoch": 4.118150684931507,
      "grad_norm": 3.693969249725342,
      "learning_rate": 5.293664383561644e-06,
      "loss": 0.2364,
      "step": 9620
    },
    {
      "epoch": 4.122431506849315,
      "grad_norm": 4.623680114746094,
      "learning_rate": 5.267979452054795e-06,
      "loss": 0.3283,
      "step": 9630
    },
    {
      "epoch": 4.126712328767123,
      "grad_norm": 8.579291343688965,
      "learning_rate": 5.242294520547946e-06,
      "loss": 0.4173,
      "step": 9640
    },
    {
      "epoch": 4.130993150684931,
      "grad_norm": 7.265303611755371,
      "learning_rate": 5.2166095890410965e-06,
      "loss": 0.2383,
      "step": 9650
    },
    {
      "epoch": 4.135273972602739,
      "grad_norm": 3.236719846725464,
      "learning_rate": 5.190924657534247e-06,
      "loss": 0.1421,
      "step": 9660
    },
    {
      "epoch": 4.1395547945205475,
      "grad_norm": 32.770389556884766,
      "learning_rate": 5.165239726027398e-06,
      "loss": 0.2264,
      "step": 9670
    },
    {
      "epoch": 4.1438356164383565,
      "grad_norm": 16.655492782592773,
      "learning_rate": 5.1395547945205475e-06,
      "loss": 0.1833,
      "step": 9680
    },
    {
      "epoch": 4.148116438356165,
      "grad_norm": 14.983390808105469,
      "learning_rate": 5.113869863013698e-06,
      "loss": 0.2185,
      "step": 9690
    },
    {
      "epoch": 4.152397260273973,
      "grad_norm": 45.233768463134766,
      "learning_rate": 5.088184931506849e-06,
      "loss": 0.2468,
      "step": 9700
    },
    {
      "epoch": 4.156678082191781,
      "grad_norm": 21.014373779296875,
      "learning_rate": 5.0625e-06,
      "loss": 0.2291,
      "step": 9710
    },
    {
      "epoch": 4.160958904109589,
      "grad_norm": 18.75553321838379,
      "learning_rate": 5.036815068493151e-06,
      "loss": 0.1554,
      "step": 9720
    },
    {
      "epoch": 4.165239726027397,
      "grad_norm": 26.396137237548828,
      "learning_rate": 5.0111301369863015e-06,
      "loss": 0.2586,
      "step": 9730
    },
    {
      "epoch": 4.169520547945205,
      "grad_norm": 8.408504486083984,
      "learning_rate": 4.985445205479452e-06,
      "loss": 0.2895,
      "step": 9740
    },
    {
      "epoch": 4.173801369863014,
      "grad_norm": 17.43798828125,
      "learning_rate": 4.959760273972603e-06,
      "loss": 0.32,
      "step": 9750
    },
    {
      "epoch": 4.178082191780822,
      "grad_norm": 19.150596618652344,
      "learning_rate": 4.934075342465753e-06,
      "loss": 0.3623,
      "step": 9760
    },
    {
      "epoch": 4.18236301369863,
      "grad_norm": 0.2289280891418457,
      "learning_rate": 4.908390410958904e-06,
      "loss": 0.1931,
      "step": 9770
    },
    {
      "epoch": 4.186643835616438,
      "grad_norm": 8.339865684509277,
      "learning_rate": 4.882705479452055e-06,
      "loss": 0.1556,
      "step": 9780
    },
    {
      "epoch": 4.1909246575342465,
      "grad_norm": 15.085919380187988,
      "learning_rate": 4.857020547945206e-06,
      "loss": 0.1529,
      "step": 9790
    },
    {
      "epoch": 4.195205479452055,
      "grad_norm": 17.470748901367188,
      "learning_rate": 4.831335616438357e-06,
      "loss": 0.2614,
      "step": 9800
    },
    {
      "epoch": 4.199486301369863,
      "grad_norm": 7.364249229431152,
      "learning_rate": 4.805650684931507e-06,
      "loss": 0.1977,
      "step": 9810
    },
    {
      "epoch": 4.203767123287672,
      "grad_norm": 6.0131402015686035,
      "learning_rate": 4.779965753424658e-06,
      "loss": 0.2422,
      "step": 9820
    },
    {
      "epoch": 4.20804794520548,
      "grad_norm": 118.00807189941406,
      "learning_rate": 4.754280821917809e-06,
      "loss": 0.3951,
      "step": 9830
    },
    {
      "epoch": 4.212328767123288,
      "grad_norm": 0.5878066420555115,
      "learning_rate": 4.728595890410959e-06,
      "loss": 0.1764,
      "step": 9840
    },
    {
      "epoch": 4.216609589041096,
      "grad_norm": 19.58621597290039,
      "learning_rate": 4.702910958904109e-06,
      "loss": 0.1394,
      "step": 9850
    },
    {
      "epoch": 4.220890410958904,
      "grad_norm": 0.5482531189918518,
      "learning_rate": 4.67722602739726e-06,
      "loss": 0.0625,
      "step": 9860
    },
    {
      "epoch": 4.225171232876712,
      "grad_norm": 5.41763162612915,
      "learning_rate": 4.651541095890411e-06,
      "loss": 0.1098,
      "step": 9870
    },
    {
      "epoch": 4.22945205479452,
      "grad_norm": 23.904165267944336,
      "learning_rate": 4.625856164383562e-06,
      "loss": 0.3386,
      "step": 9880
    },
    {
      "epoch": 4.233732876712328,
      "grad_norm": 0.5221856832504272,
      "learning_rate": 4.600171232876712e-06,
      "loss": 0.1109,
      "step": 9890
    },
    {
      "epoch": 4.238013698630137,
      "grad_norm": 0.8018145561218262,
      "learning_rate": 4.574486301369863e-06,
      "loss": 0.1007,
      "step": 9900
    },
    {
      "epoch": 4.242294520547945,
      "grad_norm": 0.2165863811969757,
      "learning_rate": 4.548801369863014e-06,
      "loss": 0.2352,
      "step": 9910
    },
    {
      "epoch": 4.2465753424657535,
      "grad_norm": 8.0960054397583,
      "learning_rate": 4.523116438356164e-06,
      "loss": 0.3218,
      "step": 9920
    },
    {
      "epoch": 4.250856164383562,
      "grad_norm": 158.24400329589844,
      "learning_rate": 4.497431506849315e-06,
      "loss": 0.4425,
      "step": 9930
    },
    {
      "epoch": 4.25513698630137,
      "grad_norm": 26.16530990600586,
      "learning_rate": 4.4717465753424656e-06,
      "loss": 0.1826,
      "step": 9940
    },
    {
      "epoch": 4.259417808219178,
      "grad_norm": 0.41072526574134827,
      "learning_rate": 4.446061643835617e-06,
      "loss": 0.1635,
      "step": 9950
    },
    {
      "epoch": 4.263698630136986,
      "grad_norm": 14.362797737121582,
      "learning_rate": 4.420376712328768e-06,
      "loss": 0.1098,
      "step": 9960
    },
    {
      "epoch": 4.267979452054795,
      "grad_norm": 0.05606302618980408,
      "learning_rate": 4.394691780821918e-06,
      "loss": 0.2447,
      "step": 9970
    },
    {
      "epoch": 4.272260273972603,
      "grad_norm": 3.875354051589966,
      "learning_rate": 4.369006849315069e-06,
      "loss": 0.2911,
      "step": 9980
    },
    {
      "epoch": 4.276541095890411,
      "grad_norm": 0.37333154678344727,
      "learning_rate": 4.3433219178082195e-06,
      "loss": 0.1612,
      "step": 9990
    },
    {
      "epoch": 4.280821917808219,
      "grad_norm": 13.591076850891113,
      "learning_rate": 4.31763698630137e-06,
      "loss": 0.2134,
      "step": 10000
    },
    {
      "epoch": 4.285102739726027,
      "grad_norm": 24.085161209106445,
      "learning_rate": 4.291952054794521e-06,
      "loss": 0.309,
      "step": 10010
    },
    {
      "epoch": 4.289383561643835,
      "grad_norm": 0.9975661635398865,
      "learning_rate": 4.266267123287671e-06,
      "loss": 0.2012,
      "step": 10020
    },
    {
      "epoch": 4.2936643835616435,
      "grad_norm": 16.550975799560547,
      "learning_rate": 4.240582191780822e-06,
      "loss": 0.3034,
      "step": 10030
    },
    {
      "epoch": 4.2979452054794525,
      "grad_norm": 8.150876998901367,
      "learning_rate": 4.214897260273973e-06,
      "loss": 0.0863,
      "step": 10040
    },
    {
      "epoch": 4.302226027397261,
      "grad_norm": 0.09430056065320969,
      "learning_rate": 4.189212328767123e-06,
      "loss": 0.1482,
      "step": 10050
    },
    {
      "epoch": 4.306506849315069,
      "grad_norm": 11.17885684967041,
      "learning_rate": 4.163527397260274e-06,
      "loss": 0.1613,
      "step": 10060
    },
    {
      "epoch": 4.310787671232877,
      "grad_norm": 3.594573974609375,
      "learning_rate": 4.137842465753425e-06,
      "loss": 0.1516,
      "step": 10070
    },
    {
      "epoch": 4.315068493150685,
      "grad_norm": 34.515464782714844,
      "learning_rate": 4.112157534246575e-06,
      "loss": 0.3555,
      "step": 10080
    },
    {
      "epoch": 4.319349315068493,
      "grad_norm": 6.706461429595947,
      "learning_rate": 4.086472602739726e-06,
      "loss": 0.2331,
      "step": 10090
    },
    {
      "epoch": 4.323630136986301,
      "grad_norm": 2.2460203170776367,
      "learning_rate": 4.0607876712328765e-06,
      "loss": 0.1396,
      "step": 10100
    },
    {
      "epoch": 4.327910958904109,
      "grad_norm": 0.06963031738996506,
      "learning_rate": 4.035102739726028e-06,
      "loss": 0.1197,
      "step": 10110
    },
    {
      "epoch": 4.332191780821918,
      "grad_norm": 7.92473840713501,
      "learning_rate": 4.0094178082191786e-06,
      "loss": 0.2132,
      "step": 10120
    },
    {
      "epoch": 4.336472602739726,
      "grad_norm": 0.621221661567688,
      "learning_rate": 3.983732876712329e-06,
      "loss": 0.2281,
      "step": 10130
    },
    {
      "epoch": 4.340753424657534,
      "grad_norm": 12.138370513916016,
      "learning_rate": 3.95804794520548e-06,
      "loss": 0.2767,
      "step": 10140
    },
    {
      "epoch": 4.345034246575342,
      "grad_norm": 0.179423525929451,
      "learning_rate": 3.9323630136986305e-06,
      "loss": 0.274,
      "step": 10150
    },
    {
      "epoch": 4.3493150684931505,
      "grad_norm": 16.458646774291992,
      "learning_rate": 3.906678082191781e-06,
      "loss": 0.1658,
      "step": 10160
    },
    {
      "epoch": 4.353595890410959,
      "grad_norm": 18.540170669555664,
      "learning_rate": 3.880993150684932e-06,
      "loss": 0.2274,
      "step": 10170
    },
    {
      "epoch": 4.357876712328767,
      "grad_norm": 0.5243396162986755,
      "learning_rate": 3.855308219178082e-06,
      "loss": 0.1733,
      "step": 10180
    },
    {
      "epoch": 4.362157534246576,
      "grad_norm": 1.2151838541030884,
      "learning_rate": 3.829623287671233e-06,
      "loss": 0.0499,
      "step": 10190
    },
    {
      "epoch": 4.366438356164384,
      "grad_norm": 0.43775713443756104,
      "learning_rate": 3.803938356164384e-06,
      "loss": 0.242,
      "step": 10200
    },
    {
      "epoch": 4.370719178082192,
      "grad_norm": 9.7334566116333,
      "learning_rate": 3.7782534246575347e-06,
      "loss": 0.1472,
      "step": 10210
    },
    {
      "epoch": 4.375,
      "grad_norm": 10.26303482055664,
      "learning_rate": 3.7525684931506853e-06,
      "loss": 0.3993,
      "step": 10220
    },
    {
      "epoch": 4.379280821917808,
      "grad_norm": 7.18704891204834,
      "learning_rate": 3.7268835616438355e-06,
      "loss": 0.2582,
      "step": 10230
    },
    {
      "epoch": 4.383561643835616,
      "grad_norm": 8.97802734375,
      "learning_rate": 3.701198630136986e-06,
      "loss": 0.2763,
      "step": 10240
    },
    {
      "epoch": 4.387842465753424,
      "grad_norm": 14.962730407714844,
      "learning_rate": 3.675513698630137e-06,
      "loss": 0.1699,
      "step": 10250
    },
    {
      "epoch": 4.392123287671233,
      "grad_norm": 44.0824089050293,
      "learning_rate": 3.649828767123288e-06,
      "loss": 0.3223,
      "step": 10260
    },
    {
      "epoch": 4.396404109589041,
      "grad_norm": 2.992832899093628,
      "learning_rate": 3.6241438356164384e-06,
      "loss": 0.3718,
      "step": 10270
    },
    {
      "epoch": 4.4006849315068495,
      "grad_norm": 33.57518005371094,
      "learning_rate": 3.598458904109589e-06,
      "loss": 0.3012,
      "step": 10280
    },
    {
      "epoch": 4.404965753424658,
      "grad_norm": 34.90806579589844,
      "learning_rate": 3.57277397260274e-06,
      "loss": 0.2673,
      "step": 10290
    },
    {
      "epoch": 4.409246575342466,
      "grad_norm": 7.058717727661133,
      "learning_rate": 3.5470890410958907e-06,
      "loss": 0.3742,
      "step": 10300
    },
    {
      "epoch": 4.413527397260274,
      "grad_norm": 4.043357849121094,
      "learning_rate": 3.5214041095890414e-06,
      "loss": 0.241,
      "step": 10310
    },
    {
      "epoch": 4.417808219178082,
      "grad_norm": 29.866592407226562,
      "learning_rate": 3.4957191780821916e-06,
      "loss": 0.1711,
      "step": 10320
    },
    {
      "epoch": 4.422089041095891,
      "grad_norm": 4.154275417327881,
      "learning_rate": 3.470034246575342e-06,
      "loss": 0.2323,
      "step": 10330
    },
    {
      "epoch": 4.426369863013699,
      "grad_norm": 2.5946431159973145,
      "learning_rate": 3.4443493150684933e-06,
      "loss": 0.0807,
      "step": 10340
    },
    {
      "epoch": 4.430650684931507,
      "grad_norm": 4.712759494781494,
      "learning_rate": 3.418664383561644e-06,
      "loss": 0.3648,
      "step": 10350
    },
    {
      "epoch": 4.434931506849315,
      "grad_norm": 5.383009910583496,
      "learning_rate": 3.3929794520547945e-06,
      "loss": 0.1701,
      "step": 10360
    },
    {
      "epoch": 4.439212328767123,
      "grad_norm": 14.927237510681152,
      "learning_rate": 3.367294520547945e-06,
      "loss": 0.2632,
      "step": 10370
    },
    {
      "epoch": 4.443493150684931,
      "grad_norm": 12.380231857299805,
      "learning_rate": 3.341609589041096e-06,
      "loss": 0.1581,
      "step": 10380
    },
    {
      "epoch": 4.447773972602739,
      "grad_norm": 12.621275901794434,
      "learning_rate": 3.315924657534247e-06,
      "loss": 0.1189,
      "step": 10390
    },
    {
      "epoch": 4.4520547945205475,
      "grad_norm": 5.456303119659424,
      "learning_rate": 3.2902397260273975e-06,
      "loss": 0.1172,
      "step": 10400
    },
    {
      "epoch": 4.4563356164383565,
      "grad_norm": 22.601648330688477,
      "learning_rate": 3.2645547945205477e-06,
      "loss": 0.2073,
      "step": 10410
    },
    {
      "epoch": 4.460616438356165,
      "grad_norm": 0.25601133704185486,
      "learning_rate": 3.2388698630136987e-06,
      "loss": 0.1633,
      "step": 10420
    },
    {
      "epoch": 4.464897260273973,
      "grad_norm": 9.355743408203125,
      "learning_rate": 3.2131849315068493e-06,
      "loss": 0.099,
      "step": 10430
    },
    {
      "epoch": 4.469178082191781,
      "grad_norm": 1.9298228025436401,
      "learning_rate": 3.1875e-06,
      "loss": 0.1957,
      "step": 10440
    },
    {
      "epoch": 4.473458904109589,
      "grad_norm": 0.11670666933059692,
      "learning_rate": 3.1618150684931506e-06,
      "loss": 0.1984,
      "step": 10450
    },
    {
      "epoch": 4.477739726027397,
      "grad_norm": 0.5440239906311035,
      "learning_rate": 3.1361301369863017e-06,
      "loss": 0.2489,
      "step": 10460
    },
    {
      "epoch": 4.482020547945205,
      "grad_norm": 3.0190939903259277,
      "learning_rate": 3.1104452054794523e-06,
      "loss": 0.147,
      "step": 10470
    },
    {
      "epoch": 4.486301369863014,
      "grad_norm": 1.3728727102279663,
      "learning_rate": 3.084760273972603e-06,
      "loss": 0.2292,
      "step": 10480
    },
    {
      "epoch": 4.490582191780822,
      "grad_norm": 0.398542195558548,
      "learning_rate": 3.059075342465753e-06,
      "loss": 0.1995,
      "step": 10490
    },
    {
      "epoch": 4.49486301369863,
      "grad_norm": 7.991480827331543,
      "learning_rate": 3.033390410958904e-06,
      "loss": 0.2686,
      "step": 10500
    },
    {
      "epoch": 4.499143835616438,
      "grad_norm": 0.3077255189418793,
      "learning_rate": 3.007705479452055e-06,
      "loss": 0.1787,
      "step": 10510
    },
    {
      "epoch": 4.5034246575342465,
      "grad_norm": 2.4034645557403564,
      "learning_rate": 2.9820205479452054e-06,
      "loss": 0.2993,
      "step": 10520
    },
    {
      "epoch": 4.507705479452055,
      "grad_norm": 0.3482672870159149,
      "learning_rate": 2.956335616438356e-06,
      "loss": 0.1414,
      "step": 10530
    },
    {
      "epoch": 4.511986301369863,
      "grad_norm": 62.32874298095703,
      "learning_rate": 2.930650684931507e-06,
      "loss": 0.1699,
      "step": 10540
    },
    {
      "epoch": 4.516267123287671,
      "grad_norm": 12.945903778076172,
      "learning_rate": 2.9049657534246577e-06,
      "loss": 0.1879,
      "step": 10550
    },
    {
      "epoch": 4.52054794520548,
      "grad_norm": 55.154964447021484,
      "learning_rate": 2.8792808219178084e-06,
      "loss": 0.1429,
      "step": 10560
    },
    {
      "epoch": 4.524828767123288,
      "grad_norm": 0.9713878631591797,
      "learning_rate": 2.853595890410959e-06,
      "loss": 0.1832,
      "step": 10570
    },
    {
      "epoch": 4.529109589041096,
      "grad_norm": 572.4226684570312,
      "learning_rate": 2.8279109589041096e-06,
      "loss": 0.357,
      "step": 10580
    },
    {
      "epoch": 4.533390410958904,
      "grad_norm": 33.831573486328125,
      "learning_rate": 2.8022260273972603e-06,
      "loss": 0.1825,
      "step": 10590
    },
    {
      "epoch": 4.537671232876712,
      "grad_norm": 1.070866584777832,
      "learning_rate": 2.776541095890411e-06,
      "loss": 0.2112,
      "step": 10600
    },
    {
      "epoch": 4.54195205479452,
      "grad_norm": 4.70510721206665,
      "learning_rate": 2.7508561643835615e-06,
      "loss": 0.1765,
      "step": 10610
    },
    {
      "epoch": 4.546232876712329,
      "grad_norm": 5.450727462768555,
      "learning_rate": 2.7251712328767126e-06,
      "loss": 0.1192,
      "step": 10620
    },
    {
      "epoch": 4.550513698630137,
      "grad_norm": 5.483882904052734,
      "learning_rate": 2.699486301369863e-06,
      "loss": 0.2798,
      "step": 10630
    },
    {
      "epoch": 4.554794520547945,
      "grad_norm": 12.832483291625977,
      "learning_rate": 2.673801369863014e-06,
      "loss": 0.1733,
      "step": 10640
    },
    {
      "epoch": 4.5590753424657535,
      "grad_norm": 11.709304809570312,
      "learning_rate": 2.6481164383561645e-06,
      "loss": 0.1707,
      "step": 10650
    },
    {
      "epoch": 4.563356164383562,
      "grad_norm": 14.506669998168945,
      "learning_rate": 2.622431506849315e-06,
      "loss": 0.6559,
      "step": 10660
    },
    {
      "epoch": 4.56763698630137,
      "grad_norm": 17.219074249267578,
      "learning_rate": 2.5967465753424657e-06,
      "loss": 0.2499,
      "step": 10670
    },
    {
      "epoch": 4.571917808219178,
      "grad_norm": 8.586283683776855,
      "learning_rate": 2.5710616438356163e-06,
      "loss": 0.2382,
      "step": 10680
    },
    {
      "epoch": 4.576198630136986,
      "grad_norm": 9.49250602722168,
      "learning_rate": 2.545376712328767e-06,
      "loss": 0.0823,
      "step": 10690
    },
    {
      "epoch": 4.580479452054795,
      "grad_norm": 3.61118483543396,
      "learning_rate": 2.519691780821918e-06,
      "loss": 0.1348,
      "step": 10700
    },
    {
      "epoch": 4.584760273972603,
      "grad_norm": 17.799699783325195,
      "learning_rate": 2.4940068493150687e-06,
      "loss": 0.2002,
      "step": 10710
    },
    {
      "epoch": 4.589041095890411,
      "grad_norm": 8.42912483215332,
      "learning_rate": 2.4683219178082193e-06,
      "loss": 0.1877,
      "step": 10720
    },
    {
      "epoch": 4.593321917808219,
      "grad_norm": 63.79692840576172,
      "learning_rate": 2.44263698630137e-06,
      "loss": 0.1727,
      "step": 10730
    },
    {
      "epoch": 4.597602739726027,
      "grad_norm": 13.490234375,
      "learning_rate": 2.416952054794521e-06,
      "loss": 0.1971,
      "step": 10740
    },
    {
      "epoch": 4.601883561643835,
      "grad_norm": 4.821283340454102,
      "learning_rate": 2.391267123287671e-06,
      "loss": 0.1663,
      "step": 10750
    },
    {
      "epoch": 4.6061643835616435,
      "grad_norm": 20.66674041748047,
      "learning_rate": 2.365582191780822e-06,
      "loss": 0.1534,
      "step": 10760
    },
    {
      "epoch": 4.6104452054794525,
      "grad_norm": 0.25134533643722534,
      "learning_rate": 2.3398972602739724e-06,
      "loss": 0.3029,
      "step": 10770
    },
    {
      "epoch": 4.614726027397261,
      "grad_norm": 0.07218410074710846,
      "learning_rate": 2.3142123287671235e-06,
      "loss": 0.1637,
      "step": 10780
    },
    {
      "epoch": 4.619006849315069,
      "grad_norm": 0.643671452999115,
      "learning_rate": 2.288527397260274e-06,
      "loss": 0.1393,
      "step": 10790
    },
    {
      "epoch": 4.623287671232877,
      "grad_norm": 25.962173461914062,
      "learning_rate": 2.2628424657534247e-06,
      "loss": 0.185,
      "step": 10800
    },
    {
      "epoch": 4.627568493150685,
      "grad_norm": 21.25493812561035,
      "learning_rate": 2.2371575342465754e-06,
      "loss": 0.2015,
      "step": 10810
    },
    {
      "epoch": 4.631849315068493,
      "grad_norm": 136.7112274169922,
      "learning_rate": 2.2114726027397264e-06,
      "loss": 0.1263,
      "step": 10820
    },
    {
      "epoch": 4.636130136986301,
      "grad_norm": 40.8122444152832,
      "learning_rate": 2.185787671232877e-06,
      "loss": 0.2511,
      "step": 10830
    },
    {
      "epoch": 4.640410958904109,
      "grad_norm": 10.706768035888672,
      "learning_rate": 2.1601027397260273e-06,
      "loss": 0.1935,
      "step": 10840
    },
    {
      "epoch": 4.644691780821918,
      "grad_norm": 11.851151466369629,
      "learning_rate": 2.134417808219178e-06,
      "loss": 0.2821,
      "step": 10850
    },
    {
      "epoch": 4.648972602739726,
      "grad_norm": 30.460500717163086,
      "learning_rate": 2.108732876712329e-06,
      "loss": 0.2481,
      "step": 10860
    },
    {
      "epoch": 4.653253424657534,
      "grad_norm": 33.86069869995117,
      "learning_rate": 2.0830479452054796e-06,
      "loss": 0.2999,
      "step": 10870
    },
    {
      "epoch": 4.657534246575342,
      "grad_norm": 1.3472483158111572,
      "learning_rate": 2.05736301369863e-06,
      "loss": 0.3203,
      "step": 10880
    },
    {
      "epoch": 4.6618150684931505,
      "grad_norm": 7.5889387130737305,
      "learning_rate": 2.031678082191781e-06,
      "loss": 0.3646,
      "step": 10890
    },
    {
      "epoch": 4.666095890410959,
      "grad_norm": 4.74727201461792,
      "learning_rate": 2.005993150684932e-06,
      "loss": 0.23,
      "step": 10900
    },
    {
      "epoch": 4.670376712328768,
      "grad_norm": 2.9358880519866943,
      "learning_rate": 1.9803082191780825e-06,
      "loss": 0.2822,
      "step": 10910
    },
    {
      "epoch": 4.674657534246576,
      "grad_norm": 21.847553253173828,
      "learning_rate": 1.9546232876712327e-06,
      "loss": 0.4892,
      "step": 10920
    },
    {
      "epoch": 4.678938356164384,
      "grad_norm": 6.8135294914245605,
      "learning_rate": 1.9289383561643833e-06,
      "loss": 0.2811,
      "step": 10930
    },
    {
      "epoch": 4.683219178082192,
      "grad_norm": 40.338687896728516,
      "learning_rate": 1.9032534246575344e-06,
      "loss": 0.2987,
      "step": 10940
    },
    {
      "epoch": 4.6875,
      "grad_norm": 6.7465925216674805,
      "learning_rate": 1.877568493150685e-06,
      "loss": 0.1416,
      "step": 10950
    },
    {
      "epoch": 4.691780821917808,
      "grad_norm": 19.93168830871582,
      "learning_rate": 1.8518835616438357e-06,
      "loss": 0.2216,
      "step": 10960
    },
    {
      "epoch": 4.696061643835616,
      "grad_norm": 11.3759765625,
      "learning_rate": 1.8261986301369865e-06,
      "loss": 0.1341,
      "step": 10970
    },
    {
      "epoch": 4.700342465753424,
      "grad_norm": 16.11250877380371,
      "learning_rate": 1.8005136986301371e-06,
      "loss": 0.4331,
      "step": 10980
    },
    {
      "epoch": 4.704623287671232,
      "grad_norm": 0.38908660411834717,
      "learning_rate": 1.7748287671232878e-06,
      "loss": 0.14,
      "step": 10990
    },
    {
      "epoch": 4.708904109589041,
      "grad_norm": 1.4327561855316162,
      "learning_rate": 1.7491438356164384e-06,
      "loss": 0.1689,
      "step": 11000
    },
    {
      "epoch": 4.7131849315068495,
      "grad_norm": 8.510637283325195,
      "learning_rate": 1.7234589041095892e-06,
      "loss": 0.4834,
      "step": 11010
    },
    {
      "epoch": 4.717465753424658,
      "grad_norm": 1.3069300651550293,
      "learning_rate": 1.6977739726027399e-06,
      "loss": 0.2379,
      "step": 11020
    },
    {
      "epoch": 4.721746575342466,
      "grad_norm": 0.6775394082069397,
      "learning_rate": 1.6720890410958903e-06,
      "loss": 0.1856,
      "step": 11030
    },
    {
      "epoch": 4.726027397260274,
      "grad_norm": 46.373992919921875,
      "learning_rate": 1.6464041095890411e-06,
      "loss": 0.3092,
      "step": 11040
    },
    {
      "epoch": 4.730308219178082,
      "grad_norm": 50.71855163574219,
      "learning_rate": 1.6207191780821917e-06,
      "loss": 0.2054,
      "step": 11050
    },
    {
      "epoch": 4.734589041095891,
      "grad_norm": 0.267348974943161,
      "learning_rate": 1.5950342465753426e-06,
      "loss": 0.2249,
      "step": 11060
    },
    {
      "epoch": 4.738869863013699,
      "grad_norm": 6.365934371948242,
      "learning_rate": 1.569349315068493e-06,
      "loss": 0.1527,
      "step": 11070
    },
    {
      "epoch": 4.743150684931507,
      "grad_norm": 44.71017074584961,
      "learning_rate": 1.5436643835616438e-06,
      "loss": 0.1502,
      "step": 11080
    },
    {
      "epoch": 4.747431506849315,
      "grad_norm": 39.7419548034668,
      "learning_rate": 1.5179794520547945e-06,
      "loss": 0.1236,
      "step": 11090
    },
    {
      "epoch": 4.751712328767123,
      "grad_norm": 16.800891876220703,
      "learning_rate": 1.4922945205479453e-06,
      "loss": 0.1579,
      "step": 11100
    },
    {
      "epoch": 4.755993150684931,
      "grad_norm": 17.548616409301758,
      "learning_rate": 1.466609589041096e-06,
      "loss": 0.2441,
      "step": 11110
    },
    {
      "epoch": 4.760273972602739,
      "grad_norm": 3.34104585647583,
      "learning_rate": 1.4409246575342466e-06,
      "loss": 0.225,
      "step": 11120
    },
    {
      "epoch": 4.7645547945205475,
      "grad_norm": 18.002182006835938,
      "learning_rate": 1.4152397260273972e-06,
      "loss": 0.272,
      "step": 11130
    },
    {
      "epoch": 4.7688356164383565,
      "grad_norm": 0.46946313977241516,
      "learning_rate": 1.389554794520548e-06,
      "loss": 0.2226,
      "step": 11140
    },
    {
      "epoch": 4.773116438356165,
      "grad_norm": 1.5212182998657227,
      "learning_rate": 1.3638698630136987e-06,
      "loss": 0.178,
      "step": 11150
    },
    {
      "epoch": 4.777397260273973,
      "grad_norm": 0.37091460824012756,
      "learning_rate": 1.3381849315068493e-06,
      "loss": 0.2054,
      "step": 11160
    },
    {
      "epoch": 4.781678082191781,
      "grad_norm": 0.35136815905570984,
      "learning_rate": 1.3125e-06,
      "loss": 0.2994,
      "step": 11170
    },
    {
      "epoch": 4.785958904109589,
      "grad_norm": 36.62333297729492,
      "learning_rate": 1.2868150684931508e-06,
      "loss": 0.1172,
      "step": 11180
    },
    {
      "epoch": 4.790239726027397,
      "grad_norm": 0.7423260807991028,
      "learning_rate": 1.2611301369863014e-06,
      "loss": 0.1825,
      "step": 11190
    },
    {
      "epoch": 4.794520547945205,
      "grad_norm": 1.2371784448623657,
      "learning_rate": 1.235445205479452e-06,
      "loss": 0.2822,
      "step": 11200
    },
    {
      "epoch": 4.798801369863014,
      "grad_norm": 8.375750541687012,
      "learning_rate": 1.2097602739726027e-06,
      "loss": 0.2196,
      "step": 11210
    },
    {
      "epoch": 4.803082191780822,
      "grad_norm": 0.14171214401721954,
      "learning_rate": 1.1840753424657535e-06,
      "loss": 0.2301,
      "step": 11220
    },
    {
      "epoch": 4.80736301369863,
      "grad_norm": 2.0647451877593994,
      "learning_rate": 1.1583904109589041e-06,
      "loss": 0.2655,
      "step": 11230
    },
    {
      "epoch": 4.811643835616438,
      "grad_norm": 0.3723751902580261,
      "learning_rate": 1.1327054794520548e-06,
      "loss": 0.1902,
      "step": 11240
    },
    {
      "epoch": 4.8159246575342465,
      "grad_norm": 8.133271217346191,
      "learning_rate": 1.1070205479452054e-06,
      "loss": 0.2534,
      "step": 11250
    },
    {
      "epoch": 4.820205479452055,
      "grad_norm": 25.68329429626465,
      "learning_rate": 1.0813356164383562e-06,
      "loss": 0.2166,
      "step": 11260
    },
    {
      "epoch": 4.824486301369863,
      "grad_norm": 8.995708465576172,
      "learning_rate": 1.0556506849315069e-06,
      "loss": 0.2687,
      "step": 11270
    },
    {
      "epoch": 4.828767123287671,
      "grad_norm": 3.6503024101257324,
      "learning_rate": 1.0299657534246577e-06,
      "loss": 0.1641,
      "step": 11280
    },
    {
      "epoch": 4.83304794520548,
      "grad_norm": 29.95808219909668,
      "learning_rate": 1.0042808219178081e-06,
      "loss": 0.2901,
      "step": 11290
    },
    {
      "epoch": 4.837328767123288,
      "grad_norm": 0.6707509160041809,
      "learning_rate": 9.78595890410959e-07,
      "loss": 0.1188,
      "step": 11300
    },
    {
      "epoch": 4.841609589041096,
      "grad_norm": 0.2616395652294159,
      "learning_rate": 9.529109589041096e-07,
      "loss": 0.1652,
      "step": 11310
    },
    {
      "epoch": 4.845890410958904,
      "grad_norm": 21.879594802856445,
      "learning_rate": 9.272260273972603e-07,
      "loss": 0.2609,
      "step": 11320
    },
    {
      "epoch": 4.850171232876712,
      "grad_norm": 0.8531849384307861,
      "learning_rate": 9.01541095890411e-07,
      "loss": 0.3245,
      "step": 11330
    },
    {
      "epoch": 4.85445205479452,
      "grad_norm": 2.412675142288208,
      "learning_rate": 8.758561643835617e-07,
      "loss": 0.233,
      "step": 11340
    },
    {
      "epoch": 4.858732876712329,
      "grad_norm": 5.911262512207031,
      "learning_rate": 8.501712328767124e-07,
      "loss": 0.2868,
      "step": 11350
    },
    {
      "epoch": 4.863013698630137,
      "grad_norm": 13.395275115966797,
      "learning_rate": 8.24486301369863e-07,
      "loss": 0.1988,
      "step": 11360
    },
    {
      "epoch": 4.867294520547945,
      "grad_norm": 0.17292103171348572,
      "learning_rate": 7.988013698630138e-07,
      "loss": 0.3109,
      "step": 11370
    },
    {
      "epoch": 4.8715753424657535,
      "grad_norm": 0.7883020043373108,
      "learning_rate": 7.731164383561644e-07,
      "loss": 0.1287,
      "step": 11380
    },
    {
      "epoch": 4.875856164383562,
      "grad_norm": 3.731945276260376,
      "learning_rate": 7.47431506849315e-07,
      "loss": 0.2113,
      "step": 11390
    },
    {
      "epoch": 4.88013698630137,
      "grad_norm": 12.408041000366211,
      "learning_rate": 7.217465753424657e-07,
      "loss": 0.2596,
      "step": 11400
    },
    {
      "epoch": 4.884417808219178,
      "grad_norm": 6.551688194274902,
      "learning_rate": 6.960616438356164e-07,
      "loss": 0.1003,
      "step": 11410
    },
    {
      "epoch": 4.888698630136986,
      "grad_norm": 6.29386568069458,
      "learning_rate": 6.703767123287671e-07,
      "loss": 0.2299,
      "step": 11420
    },
    {
      "epoch": 4.892979452054795,
      "grad_norm": 0.783917248249054,
      "learning_rate": 6.446917808219178e-07,
      "loss": 0.0591,
      "step": 11430
    },
    {
      "epoch": 4.897260273972603,
      "grad_norm": 11.426008224487305,
      "learning_rate": 6.190068493150685e-07,
      "loss": 0.2282,
      "step": 11440
    },
    {
      "epoch": 4.901541095890411,
      "grad_norm": 2.1226577758789062,
      "learning_rate": 5.933219178082191e-07,
      "loss": 0.1749,
      "step": 11450
    },
    {
      "epoch": 4.905821917808219,
      "grad_norm": 8.826642036437988,
      "learning_rate": 5.676369863013699e-07,
      "loss": 0.3636,
      "step": 11460
    },
    {
      "epoch": 4.910102739726027,
      "grad_norm": 25.797183990478516,
      "learning_rate": 5.419520547945205e-07,
      "loss": 0.2934,
      "step": 11470
    },
    {
      "epoch": 4.914383561643835,
      "grad_norm": 3.087627410888672,
      "learning_rate": 5.162671232876712e-07,
      "loss": 0.1109,
      "step": 11480
    },
    {
      "epoch": 4.9186643835616435,
      "grad_norm": 0.6253854632377625,
      "learning_rate": 4.905821917808219e-07,
      "loss": 0.1273,
      "step": 11490
    },
    {
      "epoch": 4.9229452054794525,
      "grad_norm": 33.55672836303711,
      "learning_rate": 4.6489726027397265e-07,
      "loss": 0.1542,
      "step": 11500
    },
    {
      "epoch": 4.927226027397261,
      "grad_norm": 10.914847373962402,
      "learning_rate": 4.3921232876712333e-07,
      "loss": 0.1786,
      "step": 11510
    },
    {
      "epoch": 4.931506849315069,
      "grad_norm": 0.734129786491394,
      "learning_rate": 4.13527397260274e-07,
      "loss": 0.2243,
      "step": 11520
    },
    {
      "epoch": 4.935787671232877,
      "grad_norm": 13.299185752868652,
      "learning_rate": 3.878424657534247e-07,
      "loss": 0.2193,
      "step": 11530
    },
    {
      "epoch": 4.940068493150685,
      "grad_norm": 30.120954513549805,
      "learning_rate": 3.621575342465754e-07,
      "loss": 0.1887,
      "step": 11540
    },
    {
      "epoch": 4.944349315068493,
      "grad_norm": 373.9381103515625,
      "learning_rate": 3.3647260273972606e-07,
      "loss": 0.2033,
      "step": 11550
    },
    {
      "epoch": 4.948630136986301,
      "grad_norm": 0.19820357859134674,
      "learning_rate": 3.1078767123287674e-07,
      "loss": 0.3003,
      "step": 11560
    },
    {
      "epoch": 4.952910958904109,
      "grad_norm": 1.474765419960022,
      "learning_rate": 2.851027397260274e-07,
      "loss": 0.1193,
      "step": 11570
    },
    {
      "epoch": 4.957191780821918,
      "grad_norm": 0.9303411245346069,
      "learning_rate": 2.5941780821917805e-07,
      "loss": 0.1183,
      "step": 11580
    },
    {
      "epoch": 4.961472602739726,
      "grad_norm": 6.08756685256958,
      "learning_rate": 2.337328767123288e-07,
      "loss": 0.1834,
      "step": 11590
    },
    {
      "epoch": 4.965753424657534,
      "grad_norm": 22.673341751098633,
      "learning_rate": 2.0804794520547947e-07,
      "loss": 0.2924,
      "step": 11600
    },
    {
      "epoch": 4.970034246575342,
      "grad_norm": 5.192663669586182,
      "learning_rate": 1.8236301369863015e-07,
      "loss": 0.2922,
      "step": 11610
    },
    {
      "epoch": 4.9743150684931505,
      "grad_norm": 0.13915514945983887,
      "learning_rate": 1.5667808219178083e-07,
      "loss": 0.2085,
      "step": 11620
    },
    {
      "epoch": 4.978595890410959,
      "grad_norm": 10.780571937561035,
      "learning_rate": 1.3099315068493152e-07,
      "loss": 0.1972,
      "step": 11630
    },
    {
      "epoch": 4.982876712328768,
      "grad_norm": 0.4047846496105194,
      "learning_rate": 1.0530821917808218e-07,
      "loss": 0.2247,
      "step": 11640
    },
    {
      "epoch": 4.987157534246576,
      "grad_norm": 18.27676010131836,
      "learning_rate": 7.962328767123288e-08,
      "loss": 0.2253,
      "step": 11650
    },
    {
      "epoch": 4.991438356164384,
      "grad_norm": 7.731654644012451,
      "learning_rate": 5.393835616438356e-08,
      "loss": 0.1605,
      "step": 11660
    },
    {
      "epoch": 4.995719178082192,
      "grad_norm": 4.4701714515686035,
      "learning_rate": 2.8253424657534248e-08,
      "loss": 0.4276,
      "step": 11670
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.04988027736544609,
      "learning_rate": 2.5684931506849315e-09,
      "loss": 0.1433,
      "step": 11680
    },
    {
      "epoch": 5.0,
      "eval_f1": 0.7053581778129331,
      "eval_loss": 0.8412582278251648,
      "eval_precision": 0.7124940818715997,
      "eval_recall": 0.7087692109001679,
      "eval_runtime": 519.5848,
      "eval_samples_per_second": 8.603,
      "eval_steps_per_second": 1.076,
      "step": 11680
    }
  ],
  "logging_steps": 10,
  "max_steps": 11680,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 517311861496320.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
