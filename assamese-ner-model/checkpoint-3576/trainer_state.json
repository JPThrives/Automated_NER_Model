{
  "best_global_step": 3576,
  "best_metric": 0.7091624140739441,
  "best_model_checkpoint": "./assamese-ner-model\\checkpoint-3576",
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 3576,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.011185682326621925,
      "grad_norm": 6.445121765136719,
      "learning_rate": 4.99496644295302e-05,
      "loss": 2.7648,
      "step": 10
    },
    {
      "epoch": 0.02237136465324385,
      "grad_norm": 12.056178092956543,
      "learning_rate": 4.989373601789709e-05,
      "loss": 2.3233,
      "step": 20
    },
    {
      "epoch": 0.03355704697986577,
      "grad_norm": 5.682458877563477,
      "learning_rate": 4.9837807606263986e-05,
      "loss": 2.0379,
      "step": 30
    },
    {
      "epoch": 0.0447427293064877,
      "grad_norm": 8.98845386505127,
      "learning_rate": 4.978187919463088e-05,
      "loss": 2.0233,
      "step": 40
    },
    {
      "epoch": 0.05592841163310962,
      "grad_norm": 9.50200080871582,
      "learning_rate": 4.9725950782997766e-05,
      "loss": 1.9803,
      "step": 50
    },
    {
      "epoch": 0.06711409395973154,
      "grad_norm": 8.902047157287598,
      "learning_rate": 4.967002237136465e-05,
      "loss": 1.9964,
      "step": 60
    },
    {
      "epoch": 0.07829977628635347,
      "grad_norm": 9.146327018737793,
      "learning_rate": 4.9614093959731546e-05,
      "loss": 1.5713,
      "step": 70
    },
    {
      "epoch": 0.0894854586129754,
      "grad_norm": 5.8543548583984375,
      "learning_rate": 4.955816554809843e-05,
      "loss": 1.2282,
      "step": 80
    },
    {
      "epoch": 0.10067114093959731,
      "grad_norm": 6.276554584503174,
      "learning_rate": 4.950223713646533e-05,
      "loss": 1.3,
      "step": 90
    },
    {
      "epoch": 0.11185682326621924,
      "grad_norm": 11.30167293548584,
      "learning_rate": 4.944630872483222e-05,
      "loss": 1.2322,
      "step": 100
    },
    {
      "epoch": 0.12304250559284116,
      "grad_norm": 15.552138328552246,
      "learning_rate": 4.9390380313199105e-05,
      "loss": 1.1417,
      "step": 110
    },
    {
      "epoch": 0.1342281879194631,
      "grad_norm": 8.387395858764648,
      "learning_rate": 4.9334451901566e-05,
      "loss": 1.3175,
      "step": 120
    },
    {
      "epoch": 0.14541387024608501,
      "grad_norm": 7.15743350982666,
      "learning_rate": 4.9278523489932885e-05,
      "loss": 1.5433,
      "step": 130
    },
    {
      "epoch": 0.15659955257270694,
      "grad_norm": 8.005496978759766,
      "learning_rate": 4.922259507829978e-05,
      "loss": 1.2294,
      "step": 140
    },
    {
      "epoch": 0.16778523489932887,
      "grad_norm": 12.950035095214844,
      "learning_rate": 4.9166666666666665e-05,
      "loss": 1.1804,
      "step": 150
    },
    {
      "epoch": 0.1789709172259508,
      "grad_norm": 9.608451843261719,
      "learning_rate": 4.9110738255033565e-05,
      "loss": 1.2292,
      "step": 160
    },
    {
      "epoch": 0.19015659955257272,
      "grad_norm": 13.558655738830566,
      "learning_rate": 4.905480984340045e-05,
      "loss": 1.2101,
      "step": 170
    },
    {
      "epoch": 0.20134228187919462,
      "grad_norm": 12.652565002441406,
      "learning_rate": 4.899888143176734e-05,
      "loss": 1.1959,
      "step": 180
    },
    {
      "epoch": 0.21252796420581654,
      "grad_norm": 8.720565795898438,
      "learning_rate": 4.894295302013423e-05,
      "loss": 1.4518,
      "step": 190
    },
    {
      "epoch": 0.22371364653243847,
      "grad_norm": 10.317140579223633,
      "learning_rate": 4.888702460850112e-05,
      "loss": 1.0331,
      "step": 200
    },
    {
      "epoch": 0.2348993288590604,
      "grad_norm": 4.1483378410339355,
      "learning_rate": 4.883109619686801e-05,
      "loss": 1.1236,
      "step": 210
    },
    {
      "epoch": 0.24608501118568232,
      "grad_norm": 11.045066833496094,
      "learning_rate": 4.8775167785234904e-05,
      "loss": 1.2404,
      "step": 220
    },
    {
      "epoch": 0.25727069351230425,
      "grad_norm": 4.663601398468018,
      "learning_rate": 4.871923937360179e-05,
      "loss": 1.409,
      "step": 230
    },
    {
      "epoch": 0.2684563758389262,
      "grad_norm": 9.675888061523438,
      "learning_rate": 4.8663310961968684e-05,
      "loss": 1.0603,
      "step": 240
    },
    {
      "epoch": 0.2796420581655481,
      "grad_norm": 8.415253639221191,
      "learning_rate": 4.860738255033557e-05,
      "loss": 1.2698,
      "step": 250
    },
    {
      "epoch": 0.29082774049217003,
      "grad_norm": 7.527126789093018,
      "learning_rate": 4.8551454138702463e-05,
      "loss": 1.1977,
      "step": 260
    },
    {
      "epoch": 0.30201342281879195,
      "grad_norm": 7.39705228805542,
      "learning_rate": 4.849552572706935e-05,
      "loss": 0.9019,
      "step": 270
    },
    {
      "epoch": 0.3131991051454139,
      "grad_norm": 5.218620300292969,
      "learning_rate": 4.843959731543624e-05,
      "loss": 0.9521,
      "step": 280
    },
    {
      "epoch": 0.3243847874720358,
      "grad_norm": 6.836513042449951,
      "learning_rate": 4.8383668903803136e-05,
      "loss": 1.2141,
      "step": 290
    },
    {
      "epoch": 0.33557046979865773,
      "grad_norm": 15.010504722595215,
      "learning_rate": 4.832774049217002e-05,
      "loss": 1.3071,
      "step": 300
    },
    {
      "epoch": 0.34675615212527966,
      "grad_norm": 7.33261251449585,
      "learning_rate": 4.8271812080536916e-05,
      "loss": 1.1525,
      "step": 310
    },
    {
      "epoch": 0.3579418344519016,
      "grad_norm": 9.37894344329834,
      "learning_rate": 4.82158836689038e-05,
      "loss": 0.9652,
      "step": 320
    },
    {
      "epoch": 0.3691275167785235,
      "grad_norm": 7.875756740570068,
      "learning_rate": 4.8159955257270696e-05,
      "loss": 1.2562,
      "step": 330
    },
    {
      "epoch": 0.38031319910514544,
      "grad_norm": 6.179051399230957,
      "learning_rate": 4.810402684563758e-05,
      "loss": 1.1887,
      "step": 340
    },
    {
      "epoch": 0.39149888143176736,
      "grad_norm": 6.839425563812256,
      "learning_rate": 4.804809843400448e-05,
      "loss": 1.1601,
      "step": 350
    },
    {
      "epoch": 0.40268456375838924,
      "grad_norm": 7.888421535491943,
      "learning_rate": 4.799217002237137e-05,
      "loss": 1.1104,
      "step": 360
    },
    {
      "epoch": 0.41387024608501116,
      "grad_norm": 8.489946365356445,
      "learning_rate": 4.7936241610738255e-05,
      "loss": 0.9999,
      "step": 370
    },
    {
      "epoch": 0.4250559284116331,
      "grad_norm": 4.577839374542236,
      "learning_rate": 4.788031319910515e-05,
      "loss": 1.2303,
      "step": 380
    },
    {
      "epoch": 0.436241610738255,
      "grad_norm": 7.905966758728027,
      "learning_rate": 4.7824384787472035e-05,
      "loss": 1.0334,
      "step": 390
    },
    {
      "epoch": 0.44742729306487694,
      "grad_norm": 6.242004871368408,
      "learning_rate": 4.776845637583893e-05,
      "loss": 1.1417,
      "step": 400
    },
    {
      "epoch": 0.45861297539149887,
      "grad_norm": 4.519566535949707,
      "learning_rate": 4.7712527964205815e-05,
      "loss": 0.6092,
      "step": 410
    },
    {
      "epoch": 0.4697986577181208,
      "grad_norm": 7.561840534210205,
      "learning_rate": 4.765659955257271e-05,
      "loss": 1.033,
      "step": 420
    },
    {
      "epoch": 0.4809843400447427,
      "grad_norm": 13.672502517700195,
      "learning_rate": 4.76006711409396e-05,
      "loss": 0.8485,
      "step": 430
    },
    {
      "epoch": 0.49217002237136465,
      "grad_norm": 13.74712085723877,
      "learning_rate": 4.754474272930649e-05,
      "loss": 1.0493,
      "step": 440
    },
    {
      "epoch": 0.5033557046979866,
      "grad_norm": 6.163567543029785,
      "learning_rate": 4.748881431767338e-05,
      "loss": 0.8473,
      "step": 450
    },
    {
      "epoch": 0.5145413870246085,
      "grad_norm": 9.529053688049316,
      "learning_rate": 4.743288590604027e-05,
      "loss": 1.0404,
      "step": 460
    },
    {
      "epoch": 0.5257270693512305,
      "grad_norm": 10.403264999389648,
      "learning_rate": 4.737695749440716e-05,
      "loss": 1.2456,
      "step": 470
    },
    {
      "epoch": 0.5369127516778524,
      "grad_norm": 9.783088684082031,
      "learning_rate": 4.732102908277405e-05,
      "loss": 1.0918,
      "step": 480
    },
    {
      "epoch": 0.5480984340044742,
      "grad_norm": 16.348209381103516,
      "learning_rate": 4.726510067114094e-05,
      "loss": 1.0483,
      "step": 490
    },
    {
      "epoch": 0.5592841163310962,
      "grad_norm": 9.592966079711914,
      "learning_rate": 4.7209172259507834e-05,
      "loss": 0.9247,
      "step": 500
    },
    {
      "epoch": 0.5704697986577181,
      "grad_norm": 16.716161727905273,
      "learning_rate": 4.715324384787472e-05,
      "loss": 0.9994,
      "step": 510
    },
    {
      "epoch": 0.5816554809843401,
      "grad_norm": 7.519565105438232,
      "learning_rate": 4.7097315436241614e-05,
      "loss": 0.8178,
      "step": 520
    },
    {
      "epoch": 0.5928411633109619,
      "grad_norm": 11.665682792663574,
      "learning_rate": 4.70413870246085e-05,
      "loss": 1.1515,
      "step": 530
    },
    {
      "epoch": 0.6040268456375839,
      "grad_norm": 7.613988399505615,
      "learning_rate": 4.6985458612975394e-05,
      "loss": 1.0917,
      "step": 540
    },
    {
      "epoch": 0.6152125279642058,
      "grad_norm": 9.515976905822754,
      "learning_rate": 4.692953020134229e-05,
      "loss": 1.0768,
      "step": 550
    },
    {
      "epoch": 0.6263982102908278,
      "grad_norm": 6.533619403839111,
      "learning_rate": 4.687360178970917e-05,
      "loss": 0.9487,
      "step": 560
    },
    {
      "epoch": 0.6375838926174496,
      "grad_norm": 3.1964900493621826,
      "learning_rate": 4.6817673378076067e-05,
      "loss": 0.8994,
      "step": 570
    },
    {
      "epoch": 0.6487695749440716,
      "grad_norm": 7.171006202697754,
      "learning_rate": 4.676174496644295e-05,
      "loss": 0.8582,
      "step": 580
    },
    {
      "epoch": 0.6599552572706935,
      "grad_norm": 16.003204345703125,
      "learning_rate": 4.6705816554809846e-05,
      "loss": 0.9796,
      "step": 590
    },
    {
      "epoch": 0.6711409395973155,
      "grad_norm": 7.225227355957031,
      "learning_rate": 4.664988814317673e-05,
      "loss": 0.9759,
      "step": 600
    },
    {
      "epoch": 0.6823266219239373,
      "grad_norm": 5.885578155517578,
      "learning_rate": 4.6593959731543626e-05,
      "loss": 0.7142,
      "step": 610
    },
    {
      "epoch": 0.6935123042505593,
      "grad_norm": 8.091598510742188,
      "learning_rate": 4.653803131991052e-05,
      "loss": 1.2387,
      "step": 620
    },
    {
      "epoch": 0.7046979865771812,
      "grad_norm": 4.263943672180176,
      "learning_rate": 4.6482102908277406e-05,
      "loss": 0.8733,
      "step": 630
    },
    {
      "epoch": 0.7158836689038032,
      "grad_norm": 19.80646324157715,
      "learning_rate": 4.64261744966443e-05,
      "loss": 0.9045,
      "step": 640
    },
    {
      "epoch": 0.727069351230425,
      "grad_norm": 10.986384391784668,
      "learning_rate": 4.6370246085011186e-05,
      "loss": 0.933,
      "step": 650
    },
    {
      "epoch": 0.738255033557047,
      "grad_norm": 7.241734981536865,
      "learning_rate": 4.631431767337808e-05,
      "loss": 0.9204,
      "step": 660
    },
    {
      "epoch": 0.7494407158836689,
      "grad_norm": 7.903285026550293,
      "learning_rate": 4.6258389261744965e-05,
      "loss": 0.9081,
      "step": 670
    },
    {
      "epoch": 0.7606263982102909,
      "grad_norm": 7.605363845825195,
      "learning_rate": 4.620246085011186e-05,
      "loss": 0.9576,
      "step": 680
    },
    {
      "epoch": 0.7718120805369127,
      "grad_norm": 17.94335174560547,
      "learning_rate": 4.614653243847875e-05,
      "loss": 1.1981,
      "step": 690
    },
    {
      "epoch": 0.7829977628635347,
      "grad_norm": 7.018117427825928,
      "learning_rate": 4.609060402684564e-05,
      "loss": 0.8547,
      "step": 700
    },
    {
      "epoch": 0.7941834451901566,
      "grad_norm": 9.043609619140625,
      "learning_rate": 4.603467561521253e-05,
      "loss": 0.8944,
      "step": 710
    },
    {
      "epoch": 0.8053691275167785,
      "grad_norm": 10.868064880371094,
      "learning_rate": 4.597874720357942e-05,
      "loss": 0.9903,
      "step": 720
    },
    {
      "epoch": 0.8165548098434005,
      "grad_norm": 8.08144474029541,
      "learning_rate": 4.592281879194631e-05,
      "loss": 1.0702,
      "step": 730
    },
    {
      "epoch": 0.8277404921700223,
      "grad_norm": 8.05030632019043,
      "learning_rate": 4.58668903803132e-05,
      "loss": 0.7873,
      "step": 740
    },
    {
      "epoch": 0.8389261744966443,
      "grad_norm": 34.37476348876953,
      "learning_rate": 4.581096196868009e-05,
      "loss": 0.785,
      "step": 750
    },
    {
      "epoch": 0.8501118568232662,
      "grad_norm": 14.309159278869629,
      "learning_rate": 4.5755033557046984e-05,
      "loss": 0.8382,
      "step": 760
    },
    {
      "epoch": 0.8612975391498882,
      "grad_norm": 5.6285319328308105,
      "learning_rate": 4.569910514541387e-05,
      "loss": 0.7878,
      "step": 770
    },
    {
      "epoch": 0.87248322147651,
      "grad_norm": 27.49907875061035,
      "learning_rate": 4.5643176733780764e-05,
      "loss": 0.7952,
      "step": 780
    },
    {
      "epoch": 0.883668903803132,
      "grad_norm": 16.661968231201172,
      "learning_rate": 4.558724832214765e-05,
      "loss": 0.9466,
      "step": 790
    },
    {
      "epoch": 0.8948545861297539,
      "grad_norm": 12.038531303405762,
      "learning_rate": 4.5531319910514544e-05,
      "loss": 0.9013,
      "step": 800
    },
    {
      "epoch": 0.9060402684563759,
      "grad_norm": 8.887724876403809,
      "learning_rate": 4.547539149888144e-05,
      "loss": 0.9247,
      "step": 810
    },
    {
      "epoch": 0.9172259507829977,
      "grad_norm": 13.604645729064941,
      "learning_rate": 4.5419463087248324e-05,
      "loss": 1.167,
      "step": 820
    },
    {
      "epoch": 0.9284116331096197,
      "grad_norm": 10.794665336608887,
      "learning_rate": 4.536353467561522e-05,
      "loss": 0.7734,
      "step": 830
    },
    {
      "epoch": 0.9395973154362416,
      "grad_norm": 5.002216815948486,
      "learning_rate": 4.5307606263982103e-05,
      "loss": 0.9819,
      "step": 840
    },
    {
      "epoch": 0.9507829977628636,
      "grad_norm": 8.768050193786621,
      "learning_rate": 4.5251677852349e-05,
      "loss": 1.1613,
      "step": 850
    },
    {
      "epoch": 0.9619686800894854,
      "grad_norm": 6.793642520904541,
      "learning_rate": 4.519574944071588e-05,
      "loss": 0.9274,
      "step": 860
    },
    {
      "epoch": 0.9731543624161074,
      "grad_norm": 7.945538520812988,
      "learning_rate": 4.5139821029082776e-05,
      "loss": 1.098,
      "step": 870
    },
    {
      "epoch": 0.9843400447427293,
      "grad_norm": 30.6758975982666,
      "learning_rate": 4.508389261744967e-05,
      "loss": 1.0597,
      "step": 880
    },
    {
      "epoch": 0.9955257270693513,
      "grad_norm": 3.957589864730835,
      "learning_rate": 4.5027964205816556e-05,
      "loss": 0.8577,
      "step": 890
    },
    {
      "epoch": 1.0,
      "eval_f1": 0.6130859636985704,
      "eval_loss": 0.866314172744751,
      "eval_precision": 0.5883246211972588,
      "eval_recall": 0.6564784053156146,
      "eval_runtime": 114.5436,
      "eval_samples_per_second": 7.805,
      "eval_steps_per_second": 0.978,
      "step": 894
    },
    {
      "epoch": 1.0067114093959733,
      "grad_norm": 11.171778678894043,
      "learning_rate": 4.497203579418345e-05,
      "loss": 0.7978,
      "step": 900
    },
    {
      "epoch": 1.0178970917225951,
      "grad_norm": 4.7888569831848145,
      "learning_rate": 4.4916107382550336e-05,
      "loss": 0.6529,
      "step": 910
    },
    {
      "epoch": 1.029082774049217,
      "grad_norm": 5.183602333068848,
      "learning_rate": 4.486017897091723e-05,
      "loss": 0.7844,
      "step": 920
    },
    {
      "epoch": 1.0402684563758389,
      "grad_norm": 5.882471561431885,
      "learning_rate": 4.4804250559284116e-05,
      "loss": 0.7615,
      "step": 930
    },
    {
      "epoch": 1.0514541387024607,
      "grad_norm": 5.921355247497559,
      "learning_rate": 4.474832214765101e-05,
      "loss": 0.6087,
      "step": 940
    },
    {
      "epoch": 1.0626398210290828,
      "grad_norm": 7.7222819328308105,
      "learning_rate": 4.46923937360179e-05,
      "loss": 0.5971,
      "step": 950
    },
    {
      "epoch": 1.0738255033557047,
      "grad_norm": 3.642073392868042,
      "learning_rate": 4.463646532438479e-05,
      "loss": 0.6882,
      "step": 960
    },
    {
      "epoch": 1.0850111856823266,
      "grad_norm": 28.307329177856445,
      "learning_rate": 4.458053691275168e-05,
      "loss": 0.9088,
      "step": 970
    },
    {
      "epoch": 1.0961968680089484,
      "grad_norm": 13.01570987701416,
      "learning_rate": 4.452460850111857e-05,
      "loss": 0.8025,
      "step": 980
    },
    {
      "epoch": 1.1073825503355705,
      "grad_norm": 3.527245283126831,
      "learning_rate": 4.446868008948546e-05,
      "loss": 1.0057,
      "step": 990
    },
    {
      "epoch": 1.1185682326621924,
      "grad_norm": 17.25554847717285,
      "learning_rate": 4.441275167785235e-05,
      "loss": 0.82,
      "step": 1000
    },
    {
      "epoch": 1.1297539149888143,
      "grad_norm": 7.181730270385742,
      "learning_rate": 4.435682326621924e-05,
      "loss": 0.6974,
      "step": 1010
    },
    {
      "epoch": 1.1409395973154361,
      "grad_norm": 26.030277252197266,
      "learning_rate": 4.4300894854586135e-05,
      "loss": 0.7294,
      "step": 1020
    },
    {
      "epoch": 1.1521252796420582,
      "grad_norm": 6.602819919586182,
      "learning_rate": 4.424496644295302e-05,
      "loss": 0.6024,
      "step": 1030
    },
    {
      "epoch": 1.1633109619686801,
      "grad_norm": 6.148924350738525,
      "learning_rate": 4.4189038031319915e-05,
      "loss": 0.6758,
      "step": 1040
    },
    {
      "epoch": 1.174496644295302,
      "grad_norm": 8.926287651062012,
      "learning_rate": 4.41331096196868e-05,
      "loss": 0.8391,
      "step": 1050
    },
    {
      "epoch": 1.1856823266219239,
      "grad_norm": 19.67667007446289,
      "learning_rate": 4.4077181208053694e-05,
      "loss": 0.6954,
      "step": 1060
    },
    {
      "epoch": 1.196868008948546,
      "grad_norm": 16.784767150878906,
      "learning_rate": 4.402125279642058e-05,
      "loss": 0.9211,
      "step": 1070
    },
    {
      "epoch": 1.2080536912751678,
      "grad_norm": 17.15730857849121,
      "learning_rate": 4.3965324384787474e-05,
      "loss": 1.0526,
      "step": 1080
    },
    {
      "epoch": 1.2192393736017897,
      "grad_norm": 4.435328006744385,
      "learning_rate": 4.390939597315437e-05,
      "loss": 0.7304,
      "step": 1090
    },
    {
      "epoch": 1.2304250559284116,
      "grad_norm": 9.929313659667969,
      "learning_rate": 4.3853467561521254e-05,
      "loss": 0.7266,
      "step": 1100
    },
    {
      "epoch": 1.2416107382550337,
      "grad_norm": 19.250404357910156,
      "learning_rate": 4.379753914988815e-05,
      "loss": 0.6875,
      "step": 1110
    },
    {
      "epoch": 1.2527964205816555,
      "grad_norm": 32.14577865600586,
      "learning_rate": 4.3741610738255034e-05,
      "loss": 0.8477,
      "step": 1120
    },
    {
      "epoch": 1.2639821029082774,
      "grad_norm": 8.198593139648438,
      "learning_rate": 4.368568232662193e-05,
      "loss": 1.0781,
      "step": 1130
    },
    {
      "epoch": 1.2751677852348993,
      "grad_norm": 20.46139907836914,
      "learning_rate": 4.362975391498882e-05,
      "loss": 0.8174,
      "step": 1140
    },
    {
      "epoch": 1.2863534675615211,
      "grad_norm": 15.483397483825684,
      "learning_rate": 4.3573825503355707e-05,
      "loss": 0.9977,
      "step": 1150
    },
    {
      "epoch": 1.2975391498881432,
      "grad_norm": 5.76029634475708,
      "learning_rate": 4.35178970917226e-05,
      "loss": 0.6763,
      "step": 1160
    },
    {
      "epoch": 1.308724832214765,
      "grad_norm": 10.269030570983887,
      "learning_rate": 4.3461968680089486e-05,
      "loss": 0.7928,
      "step": 1170
    },
    {
      "epoch": 1.319910514541387,
      "grad_norm": 6.759275913238525,
      "learning_rate": 4.340604026845638e-05,
      "loss": 0.8638,
      "step": 1180
    },
    {
      "epoch": 1.331096196868009,
      "grad_norm": 5.228484630584717,
      "learning_rate": 4.3350111856823266e-05,
      "loss": 0.6441,
      "step": 1190
    },
    {
      "epoch": 1.342281879194631,
      "grad_norm": 8.114334106445312,
      "learning_rate": 4.329418344519016e-05,
      "loss": 0.8627,
      "step": 1200
    },
    {
      "epoch": 1.3534675615212528,
      "grad_norm": 7.284810543060303,
      "learning_rate": 4.323825503355705e-05,
      "loss": 0.7164,
      "step": 1210
    },
    {
      "epoch": 1.3646532438478747,
      "grad_norm": 8.0982666015625,
      "learning_rate": 4.318232662192394e-05,
      "loss": 0.4919,
      "step": 1220
    },
    {
      "epoch": 1.3758389261744965,
      "grad_norm": 16.20885467529297,
      "learning_rate": 4.312639821029083e-05,
      "loss": 0.6471,
      "step": 1230
    },
    {
      "epoch": 1.3870246085011186,
      "grad_norm": 10.983832359313965,
      "learning_rate": 4.307046979865772e-05,
      "loss": 0.6874,
      "step": 1240
    },
    {
      "epoch": 1.3982102908277405,
      "grad_norm": 7.505484580993652,
      "learning_rate": 4.301454138702461e-05,
      "loss": 0.9341,
      "step": 1250
    },
    {
      "epoch": 1.4093959731543624,
      "grad_norm": 13.084773063659668,
      "learning_rate": 4.29586129753915e-05,
      "loss": 0.521,
      "step": 1260
    },
    {
      "epoch": 1.4205816554809845,
      "grad_norm": 9.088236808776855,
      "learning_rate": 4.290268456375839e-05,
      "loss": 0.834,
      "step": 1270
    },
    {
      "epoch": 1.4317673378076063,
      "grad_norm": 2.1336796283721924,
      "learning_rate": 4.2846756152125285e-05,
      "loss": 0.5226,
      "step": 1280
    },
    {
      "epoch": 1.4429530201342282,
      "grad_norm": 16.27454948425293,
      "learning_rate": 4.279082774049217e-05,
      "loss": 0.7618,
      "step": 1290
    },
    {
      "epoch": 1.45413870246085,
      "grad_norm": 14.340824127197266,
      "learning_rate": 4.2734899328859065e-05,
      "loss": 0.5448,
      "step": 1300
    },
    {
      "epoch": 1.465324384787472,
      "grad_norm": 15.259593963623047,
      "learning_rate": 4.267897091722595e-05,
      "loss": 0.8177,
      "step": 1310
    },
    {
      "epoch": 1.476510067114094,
      "grad_norm": 8.941088676452637,
      "learning_rate": 4.2623042505592845e-05,
      "loss": 0.8583,
      "step": 1320
    },
    {
      "epoch": 1.487695749440716,
      "grad_norm": 21.135122299194336,
      "learning_rate": 4.256711409395973e-05,
      "loss": 0.7214,
      "step": 1330
    },
    {
      "epoch": 1.4988814317673378,
      "grad_norm": 15.676156044006348,
      "learning_rate": 4.2511185682326624e-05,
      "loss": 0.7916,
      "step": 1340
    },
    {
      "epoch": 1.5100671140939599,
      "grad_norm": 18.62997817993164,
      "learning_rate": 4.245525727069352e-05,
      "loss": 0.6125,
      "step": 1350
    },
    {
      "epoch": 1.5212527964205815,
      "grad_norm": 12.759714126586914,
      "learning_rate": 4.2399328859060404e-05,
      "loss": 0.7712,
      "step": 1360
    },
    {
      "epoch": 1.5324384787472036,
      "grad_norm": 8.660520553588867,
      "learning_rate": 4.23434004474273e-05,
      "loss": 0.9854,
      "step": 1370
    },
    {
      "epoch": 1.5436241610738255,
      "grad_norm": 9.842245101928711,
      "learning_rate": 4.2287472035794184e-05,
      "loss": 1.1476,
      "step": 1380
    },
    {
      "epoch": 1.5548098434004474,
      "grad_norm": 12.491311073303223,
      "learning_rate": 4.223154362416108e-05,
      "loss": 1.0757,
      "step": 1390
    },
    {
      "epoch": 1.5659955257270695,
      "grad_norm": 19.951984405517578,
      "learning_rate": 4.2175615212527964e-05,
      "loss": 0.7235,
      "step": 1400
    },
    {
      "epoch": 1.5771812080536913,
      "grad_norm": 16.025768280029297,
      "learning_rate": 4.211968680089486e-05,
      "loss": 0.6604,
      "step": 1410
    },
    {
      "epoch": 1.5883668903803132,
      "grad_norm": 26.734500885009766,
      "learning_rate": 4.206375838926175e-05,
      "loss": 0.7089,
      "step": 1420
    },
    {
      "epoch": 1.599552572706935,
      "grad_norm": 15.206144332885742,
      "learning_rate": 4.200782997762864e-05,
      "loss": 0.8532,
      "step": 1430
    },
    {
      "epoch": 1.610738255033557,
      "grad_norm": 38.69648361206055,
      "learning_rate": 4.195190156599553e-05,
      "loss": 0.7332,
      "step": 1440
    },
    {
      "epoch": 1.621923937360179,
      "grad_norm": 4.6525373458862305,
      "learning_rate": 4.1895973154362416e-05,
      "loss": 0.6235,
      "step": 1450
    },
    {
      "epoch": 1.633109619686801,
      "grad_norm": 3.8503575325012207,
      "learning_rate": 4.18400447427293e-05,
      "loss": 0.5693,
      "step": 1460
    },
    {
      "epoch": 1.6442953020134228,
      "grad_norm": 14.65234661102295,
      "learning_rate": 4.17841163310962e-05,
      "loss": 0.7046,
      "step": 1470
    },
    {
      "epoch": 1.6554809843400449,
      "grad_norm": 7.33839750289917,
      "learning_rate": 4.172818791946309e-05,
      "loss": 0.657,
      "step": 1480
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 9.892522811889648,
      "learning_rate": 4.167225950782998e-05,
      "loss": 0.7586,
      "step": 1490
    },
    {
      "epoch": 1.6778523489932886,
      "grad_norm": 14.555431365966797,
      "learning_rate": 4.161633109619687e-05,
      "loss": 0.7055,
      "step": 1500
    },
    {
      "epoch": 1.6890380313199105,
      "grad_norm": 28.105566024780273,
      "learning_rate": 4.156040268456376e-05,
      "loss": 0.8557,
      "step": 1510
    },
    {
      "epoch": 1.7002237136465324,
      "grad_norm": 7.20031213760376,
      "learning_rate": 4.150447427293065e-05,
      "loss": 0.5472,
      "step": 1520
    },
    {
      "epoch": 1.7114093959731544,
      "grad_norm": 17.08736801147461,
      "learning_rate": 4.1448545861297535e-05,
      "loss": 0.685,
      "step": 1530
    },
    {
      "epoch": 1.7225950782997763,
      "grad_norm": 12.679022789001465,
      "learning_rate": 4.1392617449664435e-05,
      "loss": 0.6725,
      "step": 1540
    },
    {
      "epoch": 1.7337807606263982,
      "grad_norm": 9.868219375610352,
      "learning_rate": 4.133668903803132e-05,
      "loss": 0.7118,
      "step": 1550
    },
    {
      "epoch": 1.7449664429530203,
      "grad_norm": 25.741071701049805,
      "learning_rate": 4.1280760626398215e-05,
      "loss": 1.0293,
      "step": 1560
    },
    {
      "epoch": 1.756152125279642,
      "grad_norm": 8.97280216217041,
      "learning_rate": 4.12248322147651e-05,
      "loss": 0.8624,
      "step": 1570
    },
    {
      "epoch": 1.767337807606264,
      "grad_norm": 5.909244060516357,
      "learning_rate": 4.1168903803131995e-05,
      "loss": 0.4979,
      "step": 1580
    },
    {
      "epoch": 1.778523489932886,
      "grad_norm": 9.875892639160156,
      "learning_rate": 4.111297539149888e-05,
      "loss": 0.5139,
      "step": 1590
    },
    {
      "epoch": 1.7897091722595078,
      "grad_norm": 10.19648551940918,
      "learning_rate": 4.1057046979865775e-05,
      "loss": 0.6168,
      "step": 1600
    },
    {
      "epoch": 1.8008948545861299,
      "grad_norm": 9.872163772583008,
      "learning_rate": 4.100111856823267e-05,
      "loss": 0.7521,
      "step": 1610
    },
    {
      "epoch": 1.8120805369127517,
      "grad_norm": 11.54631519317627,
      "learning_rate": 4.0945190156599555e-05,
      "loss": 0.8156,
      "step": 1620
    },
    {
      "epoch": 1.8232662192393736,
      "grad_norm": 17.6760311126709,
      "learning_rate": 4.088926174496645e-05,
      "loss": 0.8111,
      "step": 1630
    },
    {
      "epoch": 1.8344519015659957,
      "grad_norm": 6.5028605461120605,
      "learning_rate": 4.0833333333333334e-05,
      "loss": 0.5489,
      "step": 1640
    },
    {
      "epoch": 1.8456375838926173,
      "grad_norm": 23.788354873657227,
      "learning_rate": 4.077740492170022e-05,
      "loss": 0.691,
      "step": 1650
    },
    {
      "epoch": 1.8568232662192394,
      "grad_norm": 9.916997909545898,
      "learning_rate": 4.0721476510067114e-05,
      "loss": 0.6936,
      "step": 1660
    },
    {
      "epoch": 1.8680089485458613,
      "grad_norm": 4.999641418457031,
      "learning_rate": 4.066554809843401e-05,
      "loss": 0.5518,
      "step": 1670
    },
    {
      "epoch": 1.8791946308724832,
      "grad_norm": 3.560835599899292,
      "learning_rate": 4.06096196868009e-05,
      "loss": 0.6967,
      "step": 1680
    },
    {
      "epoch": 1.8903803131991053,
      "grad_norm": 6.980071544647217,
      "learning_rate": 4.055369127516779e-05,
      "loss": 0.697,
      "step": 1690
    },
    {
      "epoch": 1.901565995525727,
      "grad_norm": 59.474056243896484,
      "learning_rate": 4.049776286353468e-05,
      "loss": 0.699,
      "step": 1700
    },
    {
      "epoch": 1.912751677852349,
      "grad_norm": 9.88122844696045,
      "learning_rate": 4.044183445190157e-05,
      "loss": 0.6882,
      "step": 1710
    },
    {
      "epoch": 1.9239373601789709,
      "grad_norm": 8.246049880981445,
      "learning_rate": 4.038590604026845e-05,
      "loss": 0.8009,
      "step": 1720
    },
    {
      "epoch": 1.9351230425055927,
      "grad_norm": 8.473400115966797,
      "learning_rate": 4.032997762863535e-05,
      "loss": 0.5538,
      "step": 1730
    },
    {
      "epoch": 1.9463087248322148,
      "grad_norm": 26.7039737701416,
      "learning_rate": 4.027404921700224e-05,
      "loss": 0.9123,
      "step": 1740
    },
    {
      "epoch": 1.9574944071588367,
      "grad_norm": 4.037168979644775,
      "learning_rate": 4.021812080536913e-05,
      "loss": 0.6745,
      "step": 1750
    },
    {
      "epoch": 1.9686800894854586,
      "grad_norm": 30.265138626098633,
      "learning_rate": 4.016219239373602e-05,
      "loss": 0.8501,
      "step": 1760
    },
    {
      "epoch": 1.9798657718120807,
      "grad_norm": 9.820235252380371,
      "learning_rate": 4.010626398210291e-05,
      "loss": 0.373,
      "step": 1770
    },
    {
      "epoch": 1.9910514541387023,
      "grad_norm": 17.434476852416992,
      "learning_rate": 4.00503355704698e-05,
      "loss": 0.5631,
      "step": 1780
    },
    {
      "epoch": 2.0,
      "eval_f1": 0.697581693045579,
      "eval_loss": 0.8484057784080505,
      "eval_precision": 0.6916700879765761,
      "eval_recall": 0.7089700996677741,
      "eval_runtime": 137.5709,
      "eval_samples_per_second": 6.498,
      "eval_steps_per_second": 0.814,
      "step": 1788
    },
    {
      "epoch": 2.0022371364653244,
      "grad_norm": 9.519254684448242,
      "learning_rate": 3.9994407158836686e-05,
      "loss": 0.6216,
      "step": 1790
    },
    {
      "epoch": 2.0134228187919465,
      "grad_norm": 15.892457962036133,
      "learning_rate": 3.9938478747203586e-05,
      "loss": 0.7053,
      "step": 1800
    },
    {
      "epoch": 2.024608501118568,
      "grad_norm": 7.562903881072998,
      "learning_rate": 3.988255033557047e-05,
      "loss": 0.6053,
      "step": 1810
    },
    {
      "epoch": 2.0357941834451903,
      "grad_norm": 6.340253829956055,
      "learning_rate": 3.9826621923937366e-05,
      "loss": 0.471,
      "step": 1820
    },
    {
      "epoch": 2.046979865771812,
      "grad_norm": 4.557652473449707,
      "learning_rate": 3.977069351230425e-05,
      "loss": 0.5075,
      "step": 1830
    },
    {
      "epoch": 2.058165548098434,
      "grad_norm": 20.729930877685547,
      "learning_rate": 3.971476510067114e-05,
      "loss": 0.4256,
      "step": 1840
    },
    {
      "epoch": 2.069351230425056,
      "grad_norm": 7.229117393493652,
      "learning_rate": 3.965883668903803e-05,
      "loss": 0.6363,
      "step": 1850
    },
    {
      "epoch": 2.0805369127516777,
      "grad_norm": 6.887569904327393,
      "learning_rate": 3.960290827740492e-05,
      "loss": 0.3741,
      "step": 1860
    },
    {
      "epoch": 2.0917225950783,
      "grad_norm": 17.128570556640625,
      "learning_rate": 3.954697986577182e-05,
      "loss": 0.741,
      "step": 1870
    },
    {
      "epoch": 2.1029082774049215,
      "grad_norm": 3.822514295578003,
      "learning_rate": 3.9491051454138705e-05,
      "loss": 0.4629,
      "step": 1880
    },
    {
      "epoch": 2.1140939597315436,
      "grad_norm": 7.974888324737549,
      "learning_rate": 3.94351230425056e-05,
      "loss": 0.891,
      "step": 1890
    },
    {
      "epoch": 2.1252796420581657,
      "grad_norm": 13.44155216217041,
      "learning_rate": 3.9379194630872485e-05,
      "loss": 0.4781,
      "step": 1900
    },
    {
      "epoch": 2.1364653243847873,
      "grad_norm": 19.126819610595703,
      "learning_rate": 3.932326621923937e-05,
      "loss": 0.9359,
      "step": 1910
    },
    {
      "epoch": 2.1476510067114094,
      "grad_norm": 5.1482834815979,
      "learning_rate": 3.9267337807606264e-05,
      "loss": 0.6551,
      "step": 1920
    },
    {
      "epoch": 2.1588366890380315,
      "grad_norm": 7.147889614105225,
      "learning_rate": 3.921140939597316e-05,
      "loss": 0.7908,
      "step": 1930
    },
    {
      "epoch": 2.170022371364653,
      "grad_norm": 10.415604591369629,
      "learning_rate": 3.915548098434005e-05,
      "loss": 0.2807,
      "step": 1940
    },
    {
      "epoch": 2.1812080536912752,
      "grad_norm": 9.136021614074707,
      "learning_rate": 3.909955257270694e-05,
      "loss": 0.7579,
      "step": 1950
    },
    {
      "epoch": 2.192393736017897,
      "grad_norm": 10.137175559997559,
      "learning_rate": 3.904362416107383e-05,
      "loss": 0.8536,
      "step": 1960
    },
    {
      "epoch": 2.203579418344519,
      "grad_norm": 7.060102462768555,
      "learning_rate": 3.898769574944072e-05,
      "loss": 0.3377,
      "step": 1970
    },
    {
      "epoch": 2.214765100671141,
      "grad_norm": 3.610506057739258,
      "learning_rate": 3.8931767337807604e-05,
      "loss": 0.6559,
      "step": 1980
    },
    {
      "epoch": 2.2259507829977627,
      "grad_norm": 2.602890729904175,
      "learning_rate": 3.88758389261745e-05,
      "loss": 0.5494,
      "step": 1990
    },
    {
      "epoch": 2.237136465324385,
      "grad_norm": 9.927203178405762,
      "learning_rate": 3.881991051454139e-05,
      "loss": 0.7323,
      "step": 2000
    },
    {
      "epoch": 2.248322147651007,
      "grad_norm": 11.357600212097168,
      "learning_rate": 3.8763982102908283e-05,
      "loss": 0.6562,
      "step": 2010
    },
    {
      "epoch": 2.2595078299776286,
      "grad_norm": 5.039797782897949,
      "learning_rate": 3.870805369127517e-05,
      "loss": 0.5521,
      "step": 2020
    },
    {
      "epoch": 2.2706935123042506,
      "grad_norm": 10.048912048339844,
      "learning_rate": 3.8652125279642056e-05,
      "loss": 0.5925,
      "step": 2030
    },
    {
      "epoch": 2.2818791946308723,
      "grad_norm": 12.843436241149902,
      "learning_rate": 3.859619686800895e-05,
      "loss": 0.6831,
      "step": 2040
    },
    {
      "epoch": 2.2930648769574944,
      "grad_norm": 15.140279769897461,
      "learning_rate": 3.8540268456375836e-05,
      "loss": 0.837,
      "step": 2050
    },
    {
      "epoch": 2.3042505592841165,
      "grad_norm": 16.42499542236328,
      "learning_rate": 3.8484340044742736e-05,
      "loss": 0.7099,
      "step": 2060
    },
    {
      "epoch": 2.315436241610738,
      "grad_norm": 5.391152858734131,
      "learning_rate": 3.842841163310962e-05,
      "loss": 0.4391,
      "step": 2070
    },
    {
      "epoch": 2.3266219239373602,
      "grad_norm": 5.903621196746826,
      "learning_rate": 3.8372483221476516e-05,
      "loss": 0.3544,
      "step": 2080
    },
    {
      "epoch": 2.337807606263982,
      "grad_norm": 13.316259384155273,
      "learning_rate": 3.83165548098434e-05,
      "loss": 0.4597,
      "step": 2090
    },
    {
      "epoch": 2.348993288590604,
      "grad_norm": 6.594472408294678,
      "learning_rate": 3.826062639821029e-05,
      "loss": 0.6472,
      "step": 2100
    },
    {
      "epoch": 2.360178970917226,
      "grad_norm": 37.75844192504883,
      "learning_rate": 3.820469798657718e-05,
      "loss": 0.6121,
      "step": 2110
    },
    {
      "epoch": 2.3713646532438477,
      "grad_norm": 4.9652252197265625,
      "learning_rate": 3.814876957494407e-05,
      "loss": 0.8586,
      "step": 2120
    },
    {
      "epoch": 2.38255033557047,
      "grad_norm": 113.20941162109375,
      "learning_rate": 3.809284116331097e-05,
      "loss": 0.5038,
      "step": 2130
    },
    {
      "epoch": 2.393736017897092,
      "grad_norm": 23.188661575317383,
      "learning_rate": 3.8036912751677855e-05,
      "loss": 0.5114,
      "step": 2140
    },
    {
      "epoch": 2.4049217002237135,
      "grad_norm": 26.193689346313477,
      "learning_rate": 3.798098434004475e-05,
      "loss": 0.4327,
      "step": 2150
    },
    {
      "epoch": 2.4161073825503356,
      "grad_norm": 28.367130279541016,
      "learning_rate": 3.7925055928411635e-05,
      "loss": 0.6592,
      "step": 2160
    },
    {
      "epoch": 2.4272930648769577,
      "grad_norm": 17.0211181640625,
      "learning_rate": 3.786912751677852e-05,
      "loss": 0.5222,
      "step": 2170
    },
    {
      "epoch": 2.4384787472035794,
      "grad_norm": 25.867111206054688,
      "learning_rate": 3.7813199105145415e-05,
      "loss": 0.819,
      "step": 2180
    },
    {
      "epoch": 2.4496644295302015,
      "grad_norm": 11.60964298248291,
      "learning_rate": 3.775727069351231e-05,
      "loss": 0.4414,
      "step": 2190
    },
    {
      "epoch": 2.460850111856823,
      "grad_norm": 1.833175778388977,
      "learning_rate": 3.77013422818792e-05,
      "loss": 0.6173,
      "step": 2200
    },
    {
      "epoch": 2.472035794183445,
      "grad_norm": 20.375244140625,
      "learning_rate": 3.764541387024609e-05,
      "loss": 0.8659,
      "step": 2210
    },
    {
      "epoch": 2.4832214765100673,
      "grad_norm": 15.494379043579102,
      "learning_rate": 3.7589485458612974e-05,
      "loss": 0.6124,
      "step": 2220
    },
    {
      "epoch": 2.494407158836689,
      "grad_norm": 13.215065956115723,
      "learning_rate": 3.753355704697987e-05,
      "loss": 0.4511,
      "step": 2230
    },
    {
      "epoch": 2.505592841163311,
      "grad_norm": 20.694412231445312,
      "learning_rate": 3.7477628635346754e-05,
      "loss": 0.4988,
      "step": 2240
    },
    {
      "epoch": 2.5167785234899327,
      "grad_norm": 4.317370891571045,
      "learning_rate": 3.742170022371365e-05,
      "loss": 0.3208,
      "step": 2250
    },
    {
      "epoch": 2.527964205816555,
      "grad_norm": 29.796871185302734,
      "learning_rate": 3.736577181208054e-05,
      "loss": 0.6112,
      "step": 2260
    },
    {
      "epoch": 2.539149888143177,
      "grad_norm": 26.735919952392578,
      "learning_rate": 3.7309843400447434e-05,
      "loss": 0.6082,
      "step": 2270
    },
    {
      "epoch": 2.5503355704697985,
      "grad_norm": 36.557586669921875,
      "learning_rate": 3.725391498881432e-05,
      "loss": 0.7513,
      "step": 2280
    },
    {
      "epoch": 2.5615212527964206,
      "grad_norm": 8.811062812805176,
      "learning_rate": 3.719798657718121e-05,
      "loss": 0.3041,
      "step": 2290
    },
    {
      "epoch": 2.5727069351230423,
      "grad_norm": 7.671477317810059,
      "learning_rate": 3.71420581655481e-05,
      "loss": 0.4798,
      "step": 2300
    },
    {
      "epoch": 2.5838926174496644,
      "grad_norm": 10.659791946411133,
      "learning_rate": 3.7086129753914987e-05,
      "loss": 0.5906,
      "step": 2310
    },
    {
      "epoch": 2.5950782997762865,
      "grad_norm": 1.4457719326019287,
      "learning_rate": 3.703020134228188e-05,
      "loss": 0.6121,
      "step": 2320
    },
    {
      "epoch": 2.6062639821029085,
      "grad_norm": 1.5767048597335815,
      "learning_rate": 3.697427293064877e-05,
      "loss": 0.4366,
      "step": 2330
    },
    {
      "epoch": 2.61744966442953,
      "grad_norm": 34.99094009399414,
      "learning_rate": 3.691834451901566e-05,
      "loss": 0.5355,
      "step": 2340
    },
    {
      "epoch": 2.6286353467561523,
      "grad_norm": 16.700756072998047,
      "learning_rate": 3.686241610738255e-05,
      "loss": 0.4828,
      "step": 2350
    },
    {
      "epoch": 2.639821029082774,
      "grad_norm": 5.669839382171631,
      "learning_rate": 3.680648769574944e-05,
      "loss": 0.2151,
      "step": 2360
    },
    {
      "epoch": 2.651006711409396,
      "grad_norm": 14.28536605834961,
      "learning_rate": 3.675055928411633e-05,
      "loss": 0.6903,
      "step": 2370
    },
    {
      "epoch": 2.662192393736018,
      "grad_norm": 0.9947927594184875,
      "learning_rate": 3.669463087248322e-05,
      "loss": 0.2717,
      "step": 2380
    },
    {
      "epoch": 2.6733780760626398,
      "grad_norm": 2.6514408588409424,
      "learning_rate": 3.663870246085012e-05,
      "loss": 0.7001,
      "step": 2390
    },
    {
      "epoch": 2.684563758389262,
      "grad_norm": 12.299219131469727,
      "learning_rate": 3.6582774049217006e-05,
      "loss": 1.0402,
      "step": 2400
    },
    {
      "epoch": 2.6957494407158835,
      "grad_norm": 68.77140808105469,
      "learning_rate": 3.652684563758389e-05,
      "loss": 0.7087,
      "step": 2410
    },
    {
      "epoch": 2.7069351230425056,
      "grad_norm": 21.27466583251953,
      "learning_rate": 3.6470917225950785e-05,
      "loss": 0.7123,
      "step": 2420
    },
    {
      "epoch": 2.7181208053691277,
      "grad_norm": 4.810205459594727,
      "learning_rate": 3.641498881431767e-05,
      "loss": 0.3555,
      "step": 2430
    },
    {
      "epoch": 2.7293064876957494,
      "grad_norm": 49.39876174926758,
      "learning_rate": 3.6359060402684565e-05,
      "loss": 0.4931,
      "step": 2440
    },
    {
      "epoch": 2.7404921700223714,
      "grad_norm": 20.950857162475586,
      "learning_rate": 3.630313199105145e-05,
      "loss": 0.3513,
      "step": 2450
    },
    {
      "epoch": 2.751677852348993,
      "grad_norm": 101.11701202392578,
      "learning_rate": 3.624720357941835e-05,
      "loss": 0.6427,
      "step": 2460
    },
    {
      "epoch": 2.762863534675615,
      "grad_norm": 0.4060044288635254,
      "learning_rate": 3.619127516778524e-05,
      "loss": 0.3604,
      "step": 2470
    },
    {
      "epoch": 2.7740492170022373,
      "grad_norm": 3.827799081802368,
      "learning_rate": 3.6135346756152125e-05,
      "loss": 0.671,
      "step": 2480
    },
    {
      "epoch": 2.785234899328859,
      "grad_norm": 143.33856201171875,
      "learning_rate": 3.607941834451902e-05,
      "loss": 0.6631,
      "step": 2490
    },
    {
      "epoch": 2.796420581655481,
      "grad_norm": 11.15935230255127,
      "learning_rate": 3.6023489932885904e-05,
      "loss": 0.6146,
      "step": 2500
    },
    {
      "epoch": 2.8076062639821027,
      "grad_norm": 70.11410522460938,
      "learning_rate": 3.59675615212528e-05,
      "loss": 0.666,
      "step": 2510
    },
    {
      "epoch": 2.8187919463087248,
      "grad_norm": 13.382741928100586,
      "learning_rate": 3.591163310961969e-05,
      "loss": 0.4719,
      "step": 2520
    },
    {
      "epoch": 2.829977628635347,
      "grad_norm": 10.003108024597168,
      "learning_rate": 3.585570469798658e-05,
      "loss": 0.4569,
      "step": 2530
    },
    {
      "epoch": 2.841163310961969,
      "grad_norm": 28.13572120666504,
      "learning_rate": 3.579977628635347e-05,
      "loss": 0.5834,
      "step": 2540
    },
    {
      "epoch": 2.8523489932885906,
      "grad_norm": 20.774343490600586,
      "learning_rate": 3.574384787472036e-05,
      "loss": 0.7567,
      "step": 2550
    },
    {
      "epoch": 2.8635346756152127,
      "grad_norm": 1.0220783948898315,
      "learning_rate": 3.568791946308725e-05,
      "loss": 0.4937,
      "step": 2560
    },
    {
      "epoch": 2.8747203579418343,
      "grad_norm": 5.255212783813477,
      "learning_rate": 3.563199105145414e-05,
      "loss": 0.5542,
      "step": 2570
    },
    {
      "epoch": 2.8859060402684564,
      "grad_norm": 18.407949447631836,
      "learning_rate": 3.557606263982103e-05,
      "loss": 0.7926,
      "step": 2580
    },
    {
      "epoch": 2.8970917225950785,
      "grad_norm": 45.13735580444336,
      "learning_rate": 3.5520134228187923e-05,
      "loss": 0.531,
      "step": 2590
    },
    {
      "epoch": 2.9082774049217,
      "grad_norm": 26.8133487701416,
      "learning_rate": 3.546420581655481e-05,
      "loss": 0.4258,
      "step": 2600
    },
    {
      "epoch": 2.9194630872483223,
      "grad_norm": 5.964059829711914,
      "learning_rate": 3.54082774049217e-05,
      "loss": 0.3987,
      "step": 2610
    },
    {
      "epoch": 2.930648769574944,
      "grad_norm": 22.760478973388672,
      "learning_rate": 3.535234899328859e-05,
      "loss": 0.6332,
      "step": 2620
    },
    {
      "epoch": 2.941834451901566,
      "grad_norm": 0.7345831394195557,
      "learning_rate": 3.529642058165548e-05,
      "loss": 0.4693,
      "step": 2630
    },
    {
      "epoch": 2.953020134228188,
      "grad_norm": 3.421128749847412,
      "learning_rate": 3.524049217002237e-05,
      "loss": 0.6268,
      "step": 2640
    },
    {
      "epoch": 2.9642058165548097,
      "grad_norm": 13.85600757598877,
      "learning_rate": 3.518456375838926e-05,
      "loss": 0.5182,
      "step": 2650
    },
    {
      "epoch": 2.975391498881432,
      "grad_norm": 4.945644378662109,
      "learning_rate": 3.5128635346756156e-05,
      "loss": 0.7271,
      "step": 2660
    },
    {
      "epoch": 2.9865771812080535,
      "grad_norm": 20.465927124023438,
      "learning_rate": 3.507270693512304e-05,
      "loss": 0.6272,
      "step": 2670
    },
    {
      "epoch": 2.9977628635346756,
      "grad_norm": 32.25503158569336,
      "learning_rate": 3.5016778523489936e-05,
      "loss": 0.7387,
      "step": 2680
    },
    {
      "epoch": 3.0,
      "eval_f1": 0.7033037957038283,
      "eval_loss": 0.7267529964447021,
      "eval_precision": 0.6845000869876583,
      "eval_recall": 0.7282392026578073,
      "eval_runtime": 121.5004,
      "eval_samples_per_second": 7.358,
      "eval_steps_per_second": 0.922,
      "step": 2682
    },
    {
      "epoch": 3.0089485458612977,
      "grad_norm": 7.932214260101318,
      "learning_rate": 3.496085011185682e-05,
      "loss": 0.2897,
      "step": 2690
    },
    {
      "epoch": 3.0201342281879193,
      "grad_norm": 3.4759769439697266,
      "learning_rate": 3.4904921700223715e-05,
      "loss": 0.4404,
      "step": 2700
    },
    {
      "epoch": 3.0313199105145414,
      "grad_norm": 0.40191611647605896,
      "learning_rate": 3.48489932885906e-05,
      "loss": 0.3434,
      "step": 2710
    },
    {
      "epoch": 3.0425055928411635,
      "grad_norm": 14.811920166015625,
      "learning_rate": 3.4793064876957495e-05,
      "loss": 0.4998,
      "step": 2720
    },
    {
      "epoch": 3.053691275167785,
      "grad_norm": 6.55050802230835,
      "learning_rate": 3.473713646532439e-05,
      "loss": 0.2497,
      "step": 2730
    },
    {
      "epoch": 3.0648769574944073,
      "grad_norm": 1.6551542282104492,
      "learning_rate": 3.4681208053691275e-05,
      "loss": 0.5421,
      "step": 2740
    },
    {
      "epoch": 3.076062639821029,
      "grad_norm": 99.59843444824219,
      "learning_rate": 3.462527964205817e-05,
      "loss": 0.5146,
      "step": 2750
    },
    {
      "epoch": 3.087248322147651,
      "grad_norm": 11.830347061157227,
      "learning_rate": 3.4569351230425055e-05,
      "loss": 0.5131,
      "step": 2760
    },
    {
      "epoch": 3.098434004474273,
      "grad_norm": 24.406038284301758,
      "learning_rate": 3.451342281879195e-05,
      "loss": 0.3094,
      "step": 2770
    },
    {
      "epoch": 3.1096196868008947,
      "grad_norm": 1.3547981977462769,
      "learning_rate": 3.4457494407158835e-05,
      "loss": 0.8223,
      "step": 2780
    },
    {
      "epoch": 3.120805369127517,
      "grad_norm": 6.757659912109375,
      "learning_rate": 3.440156599552573e-05,
      "loss": 0.306,
      "step": 2790
    },
    {
      "epoch": 3.131991051454139,
      "grad_norm": 6.851712226867676,
      "learning_rate": 3.434563758389262e-05,
      "loss": 0.472,
      "step": 2800
    },
    {
      "epoch": 3.1431767337807606,
      "grad_norm": 15.759920120239258,
      "learning_rate": 3.428970917225951e-05,
      "loss": 0.4395,
      "step": 2810
    },
    {
      "epoch": 3.1543624161073827,
      "grad_norm": 30.686038970947266,
      "learning_rate": 3.42337807606264e-05,
      "loss": 0.5052,
      "step": 2820
    },
    {
      "epoch": 3.1655480984340043,
      "grad_norm": 4.225700855255127,
      "learning_rate": 3.417785234899329e-05,
      "loss": 0.3553,
      "step": 2830
    },
    {
      "epoch": 3.1767337807606264,
      "grad_norm": 0.3738051950931549,
      "learning_rate": 3.412192393736018e-05,
      "loss": 0.3553,
      "step": 2840
    },
    {
      "epoch": 3.1879194630872485,
      "grad_norm": 5.1415886878967285,
      "learning_rate": 3.4065995525727074e-05,
      "loss": 0.7687,
      "step": 2850
    },
    {
      "epoch": 3.19910514541387,
      "grad_norm": 1.3753386735916138,
      "learning_rate": 3.401006711409396e-05,
      "loss": 0.448,
      "step": 2860
    },
    {
      "epoch": 3.2102908277404922,
      "grad_norm": 12.891637802124023,
      "learning_rate": 3.3954138702460854e-05,
      "loss": 0.2728,
      "step": 2870
    },
    {
      "epoch": 3.221476510067114,
      "grad_norm": 1.544844150543213,
      "learning_rate": 3.389821029082774e-05,
      "loss": 0.2794,
      "step": 2880
    },
    {
      "epoch": 3.232662192393736,
      "grad_norm": 9.19554615020752,
      "learning_rate": 3.384228187919463e-05,
      "loss": 0.6449,
      "step": 2890
    },
    {
      "epoch": 3.243847874720358,
      "grad_norm": 4.993233680725098,
      "learning_rate": 3.378635346756152e-05,
      "loss": 0.3903,
      "step": 2900
    },
    {
      "epoch": 3.2550335570469797,
      "grad_norm": 43.91837692260742,
      "learning_rate": 3.373042505592841e-05,
      "loss": 0.7346,
      "step": 2910
    },
    {
      "epoch": 3.266219239373602,
      "grad_norm": 9.950104713439941,
      "learning_rate": 3.3674496644295306e-05,
      "loss": 0.5671,
      "step": 2920
    },
    {
      "epoch": 3.277404921700224,
      "grad_norm": 4.973111152648926,
      "learning_rate": 3.361856823266219e-05,
      "loss": 0.3714,
      "step": 2930
    },
    {
      "epoch": 3.2885906040268456,
      "grad_norm": 78.0310287475586,
      "learning_rate": 3.3562639821029086e-05,
      "loss": 0.455,
      "step": 2940
    },
    {
      "epoch": 3.2997762863534676,
      "grad_norm": 4.079374313354492,
      "learning_rate": 3.350671140939597e-05,
      "loss": 0.3001,
      "step": 2950
    },
    {
      "epoch": 3.3109619686800893,
      "grad_norm": 3.4112424850463867,
      "learning_rate": 3.3450782997762866e-05,
      "loss": 0.4483,
      "step": 2960
    },
    {
      "epoch": 3.3221476510067114,
      "grad_norm": 0.28285524249076843,
      "learning_rate": 3.339485458612975e-05,
      "loss": 0.3566,
      "step": 2970
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 133.24440002441406,
      "learning_rate": 3.3338926174496646e-05,
      "loss": 0.5162,
      "step": 2980
    },
    {
      "epoch": 3.344519015659955,
      "grad_norm": 18.69304084777832,
      "learning_rate": 3.328299776286354e-05,
      "loss": 0.5256,
      "step": 2990
    },
    {
      "epoch": 3.3557046979865772,
      "grad_norm": 1.6832407712936401,
      "learning_rate": 3.3227069351230425e-05,
      "loss": 0.4673,
      "step": 3000
    },
    {
      "epoch": 3.3668903803131993,
      "grad_norm": 24.800296783447266,
      "learning_rate": 3.317114093959732e-05,
      "loss": 0.682,
      "step": 3010
    },
    {
      "epoch": 3.378076062639821,
      "grad_norm": 0.8571664094924927,
      "learning_rate": 3.3115212527964205e-05,
      "loss": 0.4176,
      "step": 3020
    },
    {
      "epoch": 3.389261744966443,
      "grad_norm": 53.360267639160156,
      "learning_rate": 3.30592841163311e-05,
      "loss": 0.5588,
      "step": 3030
    },
    {
      "epoch": 3.4004474272930647,
      "grad_norm": 0.31539562344551086,
      "learning_rate": 3.3003355704697985e-05,
      "loss": 0.4109,
      "step": 3040
    },
    {
      "epoch": 3.411633109619687,
      "grad_norm": 56.04905319213867,
      "learning_rate": 3.294742729306488e-05,
      "loss": 0.8367,
      "step": 3050
    },
    {
      "epoch": 3.422818791946309,
      "grad_norm": 50.48177719116211,
      "learning_rate": 3.289149888143177e-05,
      "loss": 0.1438,
      "step": 3060
    },
    {
      "epoch": 3.4340044742729305,
      "grad_norm": 11.831338882446289,
      "learning_rate": 3.283557046979866e-05,
      "loss": 0.6288,
      "step": 3070
    },
    {
      "epoch": 3.4451901565995526,
      "grad_norm": 14.653352737426758,
      "learning_rate": 3.277964205816555e-05,
      "loss": 0.6586,
      "step": 3080
    },
    {
      "epoch": 3.4563758389261743,
      "grad_norm": 56.56664276123047,
      "learning_rate": 3.272371364653244e-05,
      "loss": 1.0232,
      "step": 3090
    },
    {
      "epoch": 3.4675615212527964,
      "grad_norm": 3.37430477142334,
      "learning_rate": 3.266778523489933e-05,
      "loss": 0.4242,
      "step": 3100
    },
    {
      "epoch": 3.4787472035794185,
      "grad_norm": 4.762359619140625,
      "learning_rate": 3.261185682326622e-05,
      "loss": 0.6763,
      "step": 3110
    },
    {
      "epoch": 3.48993288590604,
      "grad_norm": 2.7594962120056152,
      "learning_rate": 3.255592841163311e-05,
      "loss": 0.4622,
      "step": 3120
    },
    {
      "epoch": 3.501118568232662,
      "grad_norm": 101.89916229248047,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 0.2633,
      "step": 3130
    },
    {
      "epoch": 3.512304250559284,
      "grad_norm": 0.4277253746986389,
      "learning_rate": 3.244407158836689e-05,
      "loss": 0.2225,
      "step": 3140
    },
    {
      "epoch": 3.523489932885906,
      "grad_norm": 1.0604619979858398,
      "learning_rate": 3.2388143176733784e-05,
      "loss": 0.4779,
      "step": 3150
    },
    {
      "epoch": 3.534675615212528,
      "grad_norm": 2.76371431350708,
      "learning_rate": 3.233221476510067e-05,
      "loss": 0.4355,
      "step": 3160
    },
    {
      "epoch": 3.54586129753915,
      "grad_norm": 14.408403396606445,
      "learning_rate": 3.2276286353467563e-05,
      "loss": 0.5026,
      "step": 3170
    },
    {
      "epoch": 3.557046979865772,
      "grad_norm": 1.8522816896438599,
      "learning_rate": 3.222035794183446e-05,
      "loss": 0.6151,
      "step": 3180
    },
    {
      "epoch": 3.568232662192394,
      "grad_norm": 15.014477729797363,
      "learning_rate": 3.216442953020134e-05,
      "loss": 0.5105,
      "step": 3190
    },
    {
      "epoch": 3.5794183445190155,
      "grad_norm": 47.70597457885742,
      "learning_rate": 3.2108501118568236e-05,
      "loss": 0.4836,
      "step": 3200
    },
    {
      "epoch": 3.5906040268456376,
      "grad_norm": 0.6302736401557922,
      "learning_rate": 3.205257270693512e-05,
      "loss": 0.3526,
      "step": 3210
    },
    {
      "epoch": 3.6017897091722597,
      "grad_norm": 9.632420539855957,
      "learning_rate": 3.1996644295302016e-05,
      "loss": 0.501,
      "step": 3220
    },
    {
      "epoch": 3.6129753914988814,
      "grad_norm": 20.426422119140625,
      "learning_rate": 3.19407158836689e-05,
      "loss": 0.5697,
      "step": 3230
    },
    {
      "epoch": 3.6241610738255035,
      "grad_norm": 1.1134775876998901,
      "learning_rate": 3.1884787472035796e-05,
      "loss": 0.5351,
      "step": 3240
    },
    {
      "epoch": 3.635346756152125,
      "grad_norm": 0.34916529059410095,
      "learning_rate": 3.182885906040269e-05,
      "loss": 0.2733,
      "step": 3250
    },
    {
      "epoch": 3.646532438478747,
      "grad_norm": 38.61363983154297,
      "learning_rate": 3.1772930648769576e-05,
      "loss": 0.5082,
      "step": 3260
    },
    {
      "epoch": 3.6577181208053693,
      "grad_norm": 1.7942607402801514,
      "learning_rate": 3.171700223713647e-05,
      "loss": 0.7043,
      "step": 3270
    },
    {
      "epoch": 3.668903803131991,
      "grad_norm": 13.846588134765625,
      "learning_rate": 3.1661073825503355e-05,
      "loss": 0.6801,
      "step": 3280
    },
    {
      "epoch": 3.680089485458613,
      "grad_norm": 21.500337600708008,
      "learning_rate": 3.160514541387025e-05,
      "loss": 0.2938,
      "step": 3290
    },
    {
      "epoch": 3.6912751677852347,
      "grad_norm": 4.699738502502441,
      "learning_rate": 3.1549217002237135e-05,
      "loss": 0.4062,
      "step": 3300
    },
    {
      "epoch": 3.7024608501118568,
      "grad_norm": 44.61015701293945,
      "learning_rate": 3.149328859060403e-05,
      "loss": 0.4542,
      "step": 3310
    },
    {
      "epoch": 3.713646532438479,
      "grad_norm": 17.251245498657227,
      "learning_rate": 3.143736017897092e-05,
      "loss": 0.583,
      "step": 3320
    },
    {
      "epoch": 3.7248322147651005,
      "grad_norm": 3.7815053462982178,
      "learning_rate": 3.138143176733781e-05,
      "loss": 0.3404,
      "step": 3330
    },
    {
      "epoch": 3.7360178970917226,
      "grad_norm": 5.8157501220703125,
      "learning_rate": 3.13255033557047e-05,
      "loss": 0.4251,
      "step": 3340
    },
    {
      "epoch": 3.7472035794183443,
      "grad_norm": 27.221715927124023,
      "learning_rate": 3.126957494407159e-05,
      "loss": 0.4716,
      "step": 3350
    },
    {
      "epoch": 3.7583892617449663,
      "grad_norm": 11.114147186279297,
      "learning_rate": 3.121364653243848e-05,
      "loss": 0.3719,
      "step": 3360
    },
    {
      "epoch": 3.7695749440715884,
      "grad_norm": 2.545734405517578,
      "learning_rate": 3.115771812080537e-05,
      "loss": 0.4666,
      "step": 3370
    },
    {
      "epoch": 3.7807606263982105,
      "grad_norm": 15.743549346923828,
      "learning_rate": 3.110178970917226e-05,
      "loss": 0.2999,
      "step": 3380
    },
    {
      "epoch": 3.791946308724832,
      "grad_norm": 38.090023040771484,
      "learning_rate": 3.1045861297539154e-05,
      "loss": 0.7381,
      "step": 3390
    },
    {
      "epoch": 3.8031319910514543,
      "grad_norm": 56.96955490112305,
      "learning_rate": 3.098993288590604e-05,
      "loss": 0.4843,
      "step": 3400
    },
    {
      "epoch": 3.814317673378076,
      "grad_norm": 21.518037796020508,
      "learning_rate": 3.0934004474272934e-05,
      "loss": 0.5441,
      "step": 3410
    },
    {
      "epoch": 3.825503355704698,
      "grad_norm": 18.013525009155273,
      "learning_rate": 3.087807606263982e-05,
      "loss": 0.3929,
      "step": 3420
    },
    {
      "epoch": 3.83668903803132,
      "grad_norm": 10.186746597290039,
      "learning_rate": 3.0822147651006714e-05,
      "loss": 0.5405,
      "step": 3430
    },
    {
      "epoch": 3.8478747203579418,
      "grad_norm": 1.004713773727417,
      "learning_rate": 3.076621923937361e-05,
      "loss": 0.2397,
      "step": 3440
    },
    {
      "epoch": 3.859060402684564,
      "grad_norm": 17.712461471557617,
      "learning_rate": 3.0710290827740494e-05,
      "loss": 0.3323,
      "step": 3450
    },
    {
      "epoch": 3.8702460850111855,
      "grad_norm": 6.2356486320495605,
      "learning_rate": 3.065436241610739e-05,
      "loss": 0.6592,
      "step": 3460
    },
    {
      "epoch": 3.8814317673378076,
      "grad_norm": 14.173298835754395,
      "learning_rate": 3.059843400447427e-05,
      "loss": 0.6431,
      "step": 3470
    },
    {
      "epoch": 3.8926174496644297,
      "grad_norm": 29.494937896728516,
      "learning_rate": 3.0542505592841167e-05,
      "loss": 0.2836,
      "step": 3480
    },
    {
      "epoch": 3.9038031319910513,
      "grad_norm": 39.20806884765625,
      "learning_rate": 3.0486577181208053e-05,
      "loss": 0.5441,
      "step": 3490
    },
    {
      "epoch": 3.9149888143176734,
      "grad_norm": 19.94827651977539,
      "learning_rate": 3.0430648769574943e-05,
      "loss": 0.6729,
      "step": 3500
    },
    {
      "epoch": 3.926174496644295,
      "grad_norm": 29.261871337890625,
      "learning_rate": 3.037472035794184e-05,
      "loss": 0.5797,
      "step": 3510
    },
    {
      "epoch": 3.937360178970917,
      "grad_norm": 10.84627914428711,
      "learning_rate": 3.0318791946308726e-05,
      "loss": 0.4771,
      "step": 3520
    },
    {
      "epoch": 3.9485458612975393,
      "grad_norm": 16.893878936767578,
      "learning_rate": 3.0262863534675616e-05,
      "loss": 0.6204,
      "step": 3530
    },
    {
      "epoch": 3.959731543624161,
      "grad_norm": 7.320198059082031,
      "learning_rate": 3.0206935123042506e-05,
      "loss": 0.1869,
      "step": 3540
    },
    {
      "epoch": 3.970917225950783,
      "grad_norm": 4.269453048706055,
      "learning_rate": 3.0151006711409396e-05,
      "loss": 0.4529,
      "step": 3550
    },
    {
      "epoch": 3.9821029082774047,
      "grad_norm": 31.61916732788086,
      "learning_rate": 3.0095078299776286e-05,
      "loss": 0.6774,
      "step": 3560
    },
    {
      "epoch": 3.9932885906040267,
      "grad_norm": 1.5221967697143555,
      "learning_rate": 3.0039149888143175e-05,
      "loss": 0.3266,
      "step": 3570
    },
    {
      "epoch": 4.0,
      "eval_f1": 0.7294166250643198,
      "eval_loss": 0.7091624140739441,
      "eval_precision": 0.7179973335826404,
      "eval_recall": 0.7435215946843854,
      "eval_runtime": 113.939,
      "eval_samples_per_second": 7.846,
      "eval_steps_per_second": 0.983,
      "step": 3576
    }
  ],
  "logging_steps": 10,
  "max_steps": 8940,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 79140636573696.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
