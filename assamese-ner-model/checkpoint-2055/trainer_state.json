{
  "best_global_step": 822,
  "best_metric": 0.14142312109470367,
  "best_model_checkpoint": "./assamese-ner-model\\checkpoint-822",
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 2055,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.024330900243309004,
      "grad_norm": 9.92025375366211,
      "learning_rate": 2.986861313868613e-05,
      "loss": 2.0306,
      "step": 10
    },
    {
      "epoch": 0.04866180048661801,
      "grad_norm": 5.725038528442383,
      "learning_rate": 2.972262773722628e-05,
      "loss": 1.4597,
      "step": 20
    },
    {
      "epoch": 0.072992700729927,
      "grad_norm": 3.1967737674713135,
      "learning_rate": 2.9576642335766424e-05,
      "loss": 0.9093,
      "step": 30
    },
    {
      "epoch": 0.09732360097323602,
      "grad_norm": 5.499932765960693,
      "learning_rate": 2.943065693430657e-05,
      "loss": 0.94,
      "step": 40
    },
    {
      "epoch": 0.12165450121654502,
      "grad_norm": 8.96096134185791,
      "learning_rate": 2.9284671532846718e-05,
      "loss": 0.8524,
      "step": 50
    },
    {
      "epoch": 0.145985401459854,
      "grad_norm": 4.771061420440674,
      "learning_rate": 2.9138686131386863e-05,
      "loss": 0.6931,
      "step": 60
    },
    {
      "epoch": 0.170316301703163,
      "grad_norm": 4.635921478271484,
      "learning_rate": 2.8992700729927008e-05,
      "loss": 1.0057,
      "step": 70
    },
    {
      "epoch": 0.19464720194647203,
      "grad_norm": 2.315779447555542,
      "learning_rate": 2.8846715328467153e-05,
      "loss": 0.746,
      "step": 80
    },
    {
      "epoch": 0.21897810218978103,
      "grad_norm": 5.268184185028076,
      "learning_rate": 2.87007299270073e-05,
      "loss": 0.7552,
      "step": 90
    },
    {
      "epoch": 0.24330900243309003,
      "grad_norm": 11.411809921264648,
      "learning_rate": 2.8554744525547447e-05,
      "loss": 0.871,
      "step": 100
    },
    {
      "epoch": 0.26763990267639903,
      "grad_norm": 1.683068871498108,
      "learning_rate": 2.8408759124087592e-05,
      "loss": 0.784,
      "step": 110
    },
    {
      "epoch": 0.291970802919708,
      "grad_norm": 3.8470547199249268,
      "learning_rate": 2.826277372262774e-05,
      "loss": 0.7669,
      "step": 120
    },
    {
      "epoch": 0.31630170316301703,
      "grad_norm": 4.337381362915039,
      "learning_rate": 2.8116788321167886e-05,
      "loss": 0.5524,
      "step": 130
    },
    {
      "epoch": 0.340632603406326,
      "grad_norm": 3.257624864578247,
      "learning_rate": 2.7970802919708027e-05,
      "loss": 0.5405,
      "step": 140
    },
    {
      "epoch": 0.36496350364963503,
      "grad_norm": 2.425150156021118,
      "learning_rate": 2.7824817518248176e-05,
      "loss": 0.4688,
      "step": 150
    },
    {
      "epoch": 0.38929440389294406,
      "grad_norm": 5.331384181976318,
      "learning_rate": 2.767883211678832e-05,
      "loss": 0.575,
      "step": 160
    },
    {
      "epoch": 0.41362530413625304,
      "grad_norm": 3.0689306259155273,
      "learning_rate": 2.7532846715328466e-05,
      "loss": 0.749,
      "step": 170
    },
    {
      "epoch": 0.43795620437956206,
      "grad_norm": 2.241914749145508,
      "learning_rate": 2.7386861313868615e-05,
      "loss": 0.6197,
      "step": 180
    },
    {
      "epoch": 0.46228710462287104,
      "grad_norm": 6.144436359405518,
      "learning_rate": 2.724087591240876e-05,
      "loss": 0.6017,
      "step": 190
    },
    {
      "epoch": 0.48661800486618007,
      "grad_norm": 3.459343433380127,
      "learning_rate": 2.7094890510948905e-05,
      "loss": 0.561,
      "step": 200
    },
    {
      "epoch": 0.5109489051094891,
      "grad_norm": 2.271285057067871,
      "learning_rate": 2.694890510948905e-05,
      "loss": 0.5629,
      "step": 210
    },
    {
      "epoch": 0.5352798053527981,
      "grad_norm": 4.38702917098999,
      "learning_rate": 2.68029197080292e-05,
      "loss": 0.601,
      "step": 220
    },
    {
      "epoch": 0.559610705596107,
      "grad_norm": 5.947737216949463,
      "learning_rate": 2.6656934306569344e-05,
      "loss": 0.6019,
      "step": 230
    },
    {
      "epoch": 0.583941605839416,
      "grad_norm": 4.9341654777526855,
      "learning_rate": 2.651094890510949e-05,
      "loss": 0.5257,
      "step": 240
    },
    {
      "epoch": 0.6082725060827251,
      "grad_norm": 3.674595832824707,
      "learning_rate": 2.6364963503649637e-05,
      "loss": 0.5465,
      "step": 250
    },
    {
      "epoch": 0.6326034063260341,
      "grad_norm": 14.41065788269043,
      "learning_rate": 2.6218978102189782e-05,
      "loss": 0.8378,
      "step": 260
    },
    {
      "epoch": 0.656934306569343,
      "grad_norm": 8.496246337890625,
      "learning_rate": 2.6072992700729928e-05,
      "loss": 0.5558,
      "step": 270
    },
    {
      "epoch": 0.681265206812652,
      "grad_norm": 1.7409957647323608,
      "learning_rate": 2.5927007299270076e-05,
      "loss": 0.4619,
      "step": 280
    },
    {
      "epoch": 0.7055961070559611,
      "grad_norm": 4.854860305786133,
      "learning_rate": 2.578102189781022e-05,
      "loss": 0.5325,
      "step": 290
    },
    {
      "epoch": 0.7299270072992701,
      "grad_norm": 4.894105911254883,
      "learning_rate": 2.5635036496350366e-05,
      "loss": 0.683,
      "step": 300
    },
    {
      "epoch": 0.754257907542579,
      "grad_norm": 4.039078235626221,
      "learning_rate": 2.548905109489051e-05,
      "loss": 0.5268,
      "step": 310
    },
    {
      "epoch": 0.7785888077858881,
      "grad_norm": 7.852715969085693,
      "learning_rate": 2.5343065693430657e-05,
      "loss": 0.7042,
      "step": 320
    },
    {
      "epoch": 0.8029197080291971,
      "grad_norm": 5.258782386779785,
      "learning_rate": 2.5197080291970802e-05,
      "loss": 0.4327,
      "step": 330
    },
    {
      "epoch": 0.8272506082725061,
      "grad_norm": 2.5321261882781982,
      "learning_rate": 2.5051094890510947e-05,
      "loss": 0.657,
      "step": 340
    },
    {
      "epoch": 0.851581508515815,
      "grad_norm": 5.037079811096191,
      "learning_rate": 2.4905109489051095e-05,
      "loss": 0.7894,
      "step": 350
    },
    {
      "epoch": 0.8759124087591241,
      "grad_norm": 3.961207866668701,
      "learning_rate": 2.475912408759124e-05,
      "loss": 0.6454,
      "step": 360
    },
    {
      "epoch": 0.9002433090024331,
      "grad_norm": 4.14401388168335,
      "learning_rate": 2.4613138686131386e-05,
      "loss": 0.4405,
      "step": 370
    },
    {
      "epoch": 0.9245742092457421,
      "grad_norm": 14.548818588256836,
      "learning_rate": 2.4467153284671534e-05,
      "loss": 0.3737,
      "step": 380
    },
    {
      "epoch": 0.948905109489051,
      "grad_norm": 4.137337684631348,
      "learning_rate": 2.432116788321168e-05,
      "loss": 0.398,
      "step": 390
    },
    {
      "epoch": 0.9732360097323601,
      "grad_norm": 5.8139777183532715,
      "learning_rate": 2.4175182481751824e-05,
      "loss": 0.4764,
      "step": 400
    },
    {
      "epoch": 0.9975669099756691,
      "grad_norm": 6.301417350769043,
      "learning_rate": 2.4029197080291973e-05,
      "loss": 0.5278,
      "step": 410
    },
    {
      "epoch": 1.0,
      "eval_f1": 0.72491263105342,
      "eval_loss": 0.15651477873325348,
      "eval_precision": 0.7781350482315113,
      "eval_recall": 0.6785046728971963,
      "eval_runtime": 115.413,
      "eval_samples_per_second": 7.33,
      "eval_steps_per_second": 0.918,
      "step": 411
    },
    {
      "epoch": 1.0218978102189782,
      "grad_norm": 2.745389461517334,
      "learning_rate": 2.3883211678832118e-05,
      "loss": 0.5366,
      "step": 420
    },
    {
      "epoch": 1.0462287104622872,
      "grad_norm": 9.634721755981445,
      "learning_rate": 2.3737226277372263e-05,
      "loss": 0.4925,
      "step": 430
    },
    {
      "epoch": 1.0705596107055961,
      "grad_norm": 8.818930625915527,
      "learning_rate": 2.359124087591241e-05,
      "loss": 0.3949,
      "step": 440
    },
    {
      "epoch": 1.094890510948905,
      "grad_norm": 5.662981033325195,
      "learning_rate": 2.3445255474452557e-05,
      "loss": 0.4452,
      "step": 450
    },
    {
      "epoch": 1.119221411192214,
      "grad_norm": 2.8783557415008545,
      "learning_rate": 2.3299270072992702e-05,
      "loss": 0.4599,
      "step": 460
    },
    {
      "epoch": 1.143552311435523,
      "grad_norm": 4.750424385070801,
      "learning_rate": 2.3153284671532847e-05,
      "loss": 0.4715,
      "step": 470
    },
    {
      "epoch": 1.167883211678832,
      "grad_norm": 2.511590003967285,
      "learning_rate": 2.3007299270072996e-05,
      "loss": 0.3003,
      "step": 480
    },
    {
      "epoch": 1.1922141119221412,
      "grad_norm": 2.498732805252075,
      "learning_rate": 2.286131386861314e-05,
      "loss": 0.3003,
      "step": 490
    },
    {
      "epoch": 1.2165450121654502,
      "grad_norm": 5.417802810668945,
      "learning_rate": 2.2715328467153286e-05,
      "loss": 0.5787,
      "step": 500
    },
    {
      "epoch": 1.2408759124087592,
      "grad_norm": 6.466491222381592,
      "learning_rate": 2.256934306569343e-05,
      "loss": 0.5091,
      "step": 510
    },
    {
      "epoch": 1.2652068126520681,
      "grad_norm": 2.7897956371307373,
      "learning_rate": 2.2423357664233576e-05,
      "loss": 0.549,
      "step": 520
    },
    {
      "epoch": 1.289537712895377,
      "grad_norm": 4.645617961883545,
      "learning_rate": 2.227737226277372e-05,
      "loss": 0.4827,
      "step": 530
    },
    {
      "epoch": 1.313868613138686,
      "grad_norm": 6.071934700012207,
      "learning_rate": 2.2131386861313866e-05,
      "loss": 0.406,
      "step": 540
    },
    {
      "epoch": 1.338199513381995,
      "grad_norm": 4.536595821380615,
      "learning_rate": 2.1985401459854015e-05,
      "loss": 0.424,
      "step": 550
    },
    {
      "epoch": 1.3625304136253042,
      "grad_norm": 4.890784740447998,
      "learning_rate": 2.183941605839416e-05,
      "loss": 0.5114,
      "step": 560
    },
    {
      "epoch": 1.3868613138686132,
      "grad_norm": 3.40224289894104,
      "learning_rate": 2.1693430656934305e-05,
      "loss": 0.351,
      "step": 570
    },
    {
      "epoch": 1.4111922141119222,
      "grad_norm": 3.307668685913086,
      "learning_rate": 2.1547445255474454e-05,
      "loss": 0.4058,
      "step": 580
    },
    {
      "epoch": 1.4355231143552312,
      "grad_norm": 4.317135334014893,
      "learning_rate": 2.14014598540146e-05,
      "loss": 0.3988,
      "step": 590
    },
    {
      "epoch": 1.4598540145985401,
      "grad_norm": 3.7555887699127197,
      "learning_rate": 2.1255474452554744e-05,
      "loss": 0.3375,
      "step": 600
    },
    {
      "epoch": 1.4841849148418491,
      "grad_norm": 3.6148464679718018,
      "learning_rate": 2.1109489051094893e-05,
      "loss": 0.4131,
      "step": 610
    },
    {
      "epoch": 1.508515815085158,
      "grad_norm": 3.887115240097046,
      "learning_rate": 2.0963503649635038e-05,
      "loss": 0.3398,
      "step": 620
    },
    {
      "epoch": 1.5328467153284673,
      "grad_norm": 6.109027862548828,
      "learning_rate": 2.0817518248175183e-05,
      "loss": 0.4505,
      "step": 630
    },
    {
      "epoch": 1.557177615571776,
      "grad_norm": 10.002976417541504,
      "learning_rate": 2.067153284671533e-05,
      "loss": 0.6535,
      "step": 640
    },
    {
      "epoch": 1.5815085158150852,
      "grad_norm": 3.562697649002075,
      "learning_rate": 2.0525547445255476e-05,
      "loss": 0.3165,
      "step": 650
    },
    {
      "epoch": 1.6058394160583942,
      "grad_norm": 0.780308187007904,
      "learning_rate": 2.037956204379562e-05,
      "loss": 0.315,
      "step": 660
    },
    {
      "epoch": 1.6301703163017032,
      "grad_norm": 7.892181396484375,
      "learning_rate": 2.0233576642335767e-05,
      "loss": 0.5272,
      "step": 670
    },
    {
      "epoch": 1.6545012165450121,
      "grad_norm": 2.8028883934020996,
      "learning_rate": 2.0087591240875915e-05,
      "loss": 0.2746,
      "step": 680
    },
    {
      "epoch": 1.6788321167883211,
      "grad_norm": 1.8399162292480469,
      "learning_rate": 1.994160583941606e-05,
      "loss": 0.285,
      "step": 690
    },
    {
      "epoch": 1.7031630170316303,
      "grad_norm": 1.4701802730560303,
      "learning_rate": 1.9795620437956202e-05,
      "loss": 0.234,
      "step": 700
    },
    {
      "epoch": 1.727493917274939,
      "grad_norm": 2.967451810836792,
      "learning_rate": 1.964963503649635e-05,
      "loss": 0.4251,
      "step": 710
    },
    {
      "epoch": 1.7518248175182483,
      "grad_norm": 3.065410614013672,
      "learning_rate": 1.9503649635036496e-05,
      "loss": 0.4014,
      "step": 720
    },
    {
      "epoch": 1.7761557177615572,
      "grad_norm": 16.88504981994629,
      "learning_rate": 1.935766423357664e-05,
      "loss": 0.3392,
      "step": 730
    },
    {
      "epoch": 1.8004866180048662,
      "grad_norm": 4.112252712249756,
      "learning_rate": 1.921167883211679e-05,
      "loss": 0.4845,
      "step": 740
    },
    {
      "epoch": 1.8248175182481752,
      "grad_norm": 4.819926738739014,
      "learning_rate": 1.9065693430656935e-05,
      "loss": 0.4877,
      "step": 750
    },
    {
      "epoch": 1.8491484184914841,
      "grad_norm": 25.192840576171875,
      "learning_rate": 1.891970802919708e-05,
      "loss": 0.3692,
      "step": 760
    },
    {
      "epoch": 1.8734793187347933,
      "grad_norm": 7.519534111022949,
      "learning_rate": 1.8773722627737225e-05,
      "loss": 0.5346,
      "step": 770
    },
    {
      "epoch": 1.897810218978102,
      "grad_norm": 5.713701248168945,
      "learning_rate": 1.8627737226277373e-05,
      "loss": 0.4018,
      "step": 780
    },
    {
      "epoch": 1.9221411192214113,
      "grad_norm": 12.125812530517578,
      "learning_rate": 1.848175182481752e-05,
      "loss": 0.3325,
      "step": 790
    },
    {
      "epoch": 1.94647201946472,
      "grad_norm": 5.3678436279296875,
      "learning_rate": 1.8335766423357664e-05,
      "loss": 0.5692,
      "step": 800
    },
    {
      "epoch": 1.9708029197080292,
      "grad_norm": 4.191765308380127,
      "learning_rate": 1.8189781021897812e-05,
      "loss": 0.3801,
      "step": 810
    },
    {
      "epoch": 1.9951338199513382,
      "grad_norm": 3.5782177448272705,
      "learning_rate": 1.8043795620437957e-05,
      "loss": 0.3326,
      "step": 820
    },
    {
      "epoch": 2.0,
      "eval_f1": 0.6849162011173185,
      "eval_loss": 0.14142312109470367,
      "eval_precision": 0.8513888888888889,
      "eval_recall": 0.5728971962616822,
      "eval_runtime": 115.64,
      "eval_samples_per_second": 7.316,
      "eval_steps_per_second": 0.917,
      "step": 822
    },
    {
      "epoch": 2.019464720194647,
      "grad_norm": 10.03662395477295,
      "learning_rate": 1.7897810218978102e-05,
      "loss": 0.3986,
      "step": 830
    },
    {
      "epoch": 2.0437956204379564,
      "grad_norm": 3.7400360107421875,
      "learning_rate": 1.775182481751825e-05,
      "loss": 0.3038,
      "step": 840
    },
    {
      "epoch": 2.068126520681265,
      "grad_norm": 8.444446563720703,
      "learning_rate": 1.7605839416058396e-05,
      "loss": 0.2755,
      "step": 850
    },
    {
      "epoch": 2.0924574209245743,
      "grad_norm": 6.496382236480713,
      "learning_rate": 1.745985401459854e-05,
      "loss": 0.3093,
      "step": 860
    },
    {
      "epoch": 2.116788321167883,
      "grad_norm": 3.9183199405670166,
      "learning_rate": 1.731386861313869e-05,
      "loss": 0.3367,
      "step": 870
    },
    {
      "epoch": 2.1411192214111923,
      "grad_norm": 7.7478251457214355,
      "learning_rate": 1.716788321167883e-05,
      "loss": 0.3807,
      "step": 880
    },
    {
      "epoch": 2.165450121654501,
      "grad_norm": 6.031020164489746,
      "learning_rate": 1.7021897810218977e-05,
      "loss": 0.2381,
      "step": 890
    },
    {
      "epoch": 2.18978102189781,
      "grad_norm": 16.017614364624023,
      "learning_rate": 1.6875912408759122e-05,
      "loss": 0.515,
      "step": 900
    },
    {
      "epoch": 2.2141119221411194,
      "grad_norm": 1.3664311170578003,
      "learning_rate": 1.672992700729927e-05,
      "loss": 0.2348,
      "step": 910
    },
    {
      "epoch": 2.238442822384428,
      "grad_norm": 8.965859413146973,
      "learning_rate": 1.6583941605839415e-05,
      "loss": 0.3755,
      "step": 920
    },
    {
      "epoch": 2.2627737226277373,
      "grad_norm": 5.90708589553833,
      "learning_rate": 1.643795620437956e-05,
      "loss": 0.2352,
      "step": 930
    },
    {
      "epoch": 2.287104622871046,
      "grad_norm": 3.56278920173645,
      "learning_rate": 1.629197080291971e-05,
      "loss": 0.3215,
      "step": 940
    },
    {
      "epoch": 2.3114355231143553,
      "grad_norm": 6.830710411071777,
      "learning_rate": 1.6145985401459854e-05,
      "loss": 0.4946,
      "step": 950
    },
    {
      "epoch": 2.335766423357664,
      "grad_norm": 4.835943698883057,
      "learning_rate": 1.6e-05,
      "loss": 0.3532,
      "step": 960
    },
    {
      "epoch": 2.3600973236009732,
      "grad_norm": 28.302743911743164,
      "learning_rate": 1.5854014598540148e-05,
      "loss": 0.367,
      "step": 970
    },
    {
      "epoch": 2.3844282238442824,
      "grad_norm": 3.9924702644348145,
      "learning_rate": 1.5708029197080293e-05,
      "loss": 0.4088,
      "step": 980
    },
    {
      "epoch": 2.408759124087591,
      "grad_norm": 8.511579513549805,
      "learning_rate": 1.5562043795620438e-05,
      "loss": 0.3963,
      "step": 990
    },
    {
      "epoch": 2.4330900243309004,
      "grad_norm": 2.161227226257324,
      "learning_rate": 1.5416058394160583e-05,
      "loss": 0.2533,
      "step": 1000
    },
    {
      "epoch": 2.457420924574209,
      "grad_norm": 5.1398210525512695,
      "learning_rate": 1.5270072992700732e-05,
      "loss": 0.3621,
      "step": 1010
    },
    {
      "epoch": 2.4817518248175183,
      "grad_norm": 8.115911483764648,
      "learning_rate": 1.5124087591240877e-05,
      "loss": 0.2607,
      "step": 1020
    },
    {
      "epoch": 2.5060827250608275,
      "grad_norm": 5.644152641296387,
      "learning_rate": 1.4978102189781022e-05,
      "loss": 0.1788,
      "step": 1030
    },
    {
      "epoch": 2.5304136253041363,
      "grad_norm": 8.062714576721191,
      "learning_rate": 1.4832116788321167e-05,
      "loss": 0.3381,
      "step": 1040
    },
    {
      "epoch": 2.554744525547445,
      "grad_norm": 19.882631301879883,
      "learning_rate": 1.4686131386861314e-05,
      "loss": 0.3151,
      "step": 1050
    },
    {
      "epoch": 2.579075425790754,
      "grad_norm": 5.699784755706787,
      "learning_rate": 1.454014598540146e-05,
      "loss": 0.3127,
      "step": 1060
    },
    {
      "epoch": 2.6034063260340634,
      "grad_norm": 7.419994831085205,
      "learning_rate": 1.4394160583941606e-05,
      "loss": 0.26,
      "step": 1070
    },
    {
      "epoch": 2.627737226277372,
      "grad_norm": 6.655620574951172,
      "learning_rate": 1.4248175182481753e-05,
      "loss": 0.3914,
      "step": 1080
    },
    {
      "epoch": 2.6520681265206814,
      "grad_norm": 1.6455236673355103,
      "learning_rate": 1.4102189781021898e-05,
      "loss": 0.2415,
      "step": 1090
    },
    {
      "epoch": 2.67639902676399,
      "grad_norm": 2.229762077331543,
      "learning_rate": 1.3956204379562045e-05,
      "loss": 0.3286,
      "step": 1100
    },
    {
      "epoch": 2.7007299270072993,
      "grad_norm": 8.99122428894043,
      "learning_rate": 1.3810218978102192e-05,
      "loss": 0.6014,
      "step": 1110
    },
    {
      "epoch": 2.7250608272506085,
      "grad_norm": 2.6908364295959473,
      "learning_rate": 1.3664233576642335e-05,
      "loss": 0.408,
      "step": 1120
    },
    {
      "epoch": 2.7493917274939172,
      "grad_norm": 1.726692795753479,
      "learning_rate": 1.3518248175182482e-05,
      "loss": 0.2653,
      "step": 1130
    },
    {
      "epoch": 2.7737226277372264,
      "grad_norm": 1.9450236558914185,
      "learning_rate": 1.3372262773722627e-05,
      "loss": 0.4338,
      "step": 1140
    },
    {
      "epoch": 2.798053527980535,
      "grad_norm": 2.604060649871826,
      "learning_rate": 1.3226277372262774e-05,
      "loss": 0.2292,
      "step": 1150
    },
    {
      "epoch": 2.8223844282238444,
      "grad_norm": 2.905799150466919,
      "learning_rate": 1.308029197080292e-05,
      "loss": 0.2938,
      "step": 1160
    },
    {
      "epoch": 2.846715328467153,
      "grad_norm": 3.6705586910247803,
      "learning_rate": 1.2934306569343066e-05,
      "loss": 0.4668,
      "step": 1170
    },
    {
      "epoch": 2.8710462287104623,
      "grad_norm": 2.8418140411376953,
      "learning_rate": 1.2788321167883213e-05,
      "loss": 0.3136,
      "step": 1180
    },
    {
      "epoch": 2.895377128953771,
      "grad_norm": 4.437412738800049,
      "learning_rate": 1.264233576642336e-05,
      "loss": 0.1953,
      "step": 1190
    },
    {
      "epoch": 2.9197080291970803,
      "grad_norm": 5.639729022979736,
      "learning_rate": 1.2496350364963504e-05,
      "loss": 0.3756,
      "step": 1200
    },
    {
      "epoch": 2.9440389294403895,
      "grad_norm": 7.636282444000244,
      "learning_rate": 1.235036496350365e-05,
      "loss": 0.3306,
      "step": 1210
    },
    {
      "epoch": 2.9683698296836982,
      "grad_norm": 12.881672859191895,
      "learning_rate": 1.2204379562043795e-05,
      "loss": 0.4252,
      "step": 1220
    },
    {
      "epoch": 2.9927007299270074,
      "grad_norm": 2.1252357959747314,
      "learning_rate": 1.2058394160583942e-05,
      "loss": 0.2617,
      "step": 1230
    },
    {
      "epoch": 3.0,
      "eval_f1": 0.5640695428203477,
      "eval_loss": 0.21638774871826172,
      "eval_precision": 0.906832298136646,
      "eval_recall": 0.4093457943925234,
      "eval_runtime": 115.4449,
      "eval_samples_per_second": 7.328,
      "eval_steps_per_second": 0.918,
      "step": 1233
    },
    {
      "epoch": 3.017031630170316,
      "grad_norm": 3.214691162109375,
      "learning_rate": 1.1912408759124088e-05,
      "loss": 0.2447,
      "step": 1240
    },
    {
      "epoch": 3.0413625304136254,
      "grad_norm": 6.055796146392822,
      "learning_rate": 1.1766423357664234e-05,
      "loss": 0.196,
      "step": 1250
    },
    {
      "epoch": 3.065693430656934,
      "grad_norm": 4.895889759063721,
      "learning_rate": 1.162043795620438e-05,
      "loss": 0.2613,
      "step": 1260
    },
    {
      "epoch": 3.0900243309002433,
      "grad_norm": 5.470518589019775,
      "learning_rate": 1.1474452554744525e-05,
      "loss": 0.2616,
      "step": 1270
    },
    {
      "epoch": 3.1143552311435525,
      "grad_norm": 4.78473424911499,
      "learning_rate": 1.1328467153284672e-05,
      "loss": 0.2608,
      "step": 1280
    },
    {
      "epoch": 3.1386861313868613,
      "grad_norm": 7.97360372543335,
      "learning_rate": 1.1182481751824819e-05,
      "loss": 0.301,
      "step": 1290
    },
    {
      "epoch": 3.1630170316301705,
      "grad_norm": 7.842792987823486,
      "learning_rate": 1.1036496350364963e-05,
      "loss": 0.2915,
      "step": 1300
    },
    {
      "epoch": 3.187347931873479,
      "grad_norm": 2.1441214084625244,
      "learning_rate": 1.089051094890511e-05,
      "loss": 0.2398,
      "step": 1310
    },
    {
      "epoch": 3.2116788321167884,
      "grad_norm": 12.277222633361816,
      "learning_rate": 1.0744525547445255e-05,
      "loss": 0.2297,
      "step": 1320
    },
    {
      "epoch": 3.236009732360097,
      "grad_norm": 3.3022584915161133,
      "learning_rate": 1.0598540145985401e-05,
      "loss": 0.2285,
      "step": 1330
    },
    {
      "epoch": 3.2603406326034063,
      "grad_norm": 6.960573196411133,
      "learning_rate": 1.0452554744525548e-05,
      "loss": 0.5437,
      "step": 1340
    },
    {
      "epoch": 3.2846715328467155,
      "grad_norm": 9.383133888244629,
      "learning_rate": 1.0306569343065693e-05,
      "loss": 0.2383,
      "step": 1350
    },
    {
      "epoch": 3.3090024330900243,
      "grad_norm": 4.6469268798828125,
      "learning_rate": 1.016058394160584e-05,
      "loss": 0.27,
      "step": 1360
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 5.212521553039551,
      "learning_rate": 1.0014598540145985e-05,
      "loss": 0.2843,
      "step": 1370
    },
    {
      "epoch": 3.3576642335766422,
      "grad_norm": 30.86004638671875,
      "learning_rate": 9.868613138686132e-06,
      "loss": 0.3106,
      "step": 1380
    },
    {
      "epoch": 3.3819951338199514,
      "grad_norm": 12.679461479187012,
      "learning_rate": 9.722627737226279e-06,
      "loss": 0.257,
      "step": 1390
    },
    {
      "epoch": 3.40632603406326,
      "grad_norm": 7.737640380859375,
      "learning_rate": 9.576642335766422e-06,
      "loss": 0.3577,
      "step": 1400
    },
    {
      "epoch": 3.4306569343065694,
      "grad_norm": 7.2018303871154785,
      "learning_rate": 9.43065693430657e-06,
      "loss": 0.262,
      "step": 1410
    },
    {
      "epoch": 3.454987834549878,
      "grad_norm": 24.311166763305664,
      "learning_rate": 9.284671532846716e-06,
      "loss": 0.3214,
      "step": 1420
    },
    {
      "epoch": 3.4793187347931873,
      "grad_norm": 10.015305519104004,
      "learning_rate": 9.138686131386861e-06,
      "loss": 0.2812,
      "step": 1430
    },
    {
      "epoch": 3.5036496350364965,
      "grad_norm": 7.341407775878906,
      "learning_rate": 8.992700729927008e-06,
      "loss": 0.3264,
      "step": 1440
    },
    {
      "epoch": 3.5279805352798053,
      "grad_norm": 3.4256222248077393,
      "learning_rate": 8.846715328467153e-06,
      "loss": 0.4077,
      "step": 1450
    },
    {
      "epoch": 3.5523114355231145,
      "grad_norm": 4.506968975067139,
      "learning_rate": 8.7007299270073e-06,
      "loss": 0.1985,
      "step": 1460
    },
    {
      "epoch": 3.576642335766423,
      "grad_norm": 9.778027534484863,
      "learning_rate": 8.554744525547447e-06,
      "loss": 0.3557,
      "step": 1470
    },
    {
      "epoch": 3.6009732360097324,
      "grad_norm": 7.314628601074219,
      "learning_rate": 8.408759124087592e-06,
      "loss": 0.2684,
      "step": 1480
    },
    {
      "epoch": 3.6253041362530416,
      "grad_norm": 3.257052421569824,
      "learning_rate": 8.262773722627737e-06,
      "loss": 0.3151,
      "step": 1490
    },
    {
      "epoch": 3.6496350364963503,
      "grad_norm": 3.5360991954803467,
      "learning_rate": 8.116788321167882e-06,
      "loss": 0.2187,
      "step": 1500
    },
    {
      "epoch": 3.673965936739659,
      "grad_norm": 2.320298433303833,
      "learning_rate": 7.970802919708029e-06,
      "loss": 0.2389,
      "step": 1510
    },
    {
      "epoch": 3.6982968369829683,
      "grad_norm": 3.3703184127807617,
      "learning_rate": 7.824817518248176e-06,
      "loss": 0.2341,
      "step": 1520
    },
    {
      "epoch": 3.7226277372262775,
      "grad_norm": 3.081862211227417,
      "learning_rate": 7.678832116788321e-06,
      "loss": 0.2284,
      "step": 1530
    },
    {
      "epoch": 3.7469586374695862,
      "grad_norm": 1.1403555870056152,
      "learning_rate": 7.532846715328468e-06,
      "loss": 0.2823,
      "step": 1540
    },
    {
      "epoch": 3.7712895377128954,
      "grad_norm": 2.535418748855591,
      "learning_rate": 7.386861313868614e-06,
      "loss": 0.2603,
      "step": 1550
    },
    {
      "epoch": 3.795620437956204,
      "grad_norm": 2.9811336994171143,
      "learning_rate": 7.240875912408759e-06,
      "loss": 0.1854,
      "step": 1560
    },
    {
      "epoch": 3.8199513381995134,
      "grad_norm": 4.691127777099609,
      "learning_rate": 7.094890510948905e-06,
      "loss": 0.2824,
      "step": 1570
    },
    {
      "epoch": 3.8442822384428226,
      "grad_norm": 4.783599853515625,
      "learning_rate": 6.948905109489051e-06,
      "loss": 0.1573,
      "step": 1580
    },
    {
      "epoch": 3.8686131386861313,
      "grad_norm": 6.699766159057617,
      "learning_rate": 6.802919708029198e-06,
      "loss": 0.212,
      "step": 1590
    },
    {
      "epoch": 3.8929440389294405,
      "grad_norm": 2.968716859817505,
      "learning_rate": 6.656934306569344e-06,
      "loss": 0.248,
      "step": 1600
    },
    {
      "epoch": 3.9172749391727493,
      "grad_norm": 11.269281387329102,
      "learning_rate": 6.510948905109489e-06,
      "loss": 0.3257,
      "step": 1610
    },
    {
      "epoch": 3.9416058394160585,
      "grad_norm": 5.060018539428711,
      "learning_rate": 6.364963503649635e-06,
      "loss": 0.2711,
      "step": 1620
    },
    {
      "epoch": 3.9659367396593677,
      "grad_norm": 5.082150936126709,
      "learning_rate": 6.218978102189782e-06,
      "loss": 0.1767,
      "step": 1630
    },
    {
      "epoch": 3.9902676399026764,
      "grad_norm": 4.037356376647949,
      "learning_rate": 6.072992700729928e-06,
      "loss": 0.2839,
      "step": 1640
    },
    {
      "epoch": 4.0,
      "eval_f1": 0.7293112653497063,
      "eval_loss": 0.15584616363048553,
      "eval_precision": 0.8505603985056039,
      "eval_recall": 0.6383177570093458,
      "eval_runtime": 115.173,
      "eval_samples_per_second": 7.345,
      "eval_steps_per_second": 0.92,
      "step": 1644
    },
    {
      "epoch": 4.014598540145985,
      "grad_norm": 6.296732425689697,
      "learning_rate": 5.927007299270073e-06,
      "loss": 0.1711,
      "step": 1650
    },
    {
      "epoch": 4.038929440389294,
      "grad_norm": 1.0838786363601685,
      "learning_rate": 5.781021897810219e-06,
      "loss": 0.2085,
      "step": 1660
    },
    {
      "epoch": 4.0632603406326036,
      "grad_norm": 4.478060245513916,
      "learning_rate": 5.635036496350365e-06,
      "loss": 0.2594,
      "step": 1670
    },
    {
      "epoch": 4.087591240875913,
      "grad_norm": 7.01576566696167,
      "learning_rate": 5.4890510948905115e-06,
      "loss": 0.1778,
      "step": 1680
    },
    {
      "epoch": 4.111922141119221,
      "grad_norm": 4.658207893371582,
      "learning_rate": 5.3430656934306575e-06,
      "loss": 0.2457,
      "step": 1690
    },
    {
      "epoch": 4.13625304136253,
      "grad_norm": 23.490081787109375,
      "learning_rate": 5.197080291970803e-06,
      "loss": 0.2398,
      "step": 1700
    },
    {
      "epoch": 4.160583941605839,
      "grad_norm": 31.518939971923828,
      "learning_rate": 5.051094890510949e-06,
      "loss": 0.3437,
      "step": 1710
    },
    {
      "epoch": 4.184914841849149,
      "grad_norm": 2.7684390544891357,
      "learning_rate": 4.905109489051095e-06,
      "loss": 0.2511,
      "step": 1720
    },
    {
      "epoch": 4.209245742092458,
      "grad_norm": 2.108333110809326,
      "learning_rate": 4.759124087591241e-06,
      "loss": 0.2036,
      "step": 1730
    },
    {
      "epoch": 4.233576642335766,
      "grad_norm": 2.031330108642578,
      "learning_rate": 4.613138686131387e-06,
      "loss": 0.2062,
      "step": 1740
    },
    {
      "epoch": 4.257907542579075,
      "grad_norm": 7.303556442260742,
      "learning_rate": 4.4671532846715325e-06,
      "loss": 0.1881,
      "step": 1750
    },
    {
      "epoch": 4.2822384428223845,
      "grad_norm": 2.293614149093628,
      "learning_rate": 4.3211678832116785e-06,
      "loss": 0.1491,
      "step": 1760
    },
    {
      "epoch": 4.306569343065694,
      "grad_norm": 1.8717879056930542,
      "learning_rate": 4.175182481751825e-06,
      "loss": 0.1841,
      "step": 1770
    },
    {
      "epoch": 4.330900243309002,
      "grad_norm": 3.317572593688965,
      "learning_rate": 4.029197080291971e-06,
      "loss": 0.184,
      "step": 1780
    },
    {
      "epoch": 4.355231143552311,
      "grad_norm": 5.822105884552002,
      "learning_rate": 3.8832116788321164e-06,
      "loss": 0.1298,
      "step": 1790
    },
    {
      "epoch": 4.37956204379562,
      "grad_norm": 72.75727081298828,
      "learning_rate": 3.7372262773722633e-06,
      "loss": 0.3514,
      "step": 1800
    },
    {
      "epoch": 4.40389294403893,
      "grad_norm": 7.036479949951172,
      "learning_rate": 3.591240875912409e-06,
      "loss": 0.155,
      "step": 1810
    },
    {
      "epoch": 4.428223844282239,
      "grad_norm": 0.7525767087936401,
      "learning_rate": 3.445255474452555e-06,
      "loss": 0.1771,
      "step": 1820
    },
    {
      "epoch": 4.452554744525547,
      "grad_norm": 6.066376209259033,
      "learning_rate": 3.2992700729927008e-06,
      "loss": 0.1897,
      "step": 1830
    },
    {
      "epoch": 4.476885644768856,
      "grad_norm": 1.2388240098953247,
      "learning_rate": 3.1532846715328468e-06,
      "loss": 0.2968,
      "step": 1840
    },
    {
      "epoch": 4.5012165450121655,
      "grad_norm": 3.9901437759399414,
      "learning_rate": 3.0072992700729927e-06,
      "loss": 0.171,
      "step": 1850
    },
    {
      "epoch": 4.525547445255475,
      "grad_norm": 10.350813865661621,
      "learning_rate": 2.8613138686131387e-06,
      "loss": 0.2264,
      "step": 1860
    },
    {
      "epoch": 4.549878345498783,
      "grad_norm": 2.0779263973236084,
      "learning_rate": 2.715328467153285e-06,
      "loss": 0.1546,
      "step": 1870
    },
    {
      "epoch": 4.574209245742092,
      "grad_norm": 6.043684005737305,
      "learning_rate": 2.5693430656934307e-06,
      "loss": 0.0802,
      "step": 1880
    },
    {
      "epoch": 4.598540145985401,
      "grad_norm": 2.9955456256866455,
      "learning_rate": 2.4233576642335767e-06,
      "loss": 0.2629,
      "step": 1890
    },
    {
      "epoch": 4.622871046228711,
      "grad_norm": 6.990450382232666,
      "learning_rate": 2.2773722627737226e-06,
      "loss": 0.2669,
      "step": 1900
    },
    {
      "epoch": 4.64720194647202,
      "grad_norm": 1.4621081352233887,
      "learning_rate": 2.1313868613138686e-06,
      "loss": 0.2322,
      "step": 1910
    },
    {
      "epoch": 4.671532846715328,
      "grad_norm": 4.424654483795166,
      "learning_rate": 1.9854014598540146e-06,
      "loss": 0.2763,
      "step": 1920
    },
    {
      "epoch": 4.695863746958637,
      "grad_norm": 4.449821472167969,
      "learning_rate": 1.8394160583941606e-06,
      "loss": 0.1495,
      "step": 1930
    },
    {
      "epoch": 4.7201946472019465,
      "grad_norm": 12.816668510437012,
      "learning_rate": 1.6934306569343066e-06,
      "loss": 0.22,
      "step": 1940
    },
    {
      "epoch": 4.744525547445256,
      "grad_norm": 7.662026405334473,
      "learning_rate": 1.5474452554744525e-06,
      "loss": 0.3127,
      "step": 1950
    },
    {
      "epoch": 4.768856447688565,
      "grad_norm": 1.1073747873306274,
      "learning_rate": 1.4014598540145985e-06,
      "loss": 0.2643,
      "step": 1960
    },
    {
      "epoch": 4.793187347931873,
      "grad_norm": 2.337474822998047,
      "learning_rate": 1.2554744525547447e-06,
      "loss": 0.3261,
      "step": 1970
    },
    {
      "epoch": 4.817518248175182,
      "grad_norm": 10.24040699005127,
      "learning_rate": 1.1094890510948907e-06,
      "loss": 0.2205,
      "step": 1980
    },
    {
      "epoch": 4.841849148418492,
      "grad_norm": 2.2599055767059326,
      "learning_rate": 9.635036496350364e-07,
      "loss": 0.2056,
      "step": 1990
    },
    {
      "epoch": 4.866180048661801,
      "grad_norm": 15.144725799560547,
      "learning_rate": 8.175182481751825e-07,
      "loss": 0.1494,
      "step": 2000
    },
    {
      "epoch": 4.89051094890511,
      "grad_norm": 7.818324565887451,
      "learning_rate": 6.715328467153284e-07,
      "loss": 0.1693,
      "step": 2010
    },
    {
      "epoch": 4.914841849148418,
      "grad_norm": 13.587393760681152,
      "learning_rate": 5.255474452554745e-07,
      "loss": 0.426,
      "step": 2020
    },
    {
      "epoch": 4.9391727493917275,
      "grad_norm": 16.50086212158203,
      "learning_rate": 3.795620437956205e-07,
      "loss": 0.3034,
      "step": 2030
    },
    {
      "epoch": 4.963503649635037,
      "grad_norm": 5.628117084503174,
      "learning_rate": 2.3357664233576645e-07,
      "loss": 0.1338,
      "step": 2040
    },
    {
      "epoch": 4.987834549878346,
      "grad_norm": 3.038137674331665,
      "learning_rate": 8.759124087591241e-08,
      "loss": 0.2289,
      "step": 2050
    },
    {
      "epoch": 5.0,
      "eval_f1": 0.7052341597796142,
      "eval_loss": 0.18169592320919037,
      "eval_precision": 0.8590604026845637,
      "eval_recall": 0.5981308411214953,
      "eval_runtime": 114.806,
      "eval_samples_per_second": 7.369,
      "eval_steps_per_second": 0.923,
      "step": 2055
    }
  ],
  "logging_steps": 10,
  "max_steps": 2055,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 90735904861440.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
