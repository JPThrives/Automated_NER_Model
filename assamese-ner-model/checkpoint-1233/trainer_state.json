{
  "best_global_step": 822,
  "best_metric": 0.14142312109470367,
  "best_model_checkpoint": "./assamese-ner-model\\checkpoint-822",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 1233,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.024330900243309004,
      "grad_norm": 9.92025375366211,
      "learning_rate": 2.986861313868613e-05,
      "loss": 2.0306,
      "step": 10
    },
    {
      "epoch": 0.04866180048661801,
      "grad_norm": 5.725038528442383,
      "learning_rate": 2.972262773722628e-05,
      "loss": 1.4597,
      "step": 20
    },
    {
      "epoch": 0.072992700729927,
      "grad_norm": 3.1967737674713135,
      "learning_rate": 2.9576642335766424e-05,
      "loss": 0.9093,
      "step": 30
    },
    {
      "epoch": 0.09732360097323602,
      "grad_norm": 5.499932765960693,
      "learning_rate": 2.943065693430657e-05,
      "loss": 0.94,
      "step": 40
    },
    {
      "epoch": 0.12165450121654502,
      "grad_norm": 8.96096134185791,
      "learning_rate": 2.9284671532846718e-05,
      "loss": 0.8524,
      "step": 50
    },
    {
      "epoch": 0.145985401459854,
      "grad_norm": 4.771061420440674,
      "learning_rate": 2.9138686131386863e-05,
      "loss": 0.6931,
      "step": 60
    },
    {
      "epoch": 0.170316301703163,
      "grad_norm": 4.635921478271484,
      "learning_rate": 2.8992700729927008e-05,
      "loss": 1.0057,
      "step": 70
    },
    {
      "epoch": 0.19464720194647203,
      "grad_norm": 2.315779447555542,
      "learning_rate": 2.8846715328467153e-05,
      "loss": 0.746,
      "step": 80
    },
    {
      "epoch": 0.21897810218978103,
      "grad_norm": 5.268184185028076,
      "learning_rate": 2.87007299270073e-05,
      "loss": 0.7552,
      "step": 90
    },
    {
      "epoch": 0.24330900243309003,
      "grad_norm": 11.411809921264648,
      "learning_rate": 2.8554744525547447e-05,
      "loss": 0.871,
      "step": 100
    },
    {
      "epoch": 0.26763990267639903,
      "grad_norm": 1.683068871498108,
      "learning_rate": 2.8408759124087592e-05,
      "loss": 0.784,
      "step": 110
    },
    {
      "epoch": 0.291970802919708,
      "grad_norm": 3.8470547199249268,
      "learning_rate": 2.826277372262774e-05,
      "loss": 0.7669,
      "step": 120
    },
    {
      "epoch": 0.31630170316301703,
      "grad_norm": 4.337381362915039,
      "learning_rate": 2.8116788321167886e-05,
      "loss": 0.5524,
      "step": 130
    },
    {
      "epoch": 0.340632603406326,
      "grad_norm": 3.257624864578247,
      "learning_rate": 2.7970802919708027e-05,
      "loss": 0.5405,
      "step": 140
    },
    {
      "epoch": 0.36496350364963503,
      "grad_norm": 2.425150156021118,
      "learning_rate": 2.7824817518248176e-05,
      "loss": 0.4688,
      "step": 150
    },
    {
      "epoch": 0.38929440389294406,
      "grad_norm": 5.331384181976318,
      "learning_rate": 2.767883211678832e-05,
      "loss": 0.575,
      "step": 160
    },
    {
      "epoch": 0.41362530413625304,
      "grad_norm": 3.0689306259155273,
      "learning_rate": 2.7532846715328466e-05,
      "loss": 0.749,
      "step": 170
    },
    {
      "epoch": 0.43795620437956206,
      "grad_norm": 2.241914749145508,
      "learning_rate": 2.7386861313868615e-05,
      "loss": 0.6197,
      "step": 180
    },
    {
      "epoch": 0.46228710462287104,
      "grad_norm": 6.144436359405518,
      "learning_rate": 2.724087591240876e-05,
      "loss": 0.6017,
      "step": 190
    },
    {
      "epoch": 0.48661800486618007,
      "grad_norm": 3.459343433380127,
      "learning_rate": 2.7094890510948905e-05,
      "loss": 0.561,
      "step": 200
    },
    {
      "epoch": 0.5109489051094891,
      "grad_norm": 2.271285057067871,
      "learning_rate": 2.694890510948905e-05,
      "loss": 0.5629,
      "step": 210
    },
    {
      "epoch": 0.5352798053527981,
      "grad_norm": 4.38702917098999,
      "learning_rate": 2.68029197080292e-05,
      "loss": 0.601,
      "step": 220
    },
    {
      "epoch": 0.559610705596107,
      "grad_norm": 5.947737216949463,
      "learning_rate": 2.6656934306569344e-05,
      "loss": 0.6019,
      "step": 230
    },
    {
      "epoch": 0.583941605839416,
      "grad_norm": 4.9341654777526855,
      "learning_rate": 2.651094890510949e-05,
      "loss": 0.5257,
      "step": 240
    },
    {
      "epoch": 0.6082725060827251,
      "grad_norm": 3.674595832824707,
      "learning_rate": 2.6364963503649637e-05,
      "loss": 0.5465,
      "step": 250
    },
    {
      "epoch": 0.6326034063260341,
      "grad_norm": 14.41065788269043,
      "learning_rate": 2.6218978102189782e-05,
      "loss": 0.8378,
      "step": 260
    },
    {
      "epoch": 0.656934306569343,
      "grad_norm": 8.496246337890625,
      "learning_rate": 2.6072992700729928e-05,
      "loss": 0.5558,
      "step": 270
    },
    {
      "epoch": 0.681265206812652,
      "grad_norm": 1.7409957647323608,
      "learning_rate": 2.5927007299270076e-05,
      "loss": 0.4619,
      "step": 280
    },
    {
      "epoch": 0.7055961070559611,
      "grad_norm": 4.854860305786133,
      "learning_rate": 2.578102189781022e-05,
      "loss": 0.5325,
      "step": 290
    },
    {
      "epoch": 0.7299270072992701,
      "grad_norm": 4.894105911254883,
      "learning_rate": 2.5635036496350366e-05,
      "loss": 0.683,
      "step": 300
    },
    {
      "epoch": 0.754257907542579,
      "grad_norm": 4.039078235626221,
      "learning_rate": 2.548905109489051e-05,
      "loss": 0.5268,
      "step": 310
    },
    {
      "epoch": 0.7785888077858881,
      "grad_norm": 7.852715969085693,
      "learning_rate": 2.5343065693430657e-05,
      "loss": 0.7042,
      "step": 320
    },
    {
      "epoch": 0.8029197080291971,
      "grad_norm": 5.258782386779785,
      "learning_rate": 2.5197080291970802e-05,
      "loss": 0.4327,
      "step": 330
    },
    {
      "epoch": 0.8272506082725061,
      "grad_norm": 2.5321261882781982,
      "learning_rate": 2.5051094890510947e-05,
      "loss": 0.657,
      "step": 340
    },
    {
      "epoch": 0.851581508515815,
      "grad_norm": 5.037079811096191,
      "learning_rate": 2.4905109489051095e-05,
      "loss": 0.7894,
      "step": 350
    },
    {
      "epoch": 0.8759124087591241,
      "grad_norm": 3.961207866668701,
      "learning_rate": 2.475912408759124e-05,
      "loss": 0.6454,
      "step": 360
    },
    {
      "epoch": 0.9002433090024331,
      "grad_norm": 4.14401388168335,
      "learning_rate": 2.4613138686131386e-05,
      "loss": 0.4405,
      "step": 370
    },
    {
      "epoch": 0.9245742092457421,
      "grad_norm": 14.548818588256836,
      "learning_rate": 2.4467153284671534e-05,
      "loss": 0.3737,
      "step": 380
    },
    {
      "epoch": 0.948905109489051,
      "grad_norm": 4.137337684631348,
      "learning_rate": 2.432116788321168e-05,
      "loss": 0.398,
      "step": 390
    },
    {
      "epoch": 0.9732360097323601,
      "grad_norm": 5.8139777183532715,
      "learning_rate": 2.4175182481751824e-05,
      "loss": 0.4764,
      "step": 400
    },
    {
      "epoch": 0.9975669099756691,
      "grad_norm": 6.301417350769043,
      "learning_rate": 2.4029197080291973e-05,
      "loss": 0.5278,
      "step": 410
    },
    {
      "epoch": 1.0,
      "eval_f1": 0.72491263105342,
      "eval_loss": 0.15651477873325348,
      "eval_precision": 0.7781350482315113,
      "eval_recall": 0.6785046728971963,
      "eval_runtime": 115.413,
      "eval_samples_per_second": 7.33,
      "eval_steps_per_second": 0.918,
      "step": 411
    },
    {
      "epoch": 1.0218978102189782,
      "grad_norm": 2.745389461517334,
      "learning_rate": 2.3883211678832118e-05,
      "loss": 0.5366,
      "step": 420
    },
    {
      "epoch": 1.0462287104622872,
      "grad_norm": 9.634721755981445,
      "learning_rate": 2.3737226277372263e-05,
      "loss": 0.4925,
      "step": 430
    },
    {
      "epoch": 1.0705596107055961,
      "grad_norm": 8.818930625915527,
      "learning_rate": 2.359124087591241e-05,
      "loss": 0.3949,
      "step": 440
    },
    {
      "epoch": 1.094890510948905,
      "grad_norm": 5.662981033325195,
      "learning_rate": 2.3445255474452557e-05,
      "loss": 0.4452,
      "step": 450
    },
    {
      "epoch": 1.119221411192214,
      "grad_norm": 2.8783557415008545,
      "learning_rate": 2.3299270072992702e-05,
      "loss": 0.4599,
      "step": 460
    },
    {
      "epoch": 1.143552311435523,
      "grad_norm": 4.750424385070801,
      "learning_rate": 2.3153284671532847e-05,
      "loss": 0.4715,
      "step": 470
    },
    {
      "epoch": 1.167883211678832,
      "grad_norm": 2.511590003967285,
      "learning_rate": 2.3007299270072996e-05,
      "loss": 0.3003,
      "step": 480
    },
    {
      "epoch": 1.1922141119221412,
      "grad_norm": 2.498732805252075,
      "learning_rate": 2.286131386861314e-05,
      "loss": 0.3003,
      "step": 490
    },
    {
      "epoch": 1.2165450121654502,
      "grad_norm": 5.417802810668945,
      "learning_rate": 2.2715328467153286e-05,
      "loss": 0.5787,
      "step": 500
    },
    {
      "epoch": 1.2408759124087592,
      "grad_norm": 6.466491222381592,
      "learning_rate": 2.256934306569343e-05,
      "loss": 0.5091,
      "step": 510
    },
    {
      "epoch": 1.2652068126520681,
      "grad_norm": 2.7897956371307373,
      "learning_rate": 2.2423357664233576e-05,
      "loss": 0.549,
      "step": 520
    },
    {
      "epoch": 1.289537712895377,
      "grad_norm": 4.645617961883545,
      "learning_rate": 2.227737226277372e-05,
      "loss": 0.4827,
      "step": 530
    },
    {
      "epoch": 1.313868613138686,
      "grad_norm": 6.071934700012207,
      "learning_rate": 2.2131386861313866e-05,
      "loss": 0.406,
      "step": 540
    },
    {
      "epoch": 1.338199513381995,
      "grad_norm": 4.536595821380615,
      "learning_rate": 2.1985401459854015e-05,
      "loss": 0.424,
      "step": 550
    },
    {
      "epoch": 1.3625304136253042,
      "grad_norm": 4.890784740447998,
      "learning_rate": 2.183941605839416e-05,
      "loss": 0.5114,
      "step": 560
    },
    {
      "epoch": 1.3868613138686132,
      "grad_norm": 3.40224289894104,
      "learning_rate": 2.1693430656934305e-05,
      "loss": 0.351,
      "step": 570
    },
    {
      "epoch": 1.4111922141119222,
      "grad_norm": 3.307668685913086,
      "learning_rate": 2.1547445255474454e-05,
      "loss": 0.4058,
      "step": 580
    },
    {
      "epoch": 1.4355231143552312,
      "grad_norm": 4.317135334014893,
      "learning_rate": 2.14014598540146e-05,
      "loss": 0.3988,
      "step": 590
    },
    {
      "epoch": 1.4598540145985401,
      "grad_norm": 3.7555887699127197,
      "learning_rate": 2.1255474452554744e-05,
      "loss": 0.3375,
      "step": 600
    },
    {
      "epoch": 1.4841849148418491,
      "grad_norm": 3.6148464679718018,
      "learning_rate": 2.1109489051094893e-05,
      "loss": 0.4131,
      "step": 610
    },
    {
      "epoch": 1.508515815085158,
      "grad_norm": 3.887115240097046,
      "learning_rate": 2.0963503649635038e-05,
      "loss": 0.3398,
      "step": 620
    },
    {
      "epoch": 1.5328467153284673,
      "grad_norm": 6.109027862548828,
      "learning_rate": 2.0817518248175183e-05,
      "loss": 0.4505,
      "step": 630
    },
    {
      "epoch": 1.557177615571776,
      "grad_norm": 10.002976417541504,
      "learning_rate": 2.067153284671533e-05,
      "loss": 0.6535,
      "step": 640
    },
    {
      "epoch": 1.5815085158150852,
      "grad_norm": 3.562697649002075,
      "learning_rate": 2.0525547445255476e-05,
      "loss": 0.3165,
      "step": 650
    },
    {
      "epoch": 1.6058394160583942,
      "grad_norm": 0.780308187007904,
      "learning_rate": 2.037956204379562e-05,
      "loss": 0.315,
      "step": 660
    },
    {
      "epoch": 1.6301703163017032,
      "grad_norm": 7.892181396484375,
      "learning_rate": 2.0233576642335767e-05,
      "loss": 0.5272,
      "step": 670
    },
    {
      "epoch": 1.6545012165450121,
      "grad_norm": 2.8028883934020996,
      "learning_rate": 2.0087591240875915e-05,
      "loss": 0.2746,
      "step": 680
    },
    {
      "epoch": 1.6788321167883211,
      "grad_norm": 1.8399162292480469,
      "learning_rate": 1.994160583941606e-05,
      "loss": 0.285,
      "step": 690
    },
    {
      "epoch": 1.7031630170316303,
      "grad_norm": 1.4701802730560303,
      "learning_rate": 1.9795620437956202e-05,
      "loss": 0.234,
      "step": 700
    },
    {
      "epoch": 1.727493917274939,
      "grad_norm": 2.967451810836792,
      "learning_rate": 1.964963503649635e-05,
      "loss": 0.4251,
      "step": 710
    },
    {
      "epoch": 1.7518248175182483,
      "grad_norm": 3.065410614013672,
      "learning_rate": 1.9503649635036496e-05,
      "loss": 0.4014,
      "step": 720
    },
    {
      "epoch": 1.7761557177615572,
      "grad_norm": 16.88504981994629,
      "learning_rate": 1.935766423357664e-05,
      "loss": 0.3392,
      "step": 730
    },
    {
      "epoch": 1.8004866180048662,
      "grad_norm": 4.112252712249756,
      "learning_rate": 1.921167883211679e-05,
      "loss": 0.4845,
      "step": 740
    },
    {
      "epoch": 1.8248175182481752,
      "grad_norm": 4.819926738739014,
      "learning_rate": 1.9065693430656935e-05,
      "loss": 0.4877,
      "step": 750
    },
    {
      "epoch": 1.8491484184914841,
      "grad_norm": 25.192840576171875,
      "learning_rate": 1.891970802919708e-05,
      "loss": 0.3692,
      "step": 760
    },
    {
      "epoch": 1.8734793187347933,
      "grad_norm": 7.519534111022949,
      "learning_rate": 1.8773722627737225e-05,
      "loss": 0.5346,
      "step": 770
    },
    {
      "epoch": 1.897810218978102,
      "grad_norm": 5.713701248168945,
      "learning_rate": 1.8627737226277373e-05,
      "loss": 0.4018,
      "step": 780
    },
    {
      "epoch": 1.9221411192214113,
      "grad_norm": 12.125812530517578,
      "learning_rate": 1.848175182481752e-05,
      "loss": 0.3325,
      "step": 790
    },
    {
      "epoch": 1.94647201946472,
      "grad_norm": 5.3678436279296875,
      "learning_rate": 1.8335766423357664e-05,
      "loss": 0.5692,
      "step": 800
    },
    {
      "epoch": 1.9708029197080292,
      "grad_norm": 4.191765308380127,
      "learning_rate": 1.8189781021897812e-05,
      "loss": 0.3801,
      "step": 810
    },
    {
      "epoch": 1.9951338199513382,
      "grad_norm": 3.5782177448272705,
      "learning_rate": 1.8043795620437957e-05,
      "loss": 0.3326,
      "step": 820
    },
    {
      "epoch": 2.0,
      "eval_f1": 0.6849162011173185,
      "eval_loss": 0.14142312109470367,
      "eval_precision": 0.8513888888888889,
      "eval_recall": 0.5728971962616822,
      "eval_runtime": 115.64,
      "eval_samples_per_second": 7.316,
      "eval_steps_per_second": 0.917,
      "step": 822
    },
    {
      "epoch": 2.019464720194647,
      "grad_norm": 10.03662395477295,
      "learning_rate": 1.7897810218978102e-05,
      "loss": 0.3986,
      "step": 830
    },
    {
      "epoch": 2.0437956204379564,
      "grad_norm": 3.7400360107421875,
      "learning_rate": 1.775182481751825e-05,
      "loss": 0.3038,
      "step": 840
    },
    {
      "epoch": 2.068126520681265,
      "grad_norm": 8.444446563720703,
      "learning_rate": 1.7605839416058396e-05,
      "loss": 0.2755,
      "step": 850
    },
    {
      "epoch": 2.0924574209245743,
      "grad_norm": 6.496382236480713,
      "learning_rate": 1.745985401459854e-05,
      "loss": 0.3093,
      "step": 860
    },
    {
      "epoch": 2.116788321167883,
      "grad_norm": 3.9183199405670166,
      "learning_rate": 1.731386861313869e-05,
      "loss": 0.3367,
      "step": 870
    },
    {
      "epoch": 2.1411192214111923,
      "grad_norm": 7.7478251457214355,
      "learning_rate": 1.716788321167883e-05,
      "loss": 0.3807,
      "step": 880
    },
    {
      "epoch": 2.165450121654501,
      "grad_norm": 6.031020164489746,
      "learning_rate": 1.7021897810218977e-05,
      "loss": 0.2381,
      "step": 890
    },
    {
      "epoch": 2.18978102189781,
      "grad_norm": 16.017614364624023,
      "learning_rate": 1.6875912408759122e-05,
      "loss": 0.515,
      "step": 900
    },
    {
      "epoch": 2.2141119221411194,
      "grad_norm": 1.3664311170578003,
      "learning_rate": 1.672992700729927e-05,
      "loss": 0.2348,
      "step": 910
    },
    {
      "epoch": 2.238442822384428,
      "grad_norm": 8.965859413146973,
      "learning_rate": 1.6583941605839415e-05,
      "loss": 0.3755,
      "step": 920
    },
    {
      "epoch": 2.2627737226277373,
      "grad_norm": 5.90708589553833,
      "learning_rate": 1.643795620437956e-05,
      "loss": 0.2352,
      "step": 930
    },
    {
      "epoch": 2.287104622871046,
      "grad_norm": 3.56278920173645,
      "learning_rate": 1.629197080291971e-05,
      "loss": 0.3215,
      "step": 940
    },
    {
      "epoch": 2.3114355231143553,
      "grad_norm": 6.830710411071777,
      "learning_rate": 1.6145985401459854e-05,
      "loss": 0.4946,
      "step": 950
    },
    {
      "epoch": 2.335766423357664,
      "grad_norm": 4.835943698883057,
      "learning_rate": 1.6e-05,
      "loss": 0.3532,
      "step": 960
    },
    {
      "epoch": 2.3600973236009732,
      "grad_norm": 28.302743911743164,
      "learning_rate": 1.5854014598540148e-05,
      "loss": 0.367,
      "step": 970
    },
    {
      "epoch": 2.3844282238442824,
      "grad_norm": 3.9924702644348145,
      "learning_rate": 1.5708029197080293e-05,
      "loss": 0.4088,
      "step": 980
    },
    {
      "epoch": 2.408759124087591,
      "grad_norm": 8.511579513549805,
      "learning_rate": 1.5562043795620438e-05,
      "loss": 0.3963,
      "step": 990
    },
    {
      "epoch": 2.4330900243309004,
      "grad_norm": 2.161227226257324,
      "learning_rate": 1.5416058394160583e-05,
      "loss": 0.2533,
      "step": 1000
    },
    {
      "epoch": 2.457420924574209,
      "grad_norm": 5.1398210525512695,
      "learning_rate": 1.5270072992700732e-05,
      "loss": 0.3621,
      "step": 1010
    },
    {
      "epoch": 2.4817518248175183,
      "grad_norm": 8.115911483764648,
      "learning_rate": 1.5124087591240877e-05,
      "loss": 0.2607,
      "step": 1020
    },
    {
      "epoch": 2.5060827250608275,
      "grad_norm": 5.644152641296387,
      "learning_rate": 1.4978102189781022e-05,
      "loss": 0.1788,
      "step": 1030
    },
    {
      "epoch": 2.5304136253041363,
      "grad_norm": 8.062714576721191,
      "learning_rate": 1.4832116788321167e-05,
      "loss": 0.3381,
      "step": 1040
    },
    {
      "epoch": 2.554744525547445,
      "grad_norm": 19.882631301879883,
      "learning_rate": 1.4686131386861314e-05,
      "loss": 0.3151,
      "step": 1050
    },
    {
      "epoch": 2.579075425790754,
      "grad_norm": 5.699784755706787,
      "learning_rate": 1.454014598540146e-05,
      "loss": 0.3127,
      "step": 1060
    },
    {
      "epoch": 2.6034063260340634,
      "grad_norm": 7.419994831085205,
      "learning_rate": 1.4394160583941606e-05,
      "loss": 0.26,
      "step": 1070
    },
    {
      "epoch": 2.627737226277372,
      "grad_norm": 6.655620574951172,
      "learning_rate": 1.4248175182481753e-05,
      "loss": 0.3914,
      "step": 1080
    },
    {
      "epoch": 2.6520681265206814,
      "grad_norm": 1.6455236673355103,
      "learning_rate": 1.4102189781021898e-05,
      "loss": 0.2415,
      "step": 1090
    },
    {
      "epoch": 2.67639902676399,
      "grad_norm": 2.229762077331543,
      "learning_rate": 1.3956204379562045e-05,
      "loss": 0.3286,
      "step": 1100
    },
    {
      "epoch": 2.7007299270072993,
      "grad_norm": 8.99122428894043,
      "learning_rate": 1.3810218978102192e-05,
      "loss": 0.6014,
      "step": 1110
    },
    {
      "epoch": 2.7250608272506085,
      "grad_norm": 2.6908364295959473,
      "learning_rate": 1.3664233576642335e-05,
      "loss": 0.408,
      "step": 1120
    },
    {
      "epoch": 2.7493917274939172,
      "grad_norm": 1.726692795753479,
      "learning_rate": 1.3518248175182482e-05,
      "loss": 0.2653,
      "step": 1130
    },
    {
      "epoch": 2.7737226277372264,
      "grad_norm": 1.9450236558914185,
      "learning_rate": 1.3372262773722627e-05,
      "loss": 0.4338,
      "step": 1140
    },
    {
      "epoch": 2.798053527980535,
      "grad_norm": 2.604060649871826,
      "learning_rate": 1.3226277372262774e-05,
      "loss": 0.2292,
      "step": 1150
    },
    {
      "epoch": 2.8223844282238444,
      "grad_norm": 2.905799150466919,
      "learning_rate": 1.308029197080292e-05,
      "loss": 0.2938,
      "step": 1160
    },
    {
      "epoch": 2.846715328467153,
      "grad_norm": 3.6705586910247803,
      "learning_rate": 1.2934306569343066e-05,
      "loss": 0.4668,
      "step": 1170
    },
    {
      "epoch": 2.8710462287104623,
      "grad_norm": 2.8418140411376953,
      "learning_rate": 1.2788321167883213e-05,
      "loss": 0.3136,
      "step": 1180
    },
    {
      "epoch": 2.895377128953771,
      "grad_norm": 4.437412738800049,
      "learning_rate": 1.264233576642336e-05,
      "loss": 0.1953,
      "step": 1190
    },
    {
      "epoch": 2.9197080291970803,
      "grad_norm": 5.639729022979736,
      "learning_rate": 1.2496350364963504e-05,
      "loss": 0.3756,
      "step": 1200
    },
    {
      "epoch": 2.9440389294403895,
      "grad_norm": 7.636282444000244,
      "learning_rate": 1.235036496350365e-05,
      "loss": 0.3306,
      "step": 1210
    },
    {
      "epoch": 2.9683698296836982,
      "grad_norm": 12.881672859191895,
      "learning_rate": 1.2204379562043795e-05,
      "loss": 0.4252,
      "step": 1220
    },
    {
      "epoch": 2.9927007299270074,
      "grad_norm": 2.1252357959747314,
      "learning_rate": 1.2058394160583942e-05,
      "loss": 0.2617,
      "step": 1230
    },
    {
      "epoch": 3.0,
      "eval_f1": 0.5640695428203477,
      "eval_loss": 0.21638774871826172,
      "eval_precision": 0.906832298136646,
      "eval_recall": 0.4093457943925234,
      "eval_runtime": 115.4449,
      "eval_samples_per_second": 7.328,
      "eval_steps_per_second": 0.918,
      "step": 1233
    }
  ],
  "logging_steps": 10,
  "max_steps": 2055,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 54441542916864.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
