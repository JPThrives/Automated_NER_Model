{
  "best_global_step": 7008,
  "best_metric": 0.6322787404060364,
  "best_model_checkpoint": "./assamese-ner-model\\checkpoint-7008",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 7008,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004280821917808219,
      "grad_norm": 9.143527030944824,
      "learning_rate": 2.9976883561643835e-05,
      "loss": 3.2571,
      "step": 10
    },
    {
      "epoch": 0.008561643835616438,
      "grad_norm": 7.429162979125977,
      "learning_rate": 2.9951198630136988e-05,
      "loss": 2.773,
      "step": 20
    },
    {
      "epoch": 0.012842465753424657,
      "grad_norm": 9.300058364868164,
      "learning_rate": 2.9925513698630138e-05,
      "loss": 2.5513,
      "step": 30
    },
    {
      "epoch": 0.017123287671232876,
      "grad_norm": 11.095643043518066,
      "learning_rate": 2.9899828767123288e-05,
      "loss": 2.2438,
      "step": 40
    },
    {
      "epoch": 0.021404109589041095,
      "grad_norm": 5.4018874168396,
      "learning_rate": 2.9874143835616437e-05,
      "loss": 2.0131,
      "step": 50
    },
    {
      "epoch": 0.025684931506849314,
      "grad_norm": 6.561296463012695,
      "learning_rate": 2.984845890410959e-05,
      "loss": 1.9834,
      "step": 60
    },
    {
      "epoch": 0.029965753424657533,
      "grad_norm": 9.898076057434082,
      "learning_rate": 2.982277397260274e-05,
      "loss": 1.8944,
      "step": 70
    },
    {
      "epoch": 0.03424657534246575,
      "grad_norm": 5.873605251312256,
      "learning_rate": 2.979708904109589e-05,
      "loss": 2.1073,
      "step": 80
    },
    {
      "epoch": 0.038527397260273974,
      "grad_norm": 9.62494945526123,
      "learning_rate": 2.977140410958904e-05,
      "loss": 1.642,
      "step": 90
    },
    {
      "epoch": 0.04280821917808219,
      "grad_norm": 7.739095687866211,
      "learning_rate": 2.9745719178082193e-05,
      "loss": 1.8328,
      "step": 100
    },
    {
      "epoch": 0.04708904109589041,
      "grad_norm": 10.027473449707031,
      "learning_rate": 2.9720034246575343e-05,
      "loss": 1.5973,
      "step": 110
    },
    {
      "epoch": 0.05136986301369863,
      "grad_norm": 9.248682975769043,
      "learning_rate": 2.9694349315068496e-05,
      "loss": 1.3256,
      "step": 120
    },
    {
      "epoch": 0.05565068493150685,
      "grad_norm": 11.150430679321289,
      "learning_rate": 2.9668664383561642e-05,
      "loss": 1.8203,
      "step": 130
    },
    {
      "epoch": 0.059931506849315065,
      "grad_norm": 7.573376178741455,
      "learning_rate": 2.9642979452054796e-05,
      "loss": 1.7003,
      "step": 140
    },
    {
      "epoch": 0.0642123287671233,
      "grad_norm": 27.757863998413086,
      "learning_rate": 2.9617294520547945e-05,
      "loss": 1.4981,
      "step": 150
    },
    {
      "epoch": 0.0684931506849315,
      "grad_norm": 3.9192657470703125,
      "learning_rate": 2.95916095890411e-05,
      "loss": 1.3755,
      "step": 160
    },
    {
      "epoch": 0.07277397260273973,
      "grad_norm": 15.962122917175293,
      "learning_rate": 2.9565924657534245e-05,
      "loss": 1.3564,
      "step": 170
    },
    {
      "epoch": 0.07705479452054795,
      "grad_norm": 6.152198791503906,
      "learning_rate": 2.9540239726027398e-05,
      "loss": 1.4281,
      "step": 180
    },
    {
      "epoch": 0.08133561643835617,
      "grad_norm": 12.189838409423828,
      "learning_rate": 2.9514554794520548e-05,
      "loss": 1.7825,
      "step": 190
    },
    {
      "epoch": 0.08561643835616438,
      "grad_norm": 15.862617492675781,
      "learning_rate": 2.94888698630137e-05,
      "loss": 1.4193,
      "step": 200
    },
    {
      "epoch": 0.0898972602739726,
      "grad_norm": 10.695959091186523,
      "learning_rate": 2.946318493150685e-05,
      "loss": 1.3413,
      "step": 210
    },
    {
      "epoch": 0.09417808219178082,
      "grad_norm": 8.591190338134766,
      "learning_rate": 2.94375e-05,
      "loss": 1.4734,
      "step": 220
    },
    {
      "epoch": 0.09845890410958905,
      "grad_norm": 23.265018463134766,
      "learning_rate": 2.941181506849315e-05,
      "loss": 1.3237,
      "step": 230
    },
    {
      "epoch": 0.10273972602739725,
      "grad_norm": 12.896937370300293,
      "learning_rate": 2.9386130136986304e-05,
      "loss": 1.281,
      "step": 240
    },
    {
      "epoch": 0.10702054794520548,
      "grad_norm": 7.430508613586426,
      "learning_rate": 2.9360445205479453e-05,
      "loss": 1.5752,
      "step": 250
    },
    {
      "epoch": 0.1113013698630137,
      "grad_norm": 9.0586519241333,
      "learning_rate": 2.9334760273972607e-05,
      "loss": 1.1468,
      "step": 260
    },
    {
      "epoch": 0.11558219178082192,
      "grad_norm": 9.041784286499023,
      "learning_rate": 2.9309075342465753e-05,
      "loss": 1.3621,
      "step": 270
    },
    {
      "epoch": 0.11986301369863013,
      "grad_norm": 17.757610321044922,
      "learning_rate": 2.9283390410958906e-05,
      "loss": 1.3347,
      "step": 280
    },
    {
      "epoch": 0.12414383561643835,
      "grad_norm": 6.512852668762207,
      "learning_rate": 2.9257705479452056e-05,
      "loss": 1.2847,
      "step": 290
    },
    {
      "epoch": 0.1284246575342466,
      "grad_norm": 11.371417999267578,
      "learning_rate": 2.923202054794521e-05,
      "loss": 1.3513,
      "step": 300
    },
    {
      "epoch": 0.1327054794520548,
      "grad_norm": 10.900997161865234,
      "learning_rate": 2.9206335616438355e-05,
      "loss": 1.3872,
      "step": 310
    },
    {
      "epoch": 0.136986301369863,
      "grad_norm": 25.528583526611328,
      "learning_rate": 2.9180650684931505e-05,
      "loss": 0.9956,
      "step": 320
    },
    {
      "epoch": 0.14126712328767124,
      "grad_norm": 11.505348205566406,
      "learning_rate": 2.915496575342466e-05,
      "loss": 1.3819,
      "step": 330
    },
    {
      "epoch": 0.14554794520547945,
      "grad_norm": 11.744062423706055,
      "learning_rate": 2.9129280821917808e-05,
      "loss": 1.1022,
      "step": 340
    },
    {
      "epoch": 0.14982876712328766,
      "grad_norm": 10.921093940734863,
      "learning_rate": 2.910359589041096e-05,
      "loss": 1.2909,
      "step": 350
    },
    {
      "epoch": 0.1541095890410959,
      "grad_norm": 28.27537727355957,
      "learning_rate": 2.9077910958904108e-05,
      "loss": 1.1084,
      "step": 360
    },
    {
      "epoch": 0.1583904109589041,
      "grad_norm": 14.745160102844238,
      "learning_rate": 2.905222602739726e-05,
      "loss": 1.3607,
      "step": 370
    },
    {
      "epoch": 0.16267123287671234,
      "grad_norm": 9.820703506469727,
      "learning_rate": 2.902654109589041e-05,
      "loss": 1.144,
      "step": 380
    },
    {
      "epoch": 0.16695205479452055,
      "grad_norm": 20.382030487060547,
      "learning_rate": 2.9000856164383564e-05,
      "loss": 1.1846,
      "step": 390
    },
    {
      "epoch": 0.17123287671232876,
      "grad_norm": 11.614411354064941,
      "learning_rate": 2.897517123287671e-05,
      "loss": 1.1864,
      "step": 400
    },
    {
      "epoch": 0.175513698630137,
      "grad_norm": 18.76705551147461,
      "learning_rate": 2.8949486301369863e-05,
      "loss": 1.229,
      "step": 410
    },
    {
      "epoch": 0.1797945205479452,
      "grad_norm": 14.084985733032227,
      "learning_rate": 2.8923801369863013e-05,
      "loss": 0.9564,
      "step": 420
    },
    {
      "epoch": 0.1840753424657534,
      "grad_norm": 15.900201797485352,
      "learning_rate": 2.8898116438356166e-05,
      "loss": 1.0985,
      "step": 430
    },
    {
      "epoch": 0.18835616438356165,
      "grad_norm": 13.179973602294922,
      "learning_rate": 2.8872431506849316e-05,
      "loss": 0.9747,
      "step": 440
    },
    {
      "epoch": 0.19263698630136986,
      "grad_norm": 26.49134635925293,
      "learning_rate": 2.8846746575342466e-05,
      "loss": 1.2553,
      "step": 450
    },
    {
      "epoch": 0.1969178082191781,
      "grad_norm": 20.5519962310791,
      "learning_rate": 2.8821061643835616e-05,
      "loss": 0.8543,
      "step": 460
    },
    {
      "epoch": 0.2011986301369863,
      "grad_norm": 21.370920181274414,
      "learning_rate": 2.879537671232877e-05,
      "loss": 1.1098,
      "step": 470
    },
    {
      "epoch": 0.2054794520547945,
      "grad_norm": 15.452706336975098,
      "learning_rate": 2.876969178082192e-05,
      "loss": 1.1207,
      "step": 480
    },
    {
      "epoch": 0.20976027397260275,
      "grad_norm": 41.02529525756836,
      "learning_rate": 2.874400684931507e-05,
      "loss": 0.8128,
      "step": 490
    },
    {
      "epoch": 0.21404109589041095,
      "grad_norm": 23.58036231994629,
      "learning_rate": 2.8718321917808218e-05,
      "loss": 1.0838,
      "step": 500
    },
    {
      "epoch": 0.2183219178082192,
      "grad_norm": 17.155776977539062,
      "learning_rate": 2.869263698630137e-05,
      "loss": 0.8445,
      "step": 510
    },
    {
      "epoch": 0.2226027397260274,
      "grad_norm": 15.116822242736816,
      "learning_rate": 2.866695205479452e-05,
      "loss": 1.2592,
      "step": 520
    },
    {
      "epoch": 0.2268835616438356,
      "grad_norm": 6.698291301727295,
      "learning_rate": 2.8641267123287674e-05,
      "loss": 1.0454,
      "step": 530
    },
    {
      "epoch": 0.23116438356164384,
      "grad_norm": 27.467140197753906,
      "learning_rate": 2.861558219178082e-05,
      "loss": 0.8242,
      "step": 540
    },
    {
      "epoch": 0.23544520547945205,
      "grad_norm": 16.7839412689209,
      "learning_rate": 2.8589897260273974e-05,
      "loss": 1.0546,
      "step": 550
    },
    {
      "epoch": 0.23972602739726026,
      "grad_norm": 8.238468170166016,
      "learning_rate": 2.8564212328767124e-05,
      "loss": 0.988,
      "step": 560
    },
    {
      "epoch": 0.2440068493150685,
      "grad_norm": 22.661073684692383,
      "learning_rate": 2.8538527397260277e-05,
      "loss": 0.9039,
      "step": 570
    },
    {
      "epoch": 0.2482876712328767,
      "grad_norm": 15.265829086303711,
      "learning_rate": 2.8512842465753427e-05,
      "loss": 1.0021,
      "step": 580
    },
    {
      "epoch": 0.2525684931506849,
      "grad_norm": 49.56181335449219,
      "learning_rate": 2.8487157534246576e-05,
      "loss": 1.146,
      "step": 590
    },
    {
      "epoch": 0.2568493150684932,
      "grad_norm": 14.604925155639648,
      "learning_rate": 2.8461472602739726e-05,
      "loss": 0.9398,
      "step": 600
    },
    {
      "epoch": 0.2611301369863014,
      "grad_norm": 8.990811347961426,
      "learning_rate": 2.843578767123288e-05,
      "loss": 0.7949,
      "step": 610
    },
    {
      "epoch": 0.2654109589041096,
      "grad_norm": 24.059226989746094,
      "learning_rate": 2.841010273972603e-05,
      "loss": 1.1607,
      "step": 620
    },
    {
      "epoch": 0.2696917808219178,
      "grad_norm": 8.618311882019043,
      "learning_rate": 2.838441780821918e-05,
      "loss": 1.0555,
      "step": 630
    },
    {
      "epoch": 0.273972602739726,
      "grad_norm": 9.342193603515625,
      "learning_rate": 2.835873287671233e-05,
      "loss": 1.0055,
      "step": 640
    },
    {
      "epoch": 0.2782534246575342,
      "grad_norm": 11.598215103149414,
      "learning_rate": 2.8333047945205482e-05,
      "loss": 1.1484,
      "step": 650
    },
    {
      "epoch": 0.2825342465753425,
      "grad_norm": 25.86655616760254,
      "learning_rate": 2.8307363013698632e-05,
      "loss": 0.9345,
      "step": 660
    },
    {
      "epoch": 0.2868150684931507,
      "grad_norm": 29.468690872192383,
      "learning_rate": 2.828167808219178e-05,
      "loss": 1.0117,
      "step": 670
    },
    {
      "epoch": 0.2910958904109589,
      "grad_norm": 21.54947280883789,
      "learning_rate": 2.825599315068493e-05,
      "loss": 1.032,
      "step": 680
    },
    {
      "epoch": 0.2953767123287671,
      "grad_norm": 13.336138725280762,
      "learning_rate": 2.823030821917808e-05,
      "loss": 1.0047,
      "step": 690
    },
    {
      "epoch": 0.2996575342465753,
      "grad_norm": 36.05213165283203,
      "learning_rate": 2.8204623287671234e-05,
      "loss": 1.0083,
      "step": 700
    },
    {
      "epoch": 0.3039383561643836,
      "grad_norm": 36.2824821472168,
      "learning_rate": 2.8178938356164384e-05,
      "loss": 0.8792,
      "step": 710
    },
    {
      "epoch": 0.3082191780821918,
      "grad_norm": 15.812623977661133,
      "learning_rate": 2.8153253424657534e-05,
      "loss": 0.9901,
      "step": 720
    },
    {
      "epoch": 0.3125,
      "grad_norm": 13.622113227844238,
      "learning_rate": 2.8127568493150684e-05,
      "loss": 1.08,
      "step": 730
    },
    {
      "epoch": 0.3167808219178082,
      "grad_norm": 19.393569946289062,
      "learning_rate": 2.8101883561643837e-05,
      "loss": 0.8416,
      "step": 740
    },
    {
      "epoch": 0.3210616438356164,
      "grad_norm": 9.216547966003418,
      "learning_rate": 2.8076198630136987e-05,
      "loss": 0.9563,
      "step": 750
    },
    {
      "epoch": 0.3253424657534247,
      "grad_norm": 64.56256103515625,
      "learning_rate": 2.805051369863014e-05,
      "loss": 0.9758,
      "step": 760
    },
    {
      "epoch": 0.3296232876712329,
      "grad_norm": 16.644256591796875,
      "learning_rate": 2.8024828767123286e-05,
      "loss": 0.9189,
      "step": 770
    },
    {
      "epoch": 0.3339041095890411,
      "grad_norm": 22.02782440185547,
      "learning_rate": 2.799914383561644e-05,
      "loss": 1.0309,
      "step": 780
    },
    {
      "epoch": 0.3381849315068493,
      "grad_norm": 14.417555809020996,
      "learning_rate": 2.797345890410959e-05,
      "loss": 1.0168,
      "step": 790
    },
    {
      "epoch": 0.3424657534246575,
      "grad_norm": 17.383155822753906,
      "learning_rate": 2.7947773972602742e-05,
      "loss": 0.8795,
      "step": 800
    },
    {
      "epoch": 0.3467465753424658,
      "grad_norm": 16.478063583374023,
      "learning_rate": 2.792208904109589e-05,
      "loss": 0.9981,
      "step": 810
    },
    {
      "epoch": 0.351027397260274,
      "grad_norm": 46.852333068847656,
      "learning_rate": 2.7896404109589042e-05,
      "loss": 0.8396,
      "step": 820
    },
    {
      "epoch": 0.3553082191780822,
      "grad_norm": 16.336999893188477,
      "learning_rate": 2.787071917808219e-05,
      "loss": 0.8869,
      "step": 830
    },
    {
      "epoch": 0.3595890410958904,
      "grad_norm": 18.43584442138672,
      "learning_rate": 2.7845034246575345e-05,
      "loss": 1.0838,
      "step": 840
    },
    {
      "epoch": 0.3638698630136986,
      "grad_norm": 27.185283660888672,
      "learning_rate": 2.7819349315068495e-05,
      "loss": 0.7127,
      "step": 850
    },
    {
      "epoch": 0.3681506849315068,
      "grad_norm": 9.711454391479492,
      "learning_rate": 2.7793664383561644e-05,
      "loss": 0.6737,
      "step": 860
    },
    {
      "epoch": 0.3724315068493151,
      "grad_norm": 18.34222412109375,
      "learning_rate": 2.7767979452054794e-05,
      "loss": 1.108,
      "step": 870
    },
    {
      "epoch": 0.3767123287671233,
      "grad_norm": 9.437512397766113,
      "learning_rate": 2.7742294520547947e-05,
      "loss": 0.9617,
      "step": 880
    },
    {
      "epoch": 0.3809931506849315,
      "grad_norm": 6.7784905433654785,
      "learning_rate": 2.7716609589041097e-05,
      "loss": 0.894,
      "step": 890
    },
    {
      "epoch": 0.3852739726027397,
      "grad_norm": 14.5828218460083,
      "learning_rate": 2.7690924657534247e-05,
      "loss": 0.8916,
      "step": 900
    },
    {
      "epoch": 0.3895547945205479,
      "grad_norm": 13.952213287353516,
      "learning_rate": 2.7665239726027397e-05,
      "loss": 1.2107,
      "step": 910
    },
    {
      "epoch": 0.3938356164383562,
      "grad_norm": 23.894481658935547,
      "learning_rate": 2.763955479452055e-05,
      "loss": 1.0384,
      "step": 920
    },
    {
      "epoch": 0.3981164383561644,
      "grad_norm": 12.81005573272705,
      "learning_rate": 2.76138698630137e-05,
      "loss": 0.8794,
      "step": 930
    },
    {
      "epoch": 0.4023972602739726,
      "grad_norm": 11.605198860168457,
      "learning_rate": 2.7588184931506853e-05,
      "loss": 1.0815,
      "step": 940
    },
    {
      "epoch": 0.4066780821917808,
      "grad_norm": 19.173080444335938,
      "learning_rate": 2.75625e-05,
      "loss": 0.879,
      "step": 950
    },
    {
      "epoch": 0.410958904109589,
      "grad_norm": 16.12763786315918,
      "learning_rate": 2.7536815068493152e-05,
      "loss": 1.2316,
      "step": 960
    },
    {
      "epoch": 0.4152397260273973,
      "grad_norm": 11.04924488067627,
      "learning_rate": 2.7511130136986302e-05,
      "loss": 1.0372,
      "step": 970
    },
    {
      "epoch": 0.4195205479452055,
      "grad_norm": 12.3121337890625,
      "learning_rate": 2.7485445205479455e-05,
      "loss": 0.8087,
      "step": 980
    },
    {
      "epoch": 0.4238013698630137,
      "grad_norm": 31.46891212463379,
      "learning_rate": 2.7459760273972605e-05,
      "loss": 0.9003,
      "step": 990
    },
    {
      "epoch": 0.4280821917808219,
      "grad_norm": 12.546882629394531,
      "learning_rate": 2.7434075342465755e-05,
      "loss": 1.07,
      "step": 1000
    },
    {
      "epoch": 0.4323630136986301,
      "grad_norm": 8.71003532409668,
      "learning_rate": 2.7408390410958905e-05,
      "loss": 0.9106,
      "step": 1010
    },
    {
      "epoch": 0.4366438356164384,
      "grad_norm": 42.478759765625,
      "learning_rate": 2.7382705479452054e-05,
      "loss": 0.8865,
      "step": 1020
    },
    {
      "epoch": 0.4409246575342466,
      "grad_norm": 15.516157150268555,
      "learning_rate": 2.7357020547945208e-05,
      "loss": 1.078,
      "step": 1030
    },
    {
      "epoch": 0.4452054794520548,
      "grad_norm": 20.25251579284668,
      "learning_rate": 2.7331335616438354e-05,
      "loss": 0.9342,
      "step": 1040
    },
    {
      "epoch": 0.449486301369863,
      "grad_norm": 5.450655937194824,
      "learning_rate": 2.7305650684931507e-05,
      "loss": 0.7869,
      "step": 1050
    },
    {
      "epoch": 0.4537671232876712,
      "grad_norm": 26.11355972290039,
      "learning_rate": 2.7279965753424657e-05,
      "loss": 1.1147,
      "step": 1060
    },
    {
      "epoch": 0.4580479452054795,
      "grad_norm": 11.541396141052246,
      "learning_rate": 2.725428082191781e-05,
      "loss": 0.9773,
      "step": 1070
    },
    {
      "epoch": 0.4623287671232877,
      "grad_norm": 9.537392616271973,
      "learning_rate": 2.722859589041096e-05,
      "loss": 0.6934,
      "step": 1080
    },
    {
      "epoch": 0.4666095890410959,
      "grad_norm": 22.873441696166992,
      "learning_rate": 2.720291095890411e-05,
      "loss": 0.9617,
      "step": 1090
    },
    {
      "epoch": 0.4708904109589041,
      "grad_norm": 10.116670608520508,
      "learning_rate": 2.717722602739726e-05,
      "loss": 0.9711,
      "step": 1100
    },
    {
      "epoch": 0.4751712328767123,
      "grad_norm": 20.031970977783203,
      "learning_rate": 2.7151541095890413e-05,
      "loss": 1.0581,
      "step": 1110
    },
    {
      "epoch": 0.4794520547945205,
      "grad_norm": 19.42740821838379,
      "learning_rate": 2.7125856164383562e-05,
      "loss": 0.8627,
      "step": 1120
    },
    {
      "epoch": 0.4837328767123288,
      "grad_norm": 12.646263122558594,
      "learning_rate": 2.7100171232876712e-05,
      "loss": 0.8751,
      "step": 1130
    },
    {
      "epoch": 0.488013698630137,
      "grad_norm": 4.523874759674072,
      "learning_rate": 2.7074486301369862e-05,
      "loss": 0.8219,
      "step": 1140
    },
    {
      "epoch": 0.4922945205479452,
      "grad_norm": 13.249631881713867,
      "learning_rate": 2.7048801369863015e-05,
      "loss": 0.7833,
      "step": 1150
    },
    {
      "epoch": 0.4965753424657534,
      "grad_norm": 7.592014789581299,
      "learning_rate": 2.7023116438356165e-05,
      "loss": 0.8157,
      "step": 1160
    },
    {
      "epoch": 0.5008561643835616,
      "grad_norm": 34.3780403137207,
      "learning_rate": 2.6997431506849318e-05,
      "loss": 0.9072,
      "step": 1170
    },
    {
      "epoch": 0.5051369863013698,
      "grad_norm": 30.691129684448242,
      "learning_rate": 2.6971746575342464e-05,
      "loss": 0.6879,
      "step": 1180
    },
    {
      "epoch": 0.509417808219178,
      "grad_norm": 6.798182487487793,
      "learning_rate": 2.6946061643835618e-05,
      "loss": 0.8668,
      "step": 1190
    },
    {
      "epoch": 0.5136986301369864,
      "grad_norm": 39.035675048828125,
      "learning_rate": 2.6920376712328767e-05,
      "loss": 0.8607,
      "step": 1200
    },
    {
      "epoch": 0.5179794520547946,
      "grad_norm": 14.542919158935547,
      "learning_rate": 2.689469178082192e-05,
      "loss": 0.811,
      "step": 1210
    },
    {
      "epoch": 0.5222602739726028,
      "grad_norm": 8.363486289978027,
      "learning_rate": 2.6869006849315067e-05,
      "loss": 0.6759,
      "step": 1220
    },
    {
      "epoch": 0.526541095890411,
      "grad_norm": 6.7858123779296875,
      "learning_rate": 2.684332191780822e-05,
      "loss": 0.6917,
      "step": 1230
    },
    {
      "epoch": 0.5308219178082192,
      "grad_norm": 32.5490837097168,
      "learning_rate": 2.681763698630137e-05,
      "loss": 0.9429,
      "step": 1240
    },
    {
      "epoch": 0.5351027397260274,
      "grad_norm": 6.6626458168029785,
      "learning_rate": 2.6791952054794523e-05,
      "loss": 0.8783,
      "step": 1250
    },
    {
      "epoch": 0.5393835616438356,
      "grad_norm": 14.975225448608398,
      "learning_rate": 2.6766267123287673e-05,
      "loss": 0.7047,
      "step": 1260
    },
    {
      "epoch": 0.5436643835616438,
      "grad_norm": 13.297146797180176,
      "learning_rate": 2.6740582191780823e-05,
      "loss": 0.8957,
      "step": 1270
    },
    {
      "epoch": 0.547945205479452,
      "grad_norm": 12.45203685760498,
      "learning_rate": 2.6714897260273972e-05,
      "loss": 0.88,
      "step": 1280
    },
    {
      "epoch": 0.5522260273972602,
      "grad_norm": 4.13766622543335,
      "learning_rate": 2.6689212328767126e-05,
      "loss": 0.6168,
      "step": 1290
    },
    {
      "epoch": 0.5565068493150684,
      "grad_norm": 45.81571578979492,
      "learning_rate": 2.6663527397260275e-05,
      "loss": 1.0284,
      "step": 1300
    },
    {
      "epoch": 0.5607876712328768,
      "grad_norm": 8.625859260559082,
      "learning_rate": 2.663784246575343e-05,
      "loss": 0.5634,
      "step": 1310
    },
    {
      "epoch": 0.565068493150685,
      "grad_norm": 20.825454711914062,
      "learning_rate": 2.6612157534246575e-05,
      "loss": 0.9129,
      "step": 1320
    },
    {
      "epoch": 0.5693493150684932,
      "grad_norm": 12.970701217651367,
      "learning_rate": 2.6586472602739728e-05,
      "loss": 0.686,
      "step": 1330
    },
    {
      "epoch": 0.5736301369863014,
      "grad_norm": 21.17759895324707,
      "learning_rate": 2.6560787671232878e-05,
      "loss": 1.027,
      "step": 1340
    },
    {
      "epoch": 0.5779109589041096,
      "grad_norm": 4.656286716461182,
      "learning_rate": 2.6535102739726028e-05,
      "loss": 0.7063,
      "step": 1350
    },
    {
      "epoch": 0.5821917808219178,
      "grad_norm": 11.171497344970703,
      "learning_rate": 2.6509417808219177e-05,
      "loss": 0.8304,
      "step": 1360
    },
    {
      "epoch": 0.586472602739726,
      "grad_norm": 13.476147651672363,
      "learning_rate": 2.6483732876712327e-05,
      "loss": 0.7401,
      "step": 1370
    },
    {
      "epoch": 0.5907534246575342,
      "grad_norm": 11.683135032653809,
      "learning_rate": 2.645804794520548e-05,
      "loss": 0.8161,
      "step": 1380
    },
    {
      "epoch": 0.5950342465753424,
      "grad_norm": 7.547550678253174,
      "learning_rate": 2.643236301369863e-05,
      "loss": 0.8678,
      "step": 1390
    },
    {
      "epoch": 0.5993150684931506,
      "grad_norm": 4.967058181762695,
      "learning_rate": 2.6406678082191783e-05,
      "loss": 0.5675,
      "step": 1400
    },
    {
      "epoch": 0.603595890410959,
      "grad_norm": 8.383954048156738,
      "learning_rate": 2.638099315068493e-05,
      "loss": 0.7366,
      "step": 1410
    },
    {
      "epoch": 0.6078767123287672,
      "grad_norm": 3.407871723175049,
      "learning_rate": 2.6355308219178083e-05,
      "loss": 0.8091,
      "step": 1420
    },
    {
      "epoch": 0.6121575342465754,
      "grad_norm": 3.6211371421813965,
      "learning_rate": 2.6329623287671233e-05,
      "loss": 0.5425,
      "step": 1430
    },
    {
      "epoch": 0.6164383561643836,
      "grad_norm": 8.027620315551758,
      "learning_rate": 2.6303938356164386e-05,
      "loss": 1.2665,
      "step": 1440
    },
    {
      "epoch": 0.6207191780821918,
      "grad_norm": 10.842984199523926,
      "learning_rate": 2.6278253424657532e-05,
      "loss": 0.8933,
      "step": 1450
    },
    {
      "epoch": 0.625,
      "grad_norm": 17.257038116455078,
      "learning_rate": 2.6252568493150685e-05,
      "loss": 0.9975,
      "step": 1460
    },
    {
      "epoch": 0.6292808219178082,
      "grad_norm": 8.613531112670898,
      "learning_rate": 2.6226883561643835e-05,
      "loss": 0.9868,
      "step": 1470
    },
    {
      "epoch": 0.6335616438356164,
      "grad_norm": 12.613505363464355,
      "learning_rate": 2.620119863013699e-05,
      "loss": 0.8849,
      "step": 1480
    },
    {
      "epoch": 0.6378424657534246,
      "grad_norm": 15.471649169921875,
      "learning_rate": 2.6175513698630138e-05,
      "loss": 0.7888,
      "step": 1490
    },
    {
      "epoch": 0.6421232876712328,
      "grad_norm": 9.704048156738281,
      "learning_rate": 2.6149828767123288e-05,
      "loss": 0.6014,
      "step": 1500
    },
    {
      "epoch": 0.646404109589041,
      "grad_norm": 18.230037689208984,
      "learning_rate": 2.6124143835616438e-05,
      "loss": 0.7973,
      "step": 1510
    },
    {
      "epoch": 0.6506849315068494,
      "grad_norm": 13.136951446533203,
      "learning_rate": 2.609845890410959e-05,
      "loss": 0.54,
      "step": 1520
    },
    {
      "epoch": 0.6549657534246576,
      "grad_norm": 37.58016586303711,
      "learning_rate": 2.607277397260274e-05,
      "loss": 0.9804,
      "step": 1530
    },
    {
      "epoch": 0.6592465753424658,
      "grad_norm": 11.665778160095215,
      "learning_rate": 2.604708904109589e-05,
      "loss": 0.7955,
      "step": 1540
    },
    {
      "epoch": 0.663527397260274,
      "grad_norm": 15.988519668579102,
      "learning_rate": 2.602140410958904e-05,
      "loss": 1.2159,
      "step": 1550
    },
    {
      "epoch": 0.6678082191780822,
      "grad_norm": 12.905730247497559,
      "learning_rate": 2.5995719178082193e-05,
      "loss": 0.9387,
      "step": 1560
    },
    {
      "epoch": 0.6720890410958904,
      "grad_norm": 5.530625343322754,
      "learning_rate": 2.5970034246575343e-05,
      "loss": 0.7292,
      "step": 1570
    },
    {
      "epoch": 0.6763698630136986,
      "grad_norm": 24.208515167236328,
      "learning_rate": 2.5944349315068496e-05,
      "loss": 0.7722,
      "step": 1580
    },
    {
      "epoch": 0.6806506849315068,
      "grad_norm": 14.226933479309082,
      "learning_rate": 2.5918664383561643e-05,
      "loss": 0.7271,
      "step": 1590
    },
    {
      "epoch": 0.684931506849315,
      "grad_norm": 15.042016983032227,
      "learning_rate": 2.5892979452054796e-05,
      "loss": 0.7905,
      "step": 1600
    },
    {
      "epoch": 0.6892123287671232,
      "grad_norm": 35.65140914916992,
      "learning_rate": 2.5867294520547946e-05,
      "loss": 0.5681,
      "step": 1610
    },
    {
      "epoch": 0.6934931506849316,
      "grad_norm": 10.235920906066895,
      "learning_rate": 2.58416095890411e-05,
      "loss": 0.8,
      "step": 1620
    },
    {
      "epoch": 0.6977739726027398,
      "grad_norm": 27.296335220336914,
      "learning_rate": 2.5815924657534245e-05,
      "loss": 0.9603,
      "step": 1630
    },
    {
      "epoch": 0.702054794520548,
      "grad_norm": 25.194801330566406,
      "learning_rate": 2.57902397260274e-05,
      "loss": 0.82,
      "step": 1640
    },
    {
      "epoch": 0.7063356164383562,
      "grad_norm": 36.33634567260742,
      "learning_rate": 2.5764554794520548e-05,
      "loss": 0.7717,
      "step": 1650
    },
    {
      "epoch": 0.7106164383561644,
      "grad_norm": 15.009870529174805,
      "learning_rate": 2.57388698630137e-05,
      "loss": 0.7878,
      "step": 1660
    },
    {
      "epoch": 0.7148972602739726,
      "grad_norm": 34.94595718383789,
      "learning_rate": 2.571318493150685e-05,
      "loss": 0.6897,
      "step": 1670
    },
    {
      "epoch": 0.7191780821917808,
      "grad_norm": 3.3885648250579834,
      "learning_rate": 2.56875e-05,
      "loss": 0.6894,
      "step": 1680
    },
    {
      "epoch": 0.723458904109589,
      "grad_norm": 18.04899024963379,
      "learning_rate": 2.566181506849315e-05,
      "loss": 0.6855,
      "step": 1690
    },
    {
      "epoch": 0.7277397260273972,
      "grad_norm": 23.512062072753906,
      "learning_rate": 2.56361301369863e-05,
      "loss": 0.8258,
      "step": 1700
    },
    {
      "epoch": 0.7320205479452054,
      "grad_norm": 13.760478973388672,
      "learning_rate": 2.5610445205479454e-05,
      "loss": 0.8224,
      "step": 1710
    },
    {
      "epoch": 0.7363013698630136,
      "grad_norm": 17.022178649902344,
      "learning_rate": 2.5584760273972603e-05,
      "loss": 0.8214,
      "step": 1720
    },
    {
      "epoch": 0.740582191780822,
      "grad_norm": 93.08991241455078,
      "learning_rate": 2.5559075342465753e-05,
      "loss": 0.6138,
      "step": 1730
    },
    {
      "epoch": 0.7448630136986302,
      "grad_norm": 12.598793983459473,
      "learning_rate": 2.5533390410958903e-05,
      "loss": 0.6085,
      "step": 1740
    },
    {
      "epoch": 0.7491438356164384,
      "grad_norm": 10.264090538024902,
      "learning_rate": 2.5507705479452056e-05,
      "loss": 0.4778,
      "step": 1750
    },
    {
      "epoch": 0.7534246575342466,
      "grad_norm": 16.948760986328125,
      "learning_rate": 2.5482020547945206e-05,
      "loss": 0.7543,
      "step": 1760
    },
    {
      "epoch": 0.7577054794520548,
      "grad_norm": 16.17952537536621,
      "learning_rate": 2.5456335616438356e-05,
      "loss": 0.676,
      "step": 1770
    },
    {
      "epoch": 0.761986301369863,
      "grad_norm": 16.751449584960938,
      "learning_rate": 2.5430650684931506e-05,
      "loss": 0.6894,
      "step": 1780
    },
    {
      "epoch": 0.7662671232876712,
      "grad_norm": 10.731999397277832,
      "learning_rate": 2.540496575342466e-05,
      "loss": 0.6445,
      "step": 1790
    },
    {
      "epoch": 0.7705479452054794,
      "grad_norm": 5.810342788696289,
      "learning_rate": 2.537928082191781e-05,
      "loss": 0.79,
      "step": 1800
    },
    {
      "epoch": 0.7748287671232876,
      "grad_norm": 23.382064819335938,
      "learning_rate": 2.535359589041096e-05,
      "loss": 0.9898,
      "step": 1810
    },
    {
      "epoch": 0.7791095890410958,
      "grad_norm": 10.761639595031738,
      "learning_rate": 2.5327910958904108e-05,
      "loss": 0.5624,
      "step": 1820
    },
    {
      "epoch": 0.7833904109589042,
      "grad_norm": 13.678400993347168,
      "learning_rate": 2.530222602739726e-05,
      "loss": 0.9474,
      "step": 1830
    },
    {
      "epoch": 0.7876712328767124,
      "grad_norm": 13.515304565429688,
      "learning_rate": 2.527654109589041e-05,
      "loss": 0.7889,
      "step": 1840
    },
    {
      "epoch": 0.7919520547945206,
      "grad_norm": 20.154905319213867,
      "learning_rate": 2.5250856164383564e-05,
      "loss": 0.8551,
      "step": 1850
    },
    {
      "epoch": 0.7962328767123288,
      "grad_norm": 16.845243453979492,
      "learning_rate": 2.522517123287671e-05,
      "loss": 0.7068,
      "step": 1860
    },
    {
      "epoch": 0.800513698630137,
      "grad_norm": 34.205780029296875,
      "learning_rate": 2.5199486301369864e-05,
      "loss": 0.805,
      "step": 1870
    },
    {
      "epoch": 0.8047945205479452,
      "grad_norm": 13.016587257385254,
      "learning_rate": 2.5173801369863014e-05,
      "loss": 0.6715,
      "step": 1880
    },
    {
      "epoch": 0.8090753424657534,
      "grad_norm": 17.59514617919922,
      "learning_rate": 2.5148116438356167e-05,
      "loss": 0.7308,
      "step": 1890
    },
    {
      "epoch": 0.8133561643835616,
      "grad_norm": 21.68423843383789,
      "learning_rate": 2.5122431506849317e-05,
      "loss": 0.9619,
      "step": 1900
    },
    {
      "epoch": 0.8176369863013698,
      "grad_norm": 10.85733413696289,
      "learning_rate": 2.5096746575342466e-05,
      "loss": 0.4836,
      "step": 1910
    },
    {
      "epoch": 0.821917808219178,
      "grad_norm": 17.5113468170166,
      "learning_rate": 2.5071061643835616e-05,
      "loss": 0.7225,
      "step": 1920
    },
    {
      "epoch": 0.8261986301369864,
      "grad_norm": 20.78669548034668,
      "learning_rate": 2.504537671232877e-05,
      "loss": 0.7372,
      "step": 1930
    },
    {
      "epoch": 0.8304794520547946,
      "grad_norm": 12.682047843933105,
      "learning_rate": 2.501969178082192e-05,
      "loss": 0.781,
      "step": 1940
    },
    {
      "epoch": 0.8347602739726028,
      "grad_norm": 18.531436920166016,
      "learning_rate": 2.499400684931507e-05,
      "loss": 0.8725,
      "step": 1950
    },
    {
      "epoch": 0.839041095890411,
      "grad_norm": 33.07869338989258,
      "learning_rate": 2.496832191780822e-05,
      "loss": 0.7145,
      "step": 1960
    },
    {
      "epoch": 0.8433219178082192,
      "grad_norm": 56.62294006347656,
      "learning_rate": 2.4942636986301372e-05,
      "loss": 0.5252,
      "step": 1970
    },
    {
      "epoch": 0.8476027397260274,
      "grad_norm": 64.34467315673828,
      "learning_rate": 2.491695205479452e-05,
      "loss": 0.8005,
      "step": 1980
    },
    {
      "epoch": 0.8518835616438356,
      "grad_norm": 12.431035041809082,
      "learning_rate": 2.4891267123287675e-05,
      "loss": 0.5862,
      "step": 1990
    },
    {
      "epoch": 0.8561643835616438,
      "grad_norm": 31.489168167114258,
      "learning_rate": 2.486558219178082e-05,
      "loss": 0.8181,
      "step": 2000
    },
    {
      "epoch": 0.860445205479452,
      "grad_norm": 20.041730880737305,
      "learning_rate": 2.4839897260273974e-05,
      "loss": 0.6091,
      "step": 2010
    },
    {
      "epoch": 0.8647260273972602,
      "grad_norm": 8.799732208251953,
      "learning_rate": 2.4814212328767124e-05,
      "loss": 0.5877,
      "step": 2020
    },
    {
      "epoch": 0.8690068493150684,
      "grad_norm": 13.108522415161133,
      "learning_rate": 2.4788527397260274e-05,
      "loss": 0.6862,
      "step": 2030
    },
    {
      "epoch": 0.8732876712328768,
      "grad_norm": 21.110349655151367,
      "learning_rate": 2.4762842465753427e-05,
      "loss": 0.6593,
      "step": 2040
    },
    {
      "epoch": 0.877568493150685,
      "grad_norm": 4.349354267120361,
      "learning_rate": 2.4737157534246573e-05,
      "loss": 0.68,
      "step": 2050
    },
    {
      "epoch": 0.8818493150684932,
      "grad_norm": 17.407764434814453,
      "learning_rate": 2.4711472602739727e-05,
      "loss": 0.7587,
      "step": 2060
    },
    {
      "epoch": 0.8861301369863014,
      "grad_norm": 10.936676025390625,
      "learning_rate": 2.4685787671232876e-05,
      "loss": 0.7355,
      "step": 2070
    },
    {
      "epoch": 0.8904109589041096,
      "grad_norm": 36.851112365722656,
      "learning_rate": 2.466010273972603e-05,
      "loss": 0.8058,
      "step": 2080
    },
    {
      "epoch": 0.8946917808219178,
      "grad_norm": 26.359825134277344,
      "learning_rate": 2.4634417808219176e-05,
      "loss": 0.8067,
      "step": 2090
    },
    {
      "epoch": 0.898972602739726,
      "grad_norm": 4.808035373687744,
      "learning_rate": 2.460873287671233e-05,
      "loss": 0.7556,
      "step": 2100
    },
    {
      "epoch": 0.9032534246575342,
      "grad_norm": 29.35092544555664,
      "learning_rate": 2.458304794520548e-05,
      "loss": 0.7164,
      "step": 2110
    },
    {
      "epoch": 0.9075342465753424,
      "grad_norm": 19.436264038085938,
      "learning_rate": 2.4557363013698632e-05,
      "loss": 0.6005,
      "step": 2120
    },
    {
      "epoch": 0.9118150684931506,
      "grad_norm": 12.285465240478516,
      "learning_rate": 2.4531678082191782e-05,
      "loss": 0.5932,
      "step": 2130
    },
    {
      "epoch": 0.916095890410959,
      "grad_norm": 25.76729965209961,
      "learning_rate": 2.450599315068493e-05,
      "loss": 0.6441,
      "step": 2140
    },
    {
      "epoch": 0.9203767123287672,
      "grad_norm": 27.039709091186523,
      "learning_rate": 2.448030821917808e-05,
      "loss": 0.8027,
      "step": 2150
    },
    {
      "epoch": 0.9246575342465754,
      "grad_norm": 28.40961265563965,
      "learning_rate": 2.4454623287671235e-05,
      "loss": 0.5723,
      "step": 2160
    },
    {
      "epoch": 0.9289383561643836,
      "grad_norm": 28.89515495300293,
      "learning_rate": 2.4428938356164384e-05,
      "loss": 0.726,
      "step": 2170
    },
    {
      "epoch": 0.9332191780821918,
      "grad_norm": 15.21774959564209,
      "learning_rate": 2.4403253424657534e-05,
      "loss": 0.5119,
      "step": 2180
    },
    {
      "epoch": 0.9375,
      "grad_norm": 31.885351181030273,
      "learning_rate": 2.4377568493150684e-05,
      "loss": 0.6956,
      "step": 2190
    },
    {
      "epoch": 0.9417808219178082,
      "grad_norm": 41.878997802734375,
      "learning_rate": 2.4351883561643837e-05,
      "loss": 0.4734,
      "step": 2200
    },
    {
      "epoch": 0.9460616438356164,
      "grad_norm": 12.264182090759277,
      "learning_rate": 2.4326198630136987e-05,
      "loss": 0.526,
      "step": 2210
    },
    {
      "epoch": 0.9503424657534246,
      "grad_norm": 6.523041248321533,
      "learning_rate": 2.430051369863014e-05,
      "loss": 0.7543,
      "step": 2220
    },
    {
      "epoch": 0.9546232876712328,
      "grad_norm": 19.857051849365234,
      "learning_rate": 2.4274828767123286e-05,
      "loss": 0.4575,
      "step": 2230
    },
    {
      "epoch": 0.958904109589041,
      "grad_norm": 41.966434478759766,
      "learning_rate": 2.424914383561644e-05,
      "loss": 0.71,
      "step": 2240
    },
    {
      "epoch": 0.9631849315068494,
      "grad_norm": 18.047285079956055,
      "learning_rate": 2.422345890410959e-05,
      "loss": 0.4611,
      "step": 2250
    },
    {
      "epoch": 0.9674657534246576,
      "grad_norm": 32.25387191772461,
      "learning_rate": 2.4197773972602743e-05,
      "loss": 0.6311,
      "step": 2260
    },
    {
      "epoch": 0.9717465753424658,
      "grad_norm": 7.983370780944824,
      "learning_rate": 2.417208904109589e-05,
      "loss": 0.6455,
      "step": 2270
    },
    {
      "epoch": 0.976027397260274,
      "grad_norm": 15.092449188232422,
      "learning_rate": 2.4146404109589042e-05,
      "loss": 0.5441,
      "step": 2280
    },
    {
      "epoch": 0.9803082191780822,
      "grad_norm": 4.433277130126953,
      "learning_rate": 2.4120719178082192e-05,
      "loss": 0.8646,
      "step": 2290
    },
    {
      "epoch": 0.9845890410958904,
      "grad_norm": 2.8443455696105957,
      "learning_rate": 2.4095034246575345e-05,
      "loss": 0.653,
      "step": 2300
    },
    {
      "epoch": 0.9888698630136986,
      "grad_norm": 0.9373922944068909,
      "learning_rate": 2.4069349315068495e-05,
      "loss": 0.5245,
      "step": 2310
    },
    {
      "epoch": 0.9931506849315068,
      "grad_norm": 54.9177131652832,
      "learning_rate": 2.4043664383561645e-05,
      "loss": 0.5716,
      "step": 2320
    },
    {
      "epoch": 0.997431506849315,
      "grad_norm": 17.051746368408203,
      "learning_rate": 2.4017979452054794e-05,
      "loss": 0.5881,
      "step": 2330
    },
    {
      "epoch": 1.0,
      "eval_f1": 0.640155605090126,
      "eval_loss": 0.7885168194770813,
      "eval_precision": 0.65989952624744,
      "eval_recall": 0.6545266692496449,
      "eval_runtime": 596.1184,
      "eval_samples_per_second": 7.499,
      "eval_steps_per_second": 0.938,
      "step": 2336
    },
    {
      "epoch": 1.0017123287671232,
      "grad_norm": 14.434311866760254,
      "learning_rate": 2.3992294520547948e-05,
      "loss": 0.5605,
      "step": 2340
    },
    {
      "epoch": 1.0059931506849316,
      "grad_norm": 1.0917059183120728,
      "learning_rate": 2.3966609589041097e-05,
      "loss": 0.7174,
      "step": 2350
    },
    {
      "epoch": 1.0102739726027397,
      "grad_norm": 10.919930458068848,
      "learning_rate": 2.3940924657534247e-05,
      "loss": 0.5105,
      "step": 2360
    },
    {
      "epoch": 1.014554794520548,
      "grad_norm": 28.403804779052734,
      "learning_rate": 2.3915239726027397e-05,
      "loss": 0.6549,
      "step": 2370
    },
    {
      "epoch": 1.018835616438356,
      "grad_norm": 26.01487922668457,
      "learning_rate": 2.3889554794520547e-05,
      "loss": 0.5837,
      "step": 2380
    },
    {
      "epoch": 1.0231164383561644,
      "grad_norm": 8.452924728393555,
      "learning_rate": 2.38638698630137e-05,
      "loss": 0.4745,
      "step": 2390
    },
    {
      "epoch": 1.0273972602739727,
      "grad_norm": 20.746458053588867,
      "learning_rate": 2.383818493150685e-05,
      "loss": 0.4122,
      "step": 2400
    },
    {
      "epoch": 1.0316780821917808,
      "grad_norm": 25.048707962036133,
      "learning_rate": 2.38125e-05,
      "loss": 0.8255,
      "step": 2410
    },
    {
      "epoch": 1.0359589041095891,
      "grad_norm": 6.413450717926025,
      "learning_rate": 2.378681506849315e-05,
      "loss": 0.5403,
      "step": 2420
    },
    {
      "epoch": 1.0402397260273972,
      "grad_norm": 11.969742774963379,
      "learning_rate": 2.3761130136986302e-05,
      "loss": 0.6837,
      "step": 2430
    },
    {
      "epoch": 1.0445205479452055,
      "grad_norm": 12.856775283813477,
      "learning_rate": 2.3735445205479452e-05,
      "loss": 0.508,
      "step": 2440
    },
    {
      "epoch": 1.0488013698630136,
      "grad_norm": 98.75533294677734,
      "learning_rate": 2.3709760273972605e-05,
      "loss": 0.5482,
      "step": 2450
    },
    {
      "epoch": 1.053082191780822,
      "grad_norm": 14.54837417602539,
      "learning_rate": 2.3684075342465752e-05,
      "loss": 0.543,
      "step": 2460
    },
    {
      "epoch": 1.05736301369863,
      "grad_norm": 13.33887004852295,
      "learning_rate": 2.3658390410958905e-05,
      "loss": 0.5926,
      "step": 2470
    },
    {
      "epoch": 1.0616438356164384,
      "grad_norm": 8.95486831665039,
      "learning_rate": 2.3632705479452055e-05,
      "loss": 0.5339,
      "step": 2480
    },
    {
      "epoch": 1.0659246575342465,
      "grad_norm": 14.323339462280273,
      "learning_rate": 2.3607020547945208e-05,
      "loss": 0.4593,
      "step": 2490
    },
    {
      "epoch": 1.0702054794520548,
      "grad_norm": 15.935379028320312,
      "learning_rate": 2.3581335616438354e-05,
      "loss": 0.5269,
      "step": 2500
    },
    {
      "epoch": 1.0744863013698631,
      "grad_norm": 10.73087215423584,
      "learning_rate": 2.3555650684931507e-05,
      "loss": 0.6115,
      "step": 2510
    },
    {
      "epoch": 1.0787671232876712,
      "grad_norm": 5.429958343505859,
      "learning_rate": 2.3529965753424657e-05,
      "loss": 0.3729,
      "step": 2520
    },
    {
      "epoch": 1.0830479452054795,
      "grad_norm": 21.095849990844727,
      "learning_rate": 2.350428082191781e-05,
      "loss": 0.4417,
      "step": 2530
    },
    {
      "epoch": 1.0873287671232876,
      "grad_norm": 11.610197067260742,
      "learning_rate": 2.347859589041096e-05,
      "loss": 0.7186,
      "step": 2540
    },
    {
      "epoch": 1.091609589041096,
      "grad_norm": 7.565066337585449,
      "learning_rate": 2.345291095890411e-05,
      "loss": 0.5585,
      "step": 2550
    },
    {
      "epoch": 1.095890410958904,
      "grad_norm": 8.311152458190918,
      "learning_rate": 2.342722602739726e-05,
      "loss": 0.6581,
      "step": 2560
    },
    {
      "epoch": 1.1001712328767124,
      "grad_norm": 9.93104076385498,
      "learning_rate": 2.3401541095890413e-05,
      "loss": 0.4014,
      "step": 2570
    },
    {
      "epoch": 1.1044520547945205,
      "grad_norm": 12.65024185180664,
      "learning_rate": 2.3375856164383563e-05,
      "loss": 0.5235,
      "step": 2580
    },
    {
      "epoch": 1.1087328767123288,
      "grad_norm": 60.201515197753906,
      "learning_rate": 2.3350171232876712e-05,
      "loss": 0.91,
      "step": 2590
    },
    {
      "epoch": 1.1130136986301369,
      "grad_norm": 9.550028800964355,
      "learning_rate": 2.3324486301369862e-05,
      "loss": 0.4735,
      "step": 2600
    },
    {
      "epoch": 1.1172945205479452,
      "grad_norm": 9.562759399414062,
      "learning_rate": 2.3298801369863015e-05,
      "loss": 0.4877,
      "step": 2610
    },
    {
      "epoch": 1.1215753424657535,
      "grad_norm": 12.886402130126953,
      "learning_rate": 2.3273116438356165e-05,
      "loss": 0.4259,
      "step": 2620
    },
    {
      "epoch": 1.1258561643835616,
      "grad_norm": 7.7619709968566895,
      "learning_rate": 2.324743150684932e-05,
      "loss": 0.502,
      "step": 2630
    },
    {
      "epoch": 1.13013698630137,
      "grad_norm": 22.827817916870117,
      "learning_rate": 2.3221746575342465e-05,
      "loss": 0.5342,
      "step": 2640
    },
    {
      "epoch": 1.134417808219178,
      "grad_norm": 26.439483642578125,
      "learning_rate": 2.3196061643835618e-05,
      "loss": 0.4635,
      "step": 2650
    },
    {
      "epoch": 1.1386986301369864,
      "grad_norm": 27.487112045288086,
      "learning_rate": 2.3170376712328768e-05,
      "loss": 0.4494,
      "step": 2660
    },
    {
      "epoch": 1.1429794520547945,
      "grad_norm": 8.665365219116211,
      "learning_rate": 2.314469178082192e-05,
      "loss": 0.5481,
      "step": 2670
    },
    {
      "epoch": 1.1472602739726028,
      "grad_norm": 35.2126579284668,
      "learning_rate": 2.3119006849315067e-05,
      "loss": 0.7088,
      "step": 2680
    },
    {
      "epoch": 1.1515410958904109,
      "grad_norm": 19.38114356994629,
      "learning_rate": 2.309332191780822e-05,
      "loss": 0.5704,
      "step": 2690
    },
    {
      "epoch": 1.1558219178082192,
      "grad_norm": 0.7317081093788147,
      "learning_rate": 2.306763698630137e-05,
      "loss": 0.4253,
      "step": 2700
    },
    {
      "epoch": 1.1601027397260273,
      "grad_norm": 8.237403869628906,
      "learning_rate": 2.3041952054794523e-05,
      "loss": 0.4897,
      "step": 2710
    },
    {
      "epoch": 1.1643835616438356,
      "grad_norm": 14.927027702331543,
      "learning_rate": 2.3016267123287673e-05,
      "loss": 0.6706,
      "step": 2720
    },
    {
      "epoch": 1.168664383561644,
      "grad_norm": 28.70943832397461,
      "learning_rate": 2.299058219178082e-05,
      "loss": 0.5062,
      "step": 2730
    },
    {
      "epoch": 1.172945205479452,
      "grad_norm": 20.617576599121094,
      "learning_rate": 2.2964897260273973e-05,
      "loss": 0.5652,
      "step": 2740
    },
    {
      "epoch": 1.1772260273972603,
      "grad_norm": 37.53913879394531,
      "learning_rate": 2.2939212328767123e-05,
      "loss": 0.682,
      "step": 2750
    },
    {
      "epoch": 1.1815068493150684,
      "grad_norm": 125.64456176757812,
      "learning_rate": 2.2913527397260276e-05,
      "loss": 0.6095,
      "step": 2760
    },
    {
      "epoch": 1.1857876712328768,
      "grad_norm": 0.6167072057723999,
      "learning_rate": 2.2887842465753425e-05,
      "loss": 0.3489,
      "step": 2770
    },
    {
      "epoch": 1.1900684931506849,
      "grad_norm": 8.875263214111328,
      "learning_rate": 2.2862157534246575e-05,
      "loss": 0.5724,
      "step": 2780
    },
    {
      "epoch": 1.1943493150684932,
      "grad_norm": 29.087247848510742,
      "learning_rate": 2.2836472602739725e-05,
      "loss": 0.5674,
      "step": 2790
    },
    {
      "epoch": 1.1986301369863013,
      "grad_norm": 108.30213165283203,
      "learning_rate": 2.2810787671232878e-05,
      "loss": 0.4056,
      "step": 2800
    },
    {
      "epoch": 1.2029109589041096,
      "grad_norm": 13.343461990356445,
      "learning_rate": 2.2785102739726028e-05,
      "loss": 0.4987,
      "step": 2810
    },
    {
      "epoch": 1.2071917808219177,
      "grad_norm": 0.6047355532646179,
      "learning_rate": 2.2759417808219178e-05,
      "loss": 0.2861,
      "step": 2820
    },
    {
      "epoch": 1.211472602739726,
      "grad_norm": 12.467401504516602,
      "learning_rate": 2.2733732876712328e-05,
      "loss": 0.7408,
      "step": 2830
    },
    {
      "epoch": 1.2157534246575343,
      "grad_norm": 15.868618965148926,
      "learning_rate": 2.270804794520548e-05,
      "loss": 0.4814,
      "step": 2840
    },
    {
      "epoch": 1.2200342465753424,
      "grad_norm": 24.539642333984375,
      "learning_rate": 2.268236301369863e-05,
      "loss": 0.6094,
      "step": 2850
    },
    {
      "epoch": 1.2243150684931507,
      "grad_norm": 15.609148979187012,
      "learning_rate": 2.2656678082191784e-05,
      "loss": 0.6397,
      "step": 2860
    },
    {
      "epoch": 1.2285958904109588,
      "grad_norm": 15.778641700744629,
      "learning_rate": 2.263099315068493e-05,
      "loss": 0.5055,
      "step": 2870
    },
    {
      "epoch": 1.2328767123287672,
      "grad_norm": 20.7979736328125,
      "learning_rate": 2.2605308219178083e-05,
      "loss": 0.3678,
      "step": 2880
    },
    {
      "epoch": 1.2371575342465753,
      "grad_norm": 39.31053924560547,
      "learning_rate": 2.2579623287671233e-05,
      "loss": 0.3605,
      "step": 2890
    },
    {
      "epoch": 1.2414383561643836,
      "grad_norm": 1.9259212017059326,
      "learning_rate": 2.2553938356164386e-05,
      "loss": 0.4457,
      "step": 2900
    },
    {
      "epoch": 1.245719178082192,
      "grad_norm": 39.82622146606445,
      "learning_rate": 2.2528253424657533e-05,
      "loss": 0.5663,
      "step": 2910
    },
    {
      "epoch": 1.25,
      "grad_norm": 18.27713394165039,
      "learning_rate": 2.2502568493150686e-05,
      "loss": 0.559,
      "step": 2920
    },
    {
      "epoch": 1.254280821917808,
      "grad_norm": 19.46271514892578,
      "learning_rate": 2.2476883561643836e-05,
      "loss": 0.5596,
      "step": 2930
    },
    {
      "epoch": 1.2585616438356164,
      "grad_norm": 15.22482681274414,
      "learning_rate": 2.245119863013699e-05,
      "loss": 0.5677,
      "step": 2940
    },
    {
      "epoch": 1.2628424657534247,
      "grad_norm": 6.7652812004089355,
      "learning_rate": 2.242551369863014e-05,
      "loss": 0.6691,
      "step": 2950
    },
    {
      "epoch": 1.2671232876712328,
      "grad_norm": 15.198921203613281,
      "learning_rate": 2.2399828767123288e-05,
      "loss": 0.4918,
      "step": 2960
    },
    {
      "epoch": 1.2714041095890412,
      "grad_norm": 103.82099151611328,
      "learning_rate": 2.2374143835616438e-05,
      "loss": 0.5306,
      "step": 2970
    },
    {
      "epoch": 1.2756849315068493,
      "grad_norm": 4.987117290496826,
      "learning_rate": 2.234845890410959e-05,
      "loss": 0.5766,
      "step": 2980
    },
    {
      "epoch": 1.2799657534246576,
      "grad_norm": 7.564026355743408,
      "learning_rate": 2.232277397260274e-05,
      "loss": 0.5242,
      "step": 2990
    },
    {
      "epoch": 1.2842465753424657,
      "grad_norm": 3.2099010944366455,
      "learning_rate": 2.229708904109589e-05,
      "loss": 0.6231,
      "step": 3000
    },
    {
      "epoch": 1.288527397260274,
      "grad_norm": 31.249996185302734,
      "learning_rate": 2.227140410958904e-05,
      "loss": 0.582,
      "step": 3010
    },
    {
      "epoch": 1.2928082191780823,
      "grad_norm": 24.8253116607666,
      "learning_rate": 2.2245719178082194e-05,
      "loss": 0.7384,
      "step": 3020
    },
    {
      "epoch": 1.2970890410958904,
      "grad_norm": 9.528766632080078,
      "learning_rate": 2.2220034246575344e-05,
      "loss": 0.3158,
      "step": 3030
    },
    {
      "epoch": 1.3013698630136985,
      "grad_norm": 20.124649047851562,
      "learning_rate": 2.2194349315068497e-05,
      "loss": 0.725,
      "step": 3040
    },
    {
      "epoch": 1.3056506849315068,
      "grad_norm": 6.461442470550537,
      "learning_rate": 2.2168664383561643e-05,
      "loss": 0.523,
      "step": 3050
    },
    {
      "epoch": 1.3099315068493151,
      "grad_norm": 18.374300003051758,
      "learning_rate": 2.2142979452054796e-05,
      "loss": 0.4177,
      "step": 3060
    },
    {
      "epoch": 1.3142123287671232,
      "grad_norm": 15.605560302734375,
      "learning_rate": 2.2117294520547946e-05,
      "loss": 0.6334,
      "step": 3070
    },
    {
      "epoch": 1.3184931506849316,
      "grad_norm": 40.33599090576172,
      "learning_rate": 2.2091609589041096e-05,
      "loss": 0.5582,
      "step": 3080
    },
    {
      "epoch": 1.3227739726027397,
      "grad_norm": 18.125089645385742,
      "learning_rate": 2.2065924657534246e-05,
      "loss": 0.5148,
      "step": 3090
    },
    {
      "epoch": 1.327054794520548,
      "grad_norm": 33.55887222290039,
      "learning_rate": 2.2040239726027395e-05,
      "loss": 0.4119,
      "step": 3100
    },
    {
      "epoch": 1.331335616438356,
      "grad_norm": 27.477209091186523,
      "learning_rate": 2.201455479452055e-05,
      "loss": 0.7335,
      "step": 3110
    },
    {
      "epoch": 1.3356164383561644,
      "grad_norm": 28.373653411865234,
      "learning_rate": 2.19888698630137e-05,
      "loss": 0.4652,
      "step": 3120
    },
    {
      "epoch": 1.3398972602739727,
      "grad_norm": 10.41767692565918,
      "learning_rate": 2.196318493150685e-05,
      "loss": 0.4556,
      "step": 3130
    },
    {
      "epoch": 1.3441780821917808,
      "grad_norm": 15.985031127929688,
      "learning_rate": 2.1937499999999998e-05,
      "loss": 0.5208,
      "step": 3140
    },
    {
      "epoch": 1.348458904109589,
      "grad_norm": 9.439993858337402,
      "learning_rate": 2.191181506849315e-05,
      "loss": 0.4854,
      "step": 3150
    },
    {
      "epoch": 1.3527397260273972,
      "grad_norm": 16.428089141845703,
      "learning_rate": 2.18861301369863e-05,
      "loss": 0.5496,
      "step": 3160
    },
    {
      "epoch": 1.3570205479452055,
      "grad_norm": 13.383630752563477,
      "learning_rate": 2.1860445205479454e-05,
      "loss": 0.4427,
      "step": 3170
    },
    {
      "epoch": 1.3613013698630136,
      "grad_norm": 21.770055770874023,
      "learning_rate": 2.1834760273972604e-05,
      "loss": 0.3362,
      "step": 3180
    },
    {
      "epoch": 1.365582191780822,
      "grad_norm": 2.661846876144409,
      "learning_rate": 2.1809075342465754e-05,
      "loss": 0.4305,
      "step": 3190
    },
    {
      "epoch": 1.36986301369863,
      "grad_norm": 9.57947826385498,
      "learning_rate": 2.1783390410958903e-05,
      "loss": 0.6307,
      "step": 3200
    },
    {
      "epoch": 1.3741438356164384,
      "grad_norm": 7.182124137878418,
      "learning_rate": 2.1757705479452057e-05,
      "loss": 0.7003,
      "step": 3210
    },
    {
      "epoch": 1.3784246575342465,
      "grad_norm": 12.542230606079102,
      "learning_rate": 2.1732020547945206e-05,
      "loss": 0.7924,
      "step": 3220
    },
    {
      "epoch": 1.3827054794520548,
      "grad_norm": 11.852202415466309,
      "learning_rate": 2.1706335616438356e-05,
      "loss": 0.5024,
      "step": 3230
    },
    {
      "epoch": 1.3869863013698631,
      "grad_norm": 22.10004997253418,
      "learning_rate": 2.1680650684931506e-05,
      "loss": 0.5869,
      "step": 3240
    },
    {
      "epoch": 1.3912671232876712,
      "grad_norm": 9.61673641204834,
      "learning_rate": 2.165496575342466e-05,
      "loss": 0.5511,
      "step": 3250
    },
    {
      "epoch": 1.3955479452054795,
      "grad_norm": 3.5772383213043213,
      "learning_rate": 2.162928082191781e-05,
      "loss": 0.5378,
      "step": 3260
    },
    {
      "epoch": 1.3998287671232876,
      "grad_norm": 14.454607009887695,
      "learning_rate": 2.1603595890410962e-05,
      "loss": 0.7021,
      "step": 3270
    },
    {
      "epoch": 1.404109589041096,
      "grad_norm": 12.301724433898926,
      "learning_rate": 2.157791095890411e-05,
      "loss": 0.7471,
      "step": 3280
    },
    {
      "epoch": 1.408390410958904,
      "grad_norm": 4.101442813873291,
      "learning_rate": 2.155222602739726e-05,
      "loss": 0.355,
      "step": 3290
    },
    {
      "epoch": 1.4126712328767124,
      "grad_norm": 39.615867614746094,
      "learning_rate": 2.152654109589041e-05,
      "loss": 0.6048,
      "step": 3300
    },
    {
      "epoch": 1.4169520547945205,
      "grad_norm": 24.081043243408203,
      "learning_rate": 2.1500856164383565e-05,
      "loss": 0.7089,
      "step": 3310
    },
    {
      "epoch": 1.4212328767123288,
      "grad_norm": 6.137239933013916,
      "learning_rate": 2.147517123287671e-05,
      "loss": 0.4565,
      "step": 3320
    },
    {
      "epoch": 1.4255136986301369,
      "grad_norm": 11.220376014709473,
      "learning_rate": 2.1449486301369864e-05,
      "loss": 0.4277,
      "step": 3330
    },
    {
      "epoch": 1.4297945205479452,
      "grad_norm": 49.353233337402344,
      "learning_rate": 2.1423801369863014e-05,
      "loss": 0.3421,
      "step": 3340
    },
    {
      "epoch": 1.4340753424657535,
      "grad_norm": 21.286075592041016,
      "learning_rate": 2.1398116438356167e-05,
      "loss": 0.4536,
      "step": 3350
    },
    {
      "epoch": 1.4383561643835616,
      "grad_norm": 10.80482006072998,
      "learning_rate": 2.1372431506849317e-05,
      "loss": 0.6355,
      "step": 3360
    },
    {
      "epoch": 1.44263698630137,
      "grad_norm": 22.60628318786621,
      "learning_rate": 2.1346746575342467e-05,
      "loss": 0.5357,
      "step": 3370
    },
    {
      "epoch": 1.446917808219178,
      "grad_norm": 7.038317680358887,
      "learning_rate": 2.1321061643835616e-05,
      "loss": 0.5848,
      "step": 3380
    },
    {
      "epoch": 1.4511986301369864,
      "grad_norm": 3.6277172565460205,
      "learning_rate": 2.129537671232877e-05,
      "loss": 0.3811,
      "step": 3390
    },
    {
      "epoch": 1.4554794520547945,
      "grad_norm": 141.23995971679688,
      "learning_rate": 2.126969178082192e-05,
      "loss": 0.3749,
      "step": 3400
    },
    {
      "epoch": 1.4597602739726028,
      "grad_norm": 9.505330085754395,
      "learning_rate": 2.1244006849315066e-05,
      "loss": 0.5169,
      "step": 3410
    },
    {
      "epoch": 1.464041095890411,
      "grad_norm": 31.260398864746094,
      "learning_rate": 2.121832191780822e-05,
      "loss": 0.6798,
      "step": 3420
    },
    {
      "epoch": 1.4683219178082192,
      "grad_norm": 14.576324462890625,
      "learning_rate": 2.119263698630137e-05,
      "loss": 0.3887,
      "step": 3430
    },
    {
      "epoch": 1.4726027397260273,
      "grad_norm": 17.221073150634766,
      "learning_rate": 2.1166952054794522e-05,
      "loss": 0.3907,
      "step": 3440
    },
    {
      "epoch": 1.4768835616438356,
      "grad_norm": 0.8417624235153198,
      "learning_rate": 2.114126712328767e-05,
      "loss": 0.3861,
      "step": 3450
    },
    {
      "epoch": 1.481164383561644,
      "grad_norm": 1.0525720119476318,
      "learning_rate": 2.111558219178082e-05,
      "loss": 0.5549,
      "step": 3460
    },
    {
      "epoch": 1.485445205479452,
      "grad_norm": 8.386690139770508,
      "learning_rate": 2.108989726027397e-05,
      "loss": 0.4114,
      "step": 3470
    },
    {
      "epoch": 1.4897260273972603,
      "grad_norm": 4.5233283042907715,
      "learning_rate": 2.1064212328767124e-05,
      "loss": 0.4464,
      "step": 3480
    },
    {
      "epoch": 1.4940068493150684,
      "grad_norm": 59.0789794921875,
      "learning_rate": 2.1038527397260274e-05,
      "loss": 0.682,
      "step": 3490
    },
    {
      "epoch": 1.4982876712328768,
      "grad_norm": 23.49764633178711,
      "learning_rate": 2.1012842465753427e-05,
      "loss": 0.4015,
      "step": 3500
    },
    {
      "epoch": 1.5025684931506849,
      "grad_norm": 26.084774017333984,
      "learning_rate": 2.0987157534246574e-05,
      "loss": 0.7139,
      "step": 3510
    },
    {
      "epoch": 1.5068493150684932,
      "grad_norm": 7.794407367706299,
      "learning_rate": 2.0961472602739727e-05,
      "loss": 0.4606,
      "step": 3520
    },
    {
      "epoch": 1.5111301369863015,
      "grad_norm": 11.288233757019043,
      "learning_rate": 2.0935787671232877e-05,
      "loss": 0.451,
      "step": 3530
    },
    {
      "epoch": 1.5154109589041096,
      "grad_norm": 2.306042194366455,
      "learning_rate": 2.091010273972603e-05,
      "loss": 0.3553,
      "step": 3540
    },
    {
      "epoch": 1.5196917808219177,
      "grad_norm": 11.157553672790527,
      "learning_rate": 2.0884417808219176e-05,
      "loss": 0.6895,
      "step": 3550
    },
    {
      "epoch": 1.523972602739726,
      "grad_norm": 11.598366737365723,
      "learning_rate": 2.085873287671233e-05,
      "loss": 0.5611,
      "step": 3560
    },
    {
      "epoch": 1.5282534246575343,
      "grad_norm": 16.561546325683594,
      "learning_rate": 2.083304794520548e-05,
      "loss": 0.5075,
      "step": 3570
    },
    {
      "epoch": 1.5325342465753424,
      "grad_norm": 0.8690369129180908,
      "learning_rate": 2.0807363013698632e-05,
      "loss": 0.6442,
      "step": 3580
    },
    {
      "epoch": 1.5368150684931505,
      "grad_norm": 19.73202896118164,
      "learning_rate": 2.0781678082191782e-05,
      "loss": 0.5141,
      "step": 3590
    },
    {
      "epoch": 1.541095890410959,
      "grad_norm": 14.27385425567627,
      "learning_rate": 2.0755993150684932e-05,
      "loss": 0.4149,
      "step": 3600
    },
    {
      "epoch": 1.5453767123287672,
      "grad_norm": 2.2302627563476562,
      "learning_rate": 2.0730308219178082e-05,
      "loss": 0.758,
      "step": 3610
    },
    {
      "epoch": 1.5496575342465753,
      "grad_norm": 4.972026824951172,
      "learning_rate": 2.0704623287671235e-05,
      "loss": 0.4684,
      "step": 3620
    },
    {
      "epoch": 1.5539383561643836,
      "grad_norm": 4.984468460083008,
      "learning_rate": 2.0678938356164385e-05,
      "loss": 0.4058,
      "step": 3630
    },
    {
      "epoch": 1.558219178082192,
      "grad_norm": 58.44465255737305,
      "learning_rate": 2.0653253424657534e-05,
      "loss": 0.6543,
      "step": 3640
    },
    {
      "epoch": 1.5625,
      "grad_norm": 10.453229904174805,
      "learning_rate": 2.0627568493150684e-05,
      "loss": 0.6567,
      "step": 3650
    },
    {
      "epoch": 1.566780821917808,
      "grad_norm": 32.872833251953125,
      "learning_rate": 2.0601883561643837e-05,
      "loss": 0.5757,
      "step": 3660
    },
    {
      "epoch": 1.5710616438356164,
      "grad_norm": 26.98045539855957,
      "learning_rate": 2.0576198630136987e-05,
      "loss": 0.6303,
      "step": 3670
    },
    {
      "epoch": 1.5753424657534247,
      "grad_norm": 19.083444595336914,
      "learning_rate": 2.055051369863014e-05,
      "loss": 0.4122,
      "step": 3680
    },
    {
      "epoch": 1.5796232876712328,
      "grad_norm": 23.704484939575195,
      "learning_rate": 2.0524828767123287e-05,
      "loss": 0.8762,
      "step": 3690
    },
    {
      "epoch": 1.583904109589041,
      "grad_norm": 10.016107559204102,
      "learning_rate": 2.049914383561644e-05,
      "loss": 0.6037,
      "step": 3700
    },
    {
      "epoch": 1.5881849315068495,
      "grad_norm": 11.916234016418457,
      "learning_rate": 2.047345890410959e-05,
      "loss": 0.4509,
      "step": 3710
    },
    {
      "epoch": 1.5924657534246576,
      "grad_norm": 12.279425621032715,
      "learning_rate": 2.0447773972602743e-05,
      "loss": 0.477,
      "step": 3720
    },
    {
      "epoch": 1.5967465753424657,
      "grad_norm": 41.168392181396484,
      "learning_rate": 2.042208904109589e-05,
      "loss": 0.3373,
      "step": 3730
    },
    {
      "epoch": 1.601027397260274,
      "grad_norm": 13.21113395690918,
      "learning_rate": 2.0396404109589042e-05,
      "loss": 0.6568,
      "step": 3740
    },
    {
      "epoch": 1.6053082191780823,
      "grad_norm": 55.25428009033203,
      "learning_rate": 2.0370719178082192e-05,
      "loss": 0.5181,
      "step": 3750
    },
    {
      "epoch": 1.6095890410958904,
      "grad_norm": 4.288461208343506,
      "learning_rate": 2.0345034246575342e-05,
      "loss": 0.4256,
      "step": 3760
    },
    {
      "epoch": 1.6138698630136985,
      "grad_norm": 9.131546020507812,
      "learning_rate": 2.0319349315068495e-05,
      "loss": 0.4068,
      "step": 3770
    },
    {
      "epoch": 1.6181506849315068,
      "grad_norm": 14.117582321166992,
      "learning_rate": 2.029366438356164e-05,
      "loss": 0.5419,
      "step": 3780
    },
    {
      "epoch": 1.6224315068493151,
      "grad_norm": 12.835063934326172,
      "learning_rate": 2.0267979452054795e-05,
      "loss": 0.3837,
      "step": 3790
    },
    {
      "epoch": 1.6267123287671232,
      "grad_norm": 3.802135944366455,
      "learning_rate": 2.0242294520547945e-05,
      "loss": 0.453,
      "step": 3800
    },
    {
      "epoch": 1.6309931506849316,
      "grad_norm": 21.60682487487793,
      "learning_rate": 2.0216609589041098e-05,
      "loss": 0.4163,
      "step": 3810
    },
    {
      "epoch": 1.6352739726027399,
      "grad_norm": 8.55812931060791,
      "learning_rate": 2.0190924657534244e-05,
      "loss": 0.6263,
      "step": 3820
    },
    {
      "epoch": 1.639554794520548,
      "grad_norm": 12.189432144165039,
      "learning_rate": 2.0165239726027397e-05,
      "loss": 0.5857,
      "step": 3830
    },
    {
      "epoch": 1.643835616438356,
      "grad_norm": 21.914569854736328,
      "learning_rate": 2.0139554794520547e-05,
      "loss": 0.6283,
      "step": 3840
    },
    {
      "epoch": 1.6481164383561644,
      "grad_norm": 6.071353912353516,
      "learning_rate": 2.01138698630137e-05,
      "loss": 0.5488,
      "step": 3850
    },
    {
      "epoch": 1.6523972602739727,
      "grad_norm": 2.474181652069092,
      "learning_rate": 2.008818493150685e-05,
      "loss": 0.7891,
      "step": 3860
    },
    {
      "epoch": 1.6566780821917808,
      "grad_norm": 5.496734142303467,
      "learning_rate": 2.00625e-05,
      "loss": 0.4428,
      "step": 3870
    },
    {
      "epoch": 1.660958904109589,
      "grad_norm": 5.611483573913574,
      "learning_rate": 2.003681506849315e-05,
      "loss": 0.5238,
      "step": 3880
    },
    {
      "epoch": 1.6652397260273972,
      "grad_norm": 18.853336334228516,
      "learning_rate": 2.0011130136986303e-05,
      "loss": 0.5035,
      "step": 3890
    },
    {
      "epoch": 1.6695205479452055,
      "grad_norm": 12.679913520812988,
      "learning_rate": 1.9985445205479453e-05,
      "loss": 0.4014,
      "step": 3900
    },
    {
      "epoch": 1.6738013698630136,
      "grad_norm": 7.984349250793457,
      "learning_rate": 1.9959760273972606e-05,
      "loss": 0.2865,
      "step": 3910
    },
    {
      "epoch": 1.678082191780822,
      "grad_norm": 27.35059356689453,
      "learning_rate": 1.9934075342465752e-05,
      "loss": 0.6572,
      "step": 3920
    },
    {
      "epoch": 1.6823630136986303,
      "grad_norm": 17.193315505981445,
      "learning_rate": 1.9908390410958905e-05,
      "loss": 0.4457,
      "step": 3930
    },
    {
      "epoch": 1.6866438356164384,
      "grad_norm": 17.416433334350586,
      "learning_rate": 1.9882705479452055e-05,
      "loss": 0.7941,
      "step": 3940
    },
    {
      "epoch": 1.6909246575342465,
      "grad_norm": 16.681852340698242,
      "learning_rate": 1.9857020547945208e-05,
      "loss": 0.412,
      "step": 3950
    },
    {
      "epoch": 1.6952054794520548,
      "grad_norm": 3.40037202835083,
      "learning_rate": 1.9831335616438355e-05,
      "loss": 0.4159,
      "step": 3960
    },
    {
      "epoch": 1.6994863013698631,
      "grad_norm": 5.397609233856201,
      "learning_rate": 1.9805650684931508e-05,
      "loss": 0.4583,
      "step": 3970
    },
    {
      "epoch": 1.7037671232876712,
      "grad_norm": 28.7585391998291,
      "learning_rate": 1.9779965753424658e-05,
      "loss": 0.6251,
      "step": 3980
    },
    {
      "epoch": 1.7080479452054793,
      "grad_norm": 16.76787567138672,
      "learning_rate": 1.975428082191781e-05,
      "loss": 0.4039,
      "step": 3990
    },
    {
      "epoch": 1.7123287671232876,
      "grad_norm": 18.519248962402344,
      "learning_rate": 1.972859589041096e-05,
      "loss": 0.5611,
      "step": 4000
    },
    {
      "epoch": 1.716609589041096,
      "grad_norm": 3.6796257495880127,
      "learning_rate": 1.970291095890411e-05,
      "loss": 0.61,
      "step": 4010
    },
    {
      "epoch": 1.720890410958904,
      "grad_norm": 2.5599374771118164,
      "learning_rate": 1.967722602739726e-05,
      "loss": 0.51,
      "step": 4020
    },
    {
      "epoch": 1.7251712328767124,
      "grad_norm": 4.009096622467041,
      "learning_rate": 1.9651541095890413e-05,
      "loss": 0.5236,
      "step": 4030
    },
    {
      "epoch": 1.7294520547945207,
      "grad_norm": 61.896385192871094,
      "learning_rate": 1.9625856164383563e-05,
      "loss": 0.4505,
      "step": 4040
    },
    {
      "epoch": 1.7337328767123288,
      "grad_norm": 1.6276532411575317,
      "learning_rate": 1.9600171232876713e-05,
      "loss": 0.3222,
      "step": 4050
    },
    {
      "epoch": 1.7380136986301369,
      "grad_norm": 23.2451114654541,
      "learning_rate": 1.9574486301369863e-05,
      "loss": 0.5861,
      "step": 4060
    },
    {
      "epoch": 1.7422945205479452,
      "grad_norm": 19.793611526489258,
      "learning_rate": 1.9548801369863016e-05,
      "loss": 0.4213,
      "step": 4070
    },
    {
      "epoch": 1.7465753424657535,
      "grad_norm": 24.211828231811523,
      "learning_rate": 1.9523116438356166e-05,
      "loss": 0.6524,
      "step": 4080
    },
    {
      "epoch": 1.7508561643835616,
      "grad_norm": 76.02991485595703,
      "learning_rate": 1.9497431506849315e-05,
      "loss": 0.4516,
      "step": 4090
    },
    {
      "epoch": 1.7551369863013697,
      "grad_norm": 31.41939353942871,
      "learning_rate": 1.9471746575342465e-05,
      "loss": 0.7432,
      "step": 4100
    },
    {
      "epoch": 1.759417808219178,
      "grad_norm": 19.59832000732422,
      "learning_rate": 1.9446061643835615e-05,
      "loss": 0.3654,
      "step": 4110
    },
    {
      "epoch": 1.7636986301369864,
      "grad_norm": 5.274127006530762,
      "learning_rate": 1.9420376712328768e-05,
      "loss": 0.5074,
      "step": 4120
    },
    {
      "epoch": 1.7679794520547945,
      "grad_norm": 14.360557556152344,
      "learning_rate": 1.9394691780821918e-05,
      "loss": 0.3533,
      "step": 4130
    },
    {
      "epoch": 1.7722602739726028,
      "grad_norm": 31.25728416442871,
      "learning_rate": 1.9369006849315068e-05,
      "loss": 0.7591,
      "step": 4140
    },
    {
      "epoch": 1.776541095890411,
      "grad_norm": 13.008493423461914,
      "learning_rate": 1.9343321917808217e-05,
      "loss": 0.6009,
      "step": 4150
    },
    {
      "epoch": 1.7808219178082192,
      "grad_norm": 11.199233055114746,
      "learning_rate": 1.931763698630137e-05,
      "loss": 0.4796,
      "step": 4160
    },
    {
      "epoch": 1.7851027397260273,
      "grad_norm": 7.007798671722412,
      "learning_rate": 1.929195205479452e-05,
      "loss": 0.359,
      "step": 4170
    },
    {
      "epoch": 1.7893835616438356,
      "grad_norm": 7.08510160446167,
      "learning_rate": 1.9266267123287674e-05,
      "loss": 0.3583,
      "step": 4180
    },
    {
      "epoch": 1.793664383561644,
      "grad_norm": 22.454998016357422,
      "learning_rate": 1.924058219178082e-05,
      "loss": 0.663,
      "step": 4190
    },
    {
      "epoch": 1.797945205479452,
      "grad_norm": 1.7827061414718628,
      "learning_rate": 1.9214897260273973e-05,
      "loss": 0.581,
      "step": 4200
    },
    {
      "epoch": 1.8022260273972601,
      "grad_norm": 9.737443923950195,
      "learning_rate": 1.9189212328767123e-05,
      "loss": 0.5445,
      "step": 4210
    },
    {
      "epoch": 1.8065068493150684,
      "grad_norm": 18.056055068969727,
      "learning_rate": 1.9163527397260276e-05,
      "loss": 0.3796,
      "step": 4220
    },
    {
      "epoch": 1.8107876712328768,
      "grad_norm": 22.148479461669922,
      "learning_rate": 1.9137842465753426e-05,
      "loss": 0.4609,
      "step": 4230
    },
    {
      "epoch": 1.8150684931506849,
      "grad_norm": 6.404452323913574,
      "learning_rate": 1.9112157534246576e-05,
      "loss": 0.3963,
      "step": 4240
    },
    {
      "epoch": 1.8193493150684932,
      "grad_norm": 17.864734649658203,
      "learning_rate": 1.9086472602739725e-05,
      "loss": 0.4972,
      "step": 4250
    },
    {
      "epoch": 1.8236301369863015,
      "grad_norm": 14.38877010345459,
      "learning_rate": 1.906078767123288e-05,
      "loss": 0.5134,
      "step": 4260
    },
    {
      "epoch": 1.8279109589041096,
      "grad_norm": 12.86507797241211,
      "learning_rate": 1.903510273972603e-05,
      "loss": 0.267,
      "step": 4270
    },
    {
      "epoch": 1.8321917808219177,
      "grad_norm": 30.7558536529541,
      "learning_rate": 1.9009417808219178e-05,
      "loss": 0.3332,
      "step": 4280
    },
    {
      "epoch": 1.836472602739726,
      "grad_norm": 46.09252166748047,
      "learning_rate": 1.8983732876712328e-05,
      "loss": 0.6937,
      "step": 4290
    },
    {
      "epoch": 1.8407534246575343,
      "grad_norm": 20.406261444091797,
      "learning_rate": 1.895804794520548e-05,
      "loss": 0.3059,
      "step": 4300
    },
    {
      "epoch": 1.8450342465753424,
      "grad_norm": 2.7680084705352783,
      "learning_rate": 1.893236301369863e-05,
      "loss": 0.5121,
      "step": 4310
    },
    {
      "epoch": 1.8493150684931505,
      "grad_norm": 12.978144645690918,
      "learning_rate": 1.8906678082191784e-05,
      "loss": 0.5167,
      "step": 4320
    },
    {
      "epoch": 1.853595890410959,
      "grad_norm": 12.749186515808105,
      "learning_rate": 1.888099315068493e-05,
      "loss": 0.4093,
      "step": 4330
    },
    {
      "epoch": 1.8578767123287672,
      "grad_norm": 4.961120128631592,
      "learning_rate": 1.8855308219178084e-05,
      "loss": 0.6228,
      "step": 4340
    },
    {
      "epoch": 1.8621575342465753,
      "grad_norm": 11.497303009033203,
      "learning_rate": 1.8829623287671233e-05,
      "loss": 0.4085,
      "step": 4350
    },
    {
      "epoch": 1.8664383561643836,
      "grad_norm": 9.498723030090332,
      "learning_rate": 1.8803938356164387e-05,
      "loss": 0.6306,
      "step": 4360
    },
    {
      "epoch": 1.870719178082192,
      "grad_norm": 18.589962005615234,
      "learning_rate": 1.8778253424657533e-05,
      "loss": 0.5089,
      "step": 4370
    },
    {
      "epoch": 1.875,
      "grad_norm": 13.742088317871094,
      "learning_rate": 1.8752568493150686e-05,
      "loss": 0.4835,
      "step": 4380
    },
    {
      "epoch": 1.879280821917808,
      "grad_norm": 13.139420509338379,
      "learning_rate": 1.8726883561643836e-05,
      "loss": 0.5298,
      "step": 4390
    },
    {
      "epoch": 1.8835616438356164,
      "grad_norm": 12.52042007446289,
      "learning_rate": 1.870119863013699e-05,
      "loss": 0.4707,
      "step": 4400
    },
    {
      "epoch": 1.8878424657534247,
      "grad_norm": 46.593074798583984,
      "learning_rate": 1.867551369863014e-05,
      "loss": 0.6026,
      "step": 4410
    },
    {
      "epoch": 1.8921232876712328,
      "grad_norm": 20.09745979309082,
      "learning_rate": 1.864982876712329e-05,
      "loss": 0.4009,
      "step": 4420
    },
    {
      "epoch": 1.896404109589041,
      "grad_norm": 21.251699447631836,
      "learning_rate": 1.862414383561644e-05,
      "loss": 0.5198,
      "step": 4430
    },
    {
      "epoch": 1.9006849315068495,
      "grad_norm": 36.72938537597656,
      "learning_rate": 1.8598458904109588e-05,
      "loss": 0.5082,
      "step": 4440
    },
    {
      "epoch": 1.9049657534246576,
      "grad_norm": 7.994814872741699,
      "learning_rate": 1.857277397260274e-05,
      "loss": 0.421,
      "step": 4450
    },
    {
      "epoch": 1.9092465753424657,
      "grad_norm": 7.301170825958252,
      "learning_rate": 1.8547089041095888e-05,
      "loss": 0.6448,
      "step": 4460
    },
    {
      "epoch": 1.913527397260274,
      "grad_norm": 6.295528888702393,
      "learning_rate": 1.852140410958904e-05,
      "loss": 0.4079,
      "step": 4470
    },
    {
      "epoch": 1.9178082191780823,
      "grad_norm": 25.070934295654297,
      "learning_rate": 1.849571917808219e-05,
      "loss": 0.6458,
      "step": 4480
    },
    {
      "epoch": 1.9220890410958904,
      "grad_norm": 15.986374855041504,
      "learning_rate": 1.8470034246575344e-05,
      "loss": 0.5191,
      "step": 4490
    },
    {
      "epoch": 1.9263698630136985,
      "grad_norm": 10.834296226501465,
      "learning_rate": 1.8444349315068494e-05,
      "loss": 0.4371,
      "step": 4500
    },
    {
      "epoch": 1.9306506849315068,
      "grad_norm": 44.80891036987305,
      "learning_rate": 1.8418664383561643e-05,
      "loss": 0.5873,
      "step": 4510
    },
    {
      "epoch": 1.9349315068493151,
      "grad_norm": 11.328543663024902,
      "learning_rate": 1.8392979452054793e-05,
      "loss": 0.4742,
      "step": 4520
    },
    {
      "epoch": 1.9392123287671232,
      "grad_norm": 2.0611612796783447,
      "learning_rate": 1.8367294520547946e-05,
      "loss": 0.57,
      "step": 4530
    },
    {
      "epoch": 1.9434931506849316,
      "grad_norm": 8.943866729736328,
      "learning_rate": 1.8341609589041096e-05,
      "loss": 0.5446,
      "step": 4540
    },
    {
      "epoch": 1.9477739726027399,
      "grad_norm": 21.342111587524414,
      "learning_rate": 1.8315924657534246e-05,
      "loss": 0.6471,
      "step": 4550
    },
    {
      "epoch": 1.952054794520548,
      "grad_norm": 6.3900604248046875,
      "learning_rate": 1.8290239726027396e-05,
      "loss": 0.6117,
      "step": 4560
    },
    {
      "epoch": 1.956335616438356,
      "grad_norm": 6.169938087463379,
      "learning_rate": 1.826455479452055e-05,
      "loss": 0.5876,
      "step": 4570
    },
    {
      "epoch": 1.9606164383561644,
      "grad_norm": 10.066176414489746,
      "learning_rate": 1.82388698630137e-05,
      "loss": 0.3932,
      "step": 4580
    },
    {
      "epoch": 1.9648972602739727,
      "grad_norm": 41.185943603515625,
      "learning_rate": 1.8213184931506852e-05,
      "loss": 0.518,
      "step": 4590
    },
    {
      "epoch": 1.9691780821917808,
      "grad_norm": 21.49726104736328,
      "learning_rate": 1.8187499999999998e-05,
      "loss": 0.4224,
      "step": 4600
    },
    {
      "epoch": 1.973458904109589,
      "grad_norm": 5.734244346618652,
      "learning_rate": 1.816181506849315e-05,
      "loss": 0.3754,
      "step": 4610
    },
    {
      "epoch": 1.9777397260273972,
      "grad_norm": 37.46115493774414,
      "learning_rate": 1.81361301369863e-05,
      "loss": 0.5325,
      "step": 4620
    },
    {
      "epoch": 1.9820205479452055,
      "grad_norm": 28.129777908325195,
      "learning_rate": 1.8110445205479454e-05,
      "loss": 0.5172,
      "step": 4630
    },
    {
      "epoch": 1.9863013698630136,
      "grad_norm": 13.365239143371582,
      "learning_rate": 1.8084760273972604e-05,
      "loss": 0.3772,
      "step": 4640
    },
    {
      "epoch": 1.990582191780822,
      "grad_norm": 8.639898300170898,
      "learning_rate": 1.8059075342465754e-05,
      "loss": 0.3761,
      "step": 4650
    },
    {
      "epoch": 1.9948630136986303,
      "grad_norm": 13.050405502319336,
      "learning_rate": 1.8033390410958904e-05,
      "loss": 0.3278,
      "step": 4660
    },
    {
      "epoch": 1.9991438356164384,
      "grad_norm": 21.96680450439453,
      "learning_rate": 1.8007705479452057e-05,
      "loss": 0.5551,
      "step": 4670
    },
    {
      "epoch": 2.0,
      "eval_f1": 0.6594364809181281,
      "eval_loss": 0.7407808303833008,
      "eval_precision": 0.690955407939604,
      "eval_recall": 0.6671832623014335,
      "eval_runtime": 539.0773,
      "eval_samples_per_second": 8.292,
      "eval_steps_per_second": 1.037,
      "step": 4672
    },
    {
      "epoch": 2.0034246575342465,
      "grad_norm": 7.5784077644348145,
      "learning_rate": 1.7982020547945207e-05,
      "loss": 0.3331,
      "step": 4680
    },
    {
      "epoch": 2.0077054794520546,
      "grad_norm": 21.390098571777344,
      "learning_rate": 1.7956335616438356e-05,
      "loss": 0.4134,
      "step": 4690
    },
    {
      "epoch": 2.011986301369863,
      "grad_norm": 18.07249641418457,
      "learning_rate": 1.7930650684931506e-05,
      "loss": 0.2384,
      "step": 4700
    },
    {
      "epoch": 2.016267123287671,
      "grad_norm": 19.398225784301758,
      "learning_rate": 1.790496575342466e-05,
      "loss": 0.6319,
      "step": 4710
    },
    {
      "epoch": 2.0205479452054793,
      "grad_norm": 5.301888465881348,
      "learning_rate": 1.787928082191781e-05,
      "loss": 0.2799,
      "step": 4720
    },
    {
      "epoch": 2.024828767123288,
      "grad_norm": 4.345090866088867,
      "learning_rate": 1.7853595890410962e-05,
      "loss": 0.418,
      "step": 4730
    },
    {
      "epoch": 2.029109589041096,
      "grad_norm": 37.557518005371094,
      "learning_rate": 1.782791095890411e-05,
      "loss": 0.3438,
      "step": 4740
    },
    {
      "epoch": 2.033390410958904,
      "grad_norm": 5.074457168579102,
      "learning_rate": 1.7802226027397262e-05,
      "loss": 0.4904,
      "step": 4750
    },
    {
      "epoch": 2.037671232876712,
      "grad_norm": 3.9058921337127686,
      "learning_rate": 1.777654109589041e-05,
      "loss": 0.453,
      "step": 4760
    },
    {
      "epoch": 2.0419520547945207,
      "grad_norm": 2.5020668506622314,
      "learning_rate": 1.7750856164383565e-05,
      "loss": 0.2404,
      "step": 4770
    },
    {
      "epoch": 2.046232876712329,
      "grad_norm": 8.928509712219238,
      "learning_rate": 1.772517123287671e-05,
      "loss": 0.4391,
      "step": 4780
    },
    {
      "epoch": 2.050513698630137,
      "grad_norm": 14.929970741271973,
      "learning_rate": 1.769948630136986e-05,
      "loss": 0.4441,
      "step": 4790
    },
    {
      "epoch": 2.0547945205479454,
      "grad_norm": 7.411099433898926,
      "learning_rate": 1.7673801369863014e-05,
      "loss": 0.3208,
      "step": 4800
    },
    {
      "epoch": 2.0590753424657535,
      "grad_norm": 8.026456832885742,
      "learning_rate": 1.7648116438356164e-05,
      "loss": 0.4963,
      "step": 4810
    },
    {
      "epoch": 2.0633561643835616,
      "grad_norm": 8.132319450378418,
      "learning_rate": 1.7622431506849317e-05,
      "loss": 0.2727,
      "step": 4820
    },
    {
      "epoch": 2.0676369863013697,
      "grad_norm": 18.716650009155273,
      "learning_rate": 1.7596746575342464e-05,
      "loss": 0.572,
      "step": 4830
    },
    {
      "epoch": 2.0719178082191783,
      "grad_norm": 24.137731552124023,
      "learning_rate": 1.7571061643835617e-05,
      "loss": 0.2845,
      "step": 4840
    },
    {
      "epoch": 2.0761986301369864,
      "grad_norm": 4.676906585693359,
      "learning_rate": 1.7545376712328767e-05,
      "loss": 0.4307,
      "step": 4850
    },
    {
      "epoch": 2.0804794520547945,
      "grad_norm": 17.069551467895508,
      "learning_rate": 1.751969178082192e-05,
      "loss": 0.3838,
      "step": 4860
    },
    {
      "epoch": 2.0847602739726026,
      "grad_norm": 7.364201068878174,
      "learning_rate": 1.7494006849315066e-05,
      "loss": 0.3437,
      "step": 4870
    },
    {
      "epoch": 2.089041095890411,
      "grad_norm": 2.5601813793182373,
      "learning_rate": 1.746832191780822e-05,
      "loss": 0.382,
      "step": 4880
    },
    {
      "epoch": 2.093321917808219,
      "grad_norm": 16.39969253540039,
      "learning_rate": 1.744263698630137e-05,
      "loss": 0.4344,
      "step": 4890
    },
    {
      "epoch": 2.0976027397260273,
      "grad_norm": 5.234245777130127,
      "learning_rate": 1.7416952054794522e-05,
      "loss": 0.302,
      "step": 4900
    },
    {
      "epoch": 2.101883561643836,
      "grad_norm": 4.944858551025391,
      "learning_rate": 1.7391267123287672e-05,
      "loss": 0.4896,
      "step": 4910
    },
    {
      "epoch": 2.106164383561644,
      "grad_norm": 15.660709381103516,
      "learning_rate": 1.7365582191780822e-05,
      "loss": 0.4176,
      "step": 4920
    },
    {
      "epoch": 2.110445205479452,
      "grad_norm": 3.9154975414276123,
      "learning_rate": 1.733989726027397e-05,
      "loss": 0.3128,
      "step": 4930
    },
    {
      "epoch": 2.11472602739726,
      "grad_norm": 27.22719955444336,
      "learning_rate": 1.7314212328767125e-05,
      "loss": 0.3307,
      "step": 4940
    },
    {
      "epoch": 2.1190068493150687,
      "grad_norm": 24.816665649414062,
      "learning_rate": 1.7288527397260274e-05,
      "loss": 0.5286,
      "step": 4950
    },
    {
      "epoch": 2.1232876712328768,
      "grad_norm": 28.056711196899414,
      "learning_rate": 1.7262842465753428e-05,
      "loss": 0.5091,
      "step": 4960
    },
    {
      "epoch": 2.127568493150685,
      "grad_norm": 36.482337951660156,
      "learning_rate": 1.7237157534246574e-05,
      "loss": 0.3768,
      "step": 4970
    },
    {
      "epoch": 2.131849315068493,
      "grad_norm": 32.92665481567383,
      "learning_rate": 1.7211472602739727e-05,
      "loss": 0.4938,
      "step": 4980
    },
    {
      "epoch": 2.1361301369863015,
      "grad_norm": 5.585390090942383,
      "learning_rate": 1.7185787671232877e-05,
      "loss": 0.4322,
      "step": 4990
    },
    {
      "epoch": 2.1404109589041096,
      "grad_norm": 17.453750610351562,
      "learning_rate": 1.716010273972603e-05,
      "loss": 0.4746,
      "step": 5000
    },
    {
      "epoch": 2.1446917808219177,
      "grad_norm": 5.330987930297852,
      "learning_rate": 1.7134417808219177e-05,
      "loss": 0.3506,
      "step": 5010
    },
    {
      "epoch": 2.1489726027397262,
      "grad_norm": 13.89369010925293,
      "learning_rate": 1.710873287671233e-05,
      "loss": 0.3438,
      "step": 5020
    },
    {
      "epoch": 2.1532534246575343,
      "grad_norm": 28.46163558959961,
      "learning_rate": 1.708304794520548e-05,
      "loss": 0.4291,
      "step": 5030
    },
    {
      "epoch": 2.1575342465753424,
      "grad_norm": 9.328664779663086,
      "learning_rate": 1.7057363013698633e-05,
      "loss": 0.2962,
      "step": 5040
    },
    {
      "epoch": 2.1618150684931505,
      "grad_norm": 26.54800796508789,
      "learning_rate": 1.7031678082191782e-05,
      "loss": 0.3664,
      "step": 5050
    },
    {
      "epoch": 2.166095890410959,
      "grad_norm": 10.387811660766602,
      "learning_rate": 1.7005993150684932e-05,
      "loss": 0.4343,
      "step": 5060
    },
    {
      "epoch": 2.170376712328767,
      "grad_norm": 7.560271739959717,
      "learning_rate": 1.6980308219178082e-05,
      "loss": 0.6425,
      "step": 5070
    },
    {
      "epoch": 2.1746575342465753,
      "grad_norm": 5.551652431488037,
      "learning_rate": 1.6954623287671235e-05,
      "loss": 0.3556,
      "step": 5080
    },
    {
      "epoch": 2.1789383561643834,
      "grad_norm": 26.02391242980957,
      "learning_rate": 1.6928938356164385e-05,
      "loss": 0.5694,
      "step": 5090
    },
    {
      "epoch": 2.183219178082192,
      "grad_norm": 6.021932125091553,
      "learning_rate": 1.6903253424657535e-05,
      "loss": 0.4364,
      "step": 5100
    },
    {
      "epoch": 2.1875,
      "grad_norm": 5.446930885314941,
      "learning_rate": 1.6877568493150685e-05,
      "loss": 0.245,
      "step": 5110
    },
    {
      "epoch": 2.191780821917808,
      "grad_norm": 82.65406799316406,
      "learning_rate": 1.6851883561643834e-05,
      "loss": 0.3438,
      "step": 5120
    },
    {
      "epoch": 2.1960616438356166,
      "grad_norm": 1.4810196161270142,
      "learning_rate": 1.6826198630136988e-05,
      "loss": 0.3721,
      "step": 5130
    },
    {
      "epoch": 2.2003424657534247,
      "grad_norm": 4.872475624084473,
      "learning_rate": 1.6800513698630137e-05,
      "loss": 0.2618,
      "step": 5140
    },
    {
      "epoch": 2.204623287671233,
      "grad_norm": 7.877708911895752,
      "learning_rate": 1.6774828767123287e-05,
      "loss": 0.3209,
      "step": 5150
    },
    {
      "epoch": 2.208904109589041,
      "grad_norm": 35.26039505004883,
      "learning_rate": 1.6749143835616437e-05,
      "loss": 0.3139,
      "step": 5160
    },
    {
      "epoch": 2.2131849315068495,
      "grad_norm": 17.693119049072266,
      "learning_rate": 1.672345890410959e-05,
      "loss": 0.5413,
      "step": 5170
    },
    {
      "epoch": 2.2174657534246576,
      "grad_norm": 19.4815616607666,
      "learning_rate": 1.669777397260274e-05,
      "loss": 0.2726,
      "step": 5180
    },
    {
      "epoch": 2.2217465753424657,
      "grad_norm": 28.45952796936035,
      "learning_rate": 1.667208904109589e-05,
      "loss": 0.4977,
      "step": 5190
    },
    {
      "epoch": 2.2260273972602738,
      "grad_norm": 27.157264709472656,
      "learning_rate": 1.664640410958904e-05,
      "loss": 0.555,
      "step": 5200
    },
    {
      "epoch": 2.2303082191780823,
      "grad_norm": 19.181812286376953,
      "learning_rate": 1.6620719178082193e-05,
      "loss": 0.1182,
      "step": 5210
    },
    {
      "epoch": 2.2345890410958904,
      "grad_norm": 19.080657958984375,
      "learning_rate": 1.6595034246575342e-05,
      "loss": 0.3881,
      "step": 5220
    },
    {
      "epoch": 2.2388698630136985,
      "grad_norm": 2.213844060897827,
      "learning_rate": 1.6569349315068495e-05,
      "loss": 0.457,
      "step": 5230
    },
    {
      "epoch": 2.243150684931507,
      "grad_norm": 41.012664794921875,
      "learning_rate": 1.6543664383561642e-05,
      "loss": 0.436,
      "step": 5240
    },
    {
      "epoch": 2.247431506849315,
      "grad_norm": 3.807694435119629,
      "learning_rate": 1.6517979452054795e-05,
      "loss": 0.7137,
      "step": 5250
    },
    {
      "epoch": 2.2517123287671232,
      "grad_norm": 8.440142631530762,
      "learning_rate": 1.6492294520547945e-05,
      "loss": 0.3145,
      "step": 5260
    },
    {
      "epoch": 2.2559931506849313,
      "grad_norm": 3.7810192108154297,
      "learning_rate": 1.6466609589041098e-05,
      "loss": 0.3953,
      "step": 5270
    },
    {
      "epoch": 2.26027397260274,
      "grad_norm": 9.430413246154785,
      "learning_rate": 1.6440924657534244e-05,
      "loss": 0.4492,
      "step": 5280
    },
    {
      "epoch": 2.264554794520548,
      "grad_norm": 32.50558853149414,
      "learning_rate": 1.6415239726027398e-05,
      "loss": 0.3626,
      "step": 5290
    },
    {
      "epoch": 2.268835616438356,
      "grad_norm": 20.190345764160156,
      "learning_rate": 1.6389554794520547e-05,
      "loss": 0.458,
      "step": 5300
    },
    {
      "epoch": 2.2731164383561646,
      "grad_norm": 14.516268730163574,
      "learning_rate": 1.63638698630137e-05,
      "loss": 0.6646,
      "step": 5310
    },
    {
      "epoch": 2.2773972602739727,
      "grad_norm": 41.566795349121094,
      "learning_rate": 1.633818493150685e-05,
      "loss": 0.2481,
      "step": 5320
    },
    {
      "epoch": 2.281678082191781,
      "grad_norm": 6.321224212646484,
      "learning_rate": 1.63125e-05,
      "loss": 0.2749,
      "step": 5330
    },
    {
      "epoch": 2.285958904109589,
      "grad_norm": 25.822124481201172,
      "learning_rate": 1.628681506849315e-05,
      "loss": 0.427,
      "step": 5340
    },
    {
      "epoch": 2.2902397260273974,
      "grad_norm": 7.016406059265137,
      "learning_rate": 1.6261130136986303e-05,
      "loss": 0.2023,
      "step": 5350
    },
    {
      "epoch": 2.2945205479452055,
      "grad_norm": 9.202388763427734,
      "learning_rate": 1.6235445205479453e-05,
      "loss": 0.3631,
      "step": 5360
    },
    {
      "epoch": 2.2988013698630136,
      "grad_norm": 68.24703216552734,
      "learning_rate": 1.6209760273972606e-05,
      "loss": 0.5583,
      "step": 5370
    },
    {
      "epoch": 2.3030821917808217,
      "grad_norm": 16.365711212158203,
      "learning_rate": 1.6184075342465752e-05,
      "loss": 0.4531,
      "step": 5380
    },
    {
      "epoch": 2.3073630136986303,
      "grad_norm": 26.823516845703125,
      "learning_rate": 1.6158390410958906e-05,
      "loss": 0.3281,
      "step": 5390
    },
    {
      "epoch": 2.3116438356164384,
      "grad_norm": 18.592607498168945,
      "learning_rate": 1.6132705479452055e-05,
      "loss": 0.3683,
      "step": 5400
    },
    {
      "epoch": 2.3159246575342465,
      "grad_norm": 25.722455978393555,
      "learning_rate": 1.610702054794521e-05,
      "loss": 0.3496,
      "step": 5410
    },
    {
      "epoch": 2.3202054794520546,
      "grad_norm": 18.97013282775879,
      "learning_rate": 1.6081335616438355e-05,
      "loss": 0.516,
      "step": 5420
    },
    {
      "epoch": 2.324486301369863,
      "grad_norm": 17.858768463134766,
      "learning_rate": 1.6055650684931508e-05,
      "loss": 0.5239,
      "step": 5430
    },
    {
      "epoch": 2.328767123287671,
      "grad_norm": 0.8647858500480652,
      "learning_rate": 1.6029965753424658e-05,
      "loss": 0.1986,
      "step": 5440
    },
    {
      "epoch": 2.3330479452054793,
      "grad_norm": 18.001590728759766,
      "learning_rate": 1.600428082191781e-05,
      "loss": 0.3065,
      "step": 5450
    },
    {
      "epoch": 2.337328767123288,
      "grad_norm": 16.14793586730957,
      "learning_rate": 1.597859589041096e-05,
      "loss": 0.3663,
      "step": 5460
    },
    {
      "epoch": 2.341609589041096,
      "grad_norm": 29.626916885375977,
      "learning_rate": 1.5952910958904107e-05,
      "loss": 0.3091,
      "step": 5470
    },
    {
      "epoch": 2.345890410958904,
      "grad_norm": 8.745993614196777,
      "learning_rate": 1.592722602739726e-05,
      "loss": 0.3453,
      "step": 5480
    },
    {
      "epoch": 2.350171232876712,
      "grad_norm": 18.141637802124023,
      "learning_rate": 1.590154109589041e-05,
      "loss": 0.4909,
      "step": 5490
    },
    {
      "epoch": 2.3544520547945207,
      "grad_norm": 7.329688549041748,
      "learning_rate": 1.5875856164383563e-05,
      "loss": 0.5541,
      "step": 5500
    },
    {
      "epoch": 2.358732876712329,
      "grad_norm": 22.19893455505371,
      "learning_rate": 1.585017123287671e-05,
      "loss": 0.3543,
      "step": 5510
    },
    {
      "epoch": 2.363013698630137,
      "grad_norm": 27.797569274902344,
      "learning_rate": 1.5824486301369863e-05,
      "loss": 0.2833,
      "step": 5520
    },
    {
      "epoch": 2.3672945205479454,
      "grad_norm": 25.593875885009766,
      "learning_rate": 1.5798801369863013e-05,
      "loss": 0.3358,
      "step": 5530
    },
    {
      "epoch": 2.3715753424657535,
      "grad_norm": 12.76595401763916,
      "learning_rate": 1.5773116438356166e-05,
      "loss": 0.4432,
      "step": 5540
    },
    {
      "epoch": 2.3758561643835616,
      "grad_norm": 12.162564277648926,
      "learning_rate": 1.5747431506849316e-05,
      "loss": 0.4548,
      "step": 5550
    },
    {
      "epoch": 2.3801369863013697,
      "grad_norm": 4.608762264251709,
      "learning_rate": 1.5721746575342465e-05,
      "loss": 0.399,
      "step": 5560
    },
    {
      "epoch": 2.3844178082191783,
      "grad_norm": 23.13904571533203,
      "learning_rate": 1.5696061643835615e-05,
      "loss": 0.3768,
      "step": 5570
    },
    {
      "epoch": 2.3886986301369864,
      "grad_norm": 13.040365219116211,
      "learning_rate": 1.567037671232877e-05,
      "loss": 0.5266,
      "step": 5580
    },
    {
      "epoch": 2.3929794520547945,
      "grad_norm": 27.029071807861328,
      "learning_rate": 1.5644691780821918e-05,
      "loss": 0.4631,
      "step": 5590
    },
    {
      "epoch": 2.3972602739726026,
      "grad_norm": 12.686609268188477,
      "learning_rate": 1.5619006849315068e-05,
      "loss": 0.3717,
      "step": 5600
    },
    {
      "epoch": 2.401541095890411,
      "grad_norm": 34.209197998046875,
      "learning_rate": 1.5593321917808218e-05,
      "loss": 0.3817,
      "step": 5610
    },
    {
      "epoch": 2.405821917808219,
      "grad_norm": 10.254599571228027,
      "learning_rate": 1.556763698630137e-05,
      "loss": 0.4157,
      "step": 5620
    },
    {
      "epoch": 2.4101027397260273,
      "grad_norm": 16.622798919677734,
      "learning_rate": 1.554195205479452e-05,
      "loss": 0.4799,
      "step": 5630
    },
    {
      "epoch": 2.4143835616438354,
      "grad_norm": 18.332128524780273,
      "learning_rate": 1.5516267123287674e-05,
      "loss": 0.3347,
      "step": 5640
    },
    {
      "epoch": 2.418664383561644,
      "grad_norm": 25.412059783935547,
      "learning_rate": 1.549058219178082e-05,
      "loss": 0.3954,
      "step": 5650
    },
    {
      "epoch": 2.422945205479452,
      "grad_norm": 12.598109245300293,
      "learning_rate": 1.5464897260273973e-05,
      "loss": 0.5304,
      "step": 5660
    },
    {
      "epoch": 2.42722602739726,
      "grad_norm": 5.205364227294922,
      "learning_rate": 1.5439212328767123e-05,
      "loss": 0.2844,
      "step": 5670
    },
    {
      "epoch": 2.4315068493150687,
      "grad_norm": 14.736977577209473,
      "learning_rate": 1.5413527397260276e-05,
      "loss": 0.2891,
      "step": 5680
    },
    {
      "epoch": 2.4357876712328768,
      "grad_norm": 16.482112884521484,
      "learning_rate": 1.5387842465753426e-05,
      "loss": 0.3232,
      "step": 5690
    },
    {
      "epoch": 2.440068493150685,
      "grad_norm": 4.986355781555176,
      "learning_rate": 1.5362157534246576e-05,
      "loss": 0.2102,
      "step": 5700
    },
    {
      "epoch": 2.444349315068493,
      "grad_norm": 16.099008560180664,
      "learning_rate": 1.5336472602739726e-05,
      "loss": 0.4282,
      "step": 5710
    },
    {
      "epoch": 2.4486301369863015,
      "grad_norm": 8.582435607910156,
      "learning_rate": 1.531078767123288e-05,
      "loss": 0.3354,
      "step": 5720
    },
    {
      "epoch": 2.4529109589041096,
      "grad_norm": 16.83907699584961,
      "learning_rate": 1.528510273972603e-05,
      "loss": 0.348,
      "step": 5730
    },
    {
      "epoch": 2.4571917808219177,
      "grad_norm": 18.713483810424805,
      "learning_rate": 1.525941780821918e-05,
      "loss": 0.5037,
      "step": 5740
    },
    {
      "epoch": 2.4614726027397262,
      "grad_norm": 8.611288070678711,
      "learning_rate": 1.5233732876712328e-05,
      "loss": 0.3006,
      "step": 5750
    },
    {
      "epoch": 2.4657534246575343,
      "grad_norm": 40.95690155029297,
      "learning_rate": 1.520804794520548e-05,
      "loss": 0.2929,
      "step": 5760
    },
    {
      "epoch": 2.4700342465753424,
      "grad_norm": 11.442882537841797,
      "learning_rate": 1.5182363013698631e-05,
      "loss": 0.4211,
      "step": 5770
    },
    {
      "epoch": 2.4743150684931505,
      "grad_norm": 9.837360382080078,
      "learning_rate": 1.5156678082191783e-05,
      "loss": 0.317,
      "step": 5780
    },
    {
      "epoch": 2.478595890410959,
      "grad_norm": 3.857771635055542,
      "learning_rate": 1.513099315068493e-05,
      "loss": 0.3037,
      "step": 5790
    },
    {
      "epoch": 2.482876712328767,
      "grad_norm": 3.8799350261688232,
      "learning_rate": 1.5105308219178082e-05,
      "loss": 0.2907,
      "step": 5800
    },
    {
      "epoch": 2.4871575342465753,
      "grad_norm": 8.690640449523926,
      "learning_rate": 1.5079623287671234e-05,
      "loss": 0.3324,
      "step": 5810
    },
    {
      "epoch": 2.491438356164384,
      "grad_norm": 16.528512954711914,
      "learning_rate": 1.5053938356164385e-05,
      "loss": 0.2389,
      "step": 5820
    },
    {
      "epoch": 2.495719178082192,
      "grad_norm": 26.928821563720703,
      "learning_rate": 1.5028253424657533e-05,
      "loss": 0.4505,
      "step": 5830
    },
    {
      "epoch": 2.5,
      "grad_norm": 14.557092666625977,
      "learning_rate": 1.5002568493150685e-05,
      "loss": 0.2683,
      "step": 5840
    },
    {
      "epoch": 2.504280821917808,
      "grad_norm": 6.253172397613525,
      "learning_rate": 1.4976883561643836e-05,
      "loss": 0.3873,
      "step": 5850
    },
    {
      "epoch": 2.508561643835616,
      "grad_norm": 2.911450147628784,
      "learning_rate": 1.4951198630136988e-05,
      "loss": 0.4035,
      "step": 5860
    },
    {
      "epoch": 2.5128424657534247,
      "grad_norm": 4.000398635864258,
      "learning_rate": 1.4925513698630137e-05,
      "loss": 0.4091,
      "step": 5870
    },
    {
      "epoch": 2.517123287671233,
      "grad_norm": 26.130159378051758,
      "learning_rate": 1.4899828767123289e-05,
      "loss": 0.4736,
      "step": 5880
    },
    {
      "epoch": 2.521404109589041,
      "grad_norm": 3.722693920135498,
      "learning_rate": 1.4874143835616439e-05,
      "loss": 0.2163,
      "step": 5890
    },
    {
      "epoch": 2.5256849315068495,
      "grad_norm": 29.399431228637695,
      "learning_rate": 1.484845890410959e-05,
      "loss": 0.3091,
      "step": 5900
    },
    {
      "epoch": 2.5299657534246576,
      "grad_norm": 17.68366241455078,
      "learning_rate": 1.482277397260274e-05,
      "loss": 0.4119,
      "step": 5910
    },
    {
      "epoch": 2.5342465753424657,
      "grad_norm": 41.45615768432617,
      "learning_rate": 1.4797089041095891e-05,
      "loss": 0.4262,
      "step": 5920
    },
    {
      "epoch": 2.5385273972602738,
      "grad_norm": 7.572283744812012,
      "learning_rate": 1.4771404109589041e-05,
      "loss": 0.5072,
      "step": 5930
    },
    {
      "epoch": 2.5428082191780823,
      "grad_norm": 1.1365571022033691,
      "learning_rate": 1.4745719178082193e-05,
      "loss": 0.2365,
      "step": 5940
    },
    {
      "epoch": 2.5470890410958904,
      "grad_norm": 5.3911967277526855,
      "learning_rate": 1.4720034246575344e-05,
      "loss": 0.5228,
      "step": 5950
    },
    {
      "epoch": 2.5513698630136985,
      "grad_norm": 15.704383850097656,
      "learning_rate": 1.4694349315068494e-05,
      "loss": 0.4204,
      "step": 5960
    },
    {
      "epoch": 2.555650684931507,
      "grad_norm": 17.785507202148438,
      "learning_rate": 1.4668664383561645e-05,
      "loss": 0.4702,
      "step": 5970
    },
    {
      "epoch": 2.559931506849315,
      "grad_norm": 54.5627555847168,
      "learning_rate": 1.4642979452054795e-05,
      "loss": 0.5656,
      "step": 5980
    },
    {
      "epoch": 2.5642123287671232,
      "grad_norm": 0.4259118437767029,
      "learning_rate": 1.4617294520547945e-05,
      "loss": 0.3317,
      "step": 5990
    },
    {
      "epoch": 2.5684931506849313,
      "grad_norm": 14.865763664245605,
      "learning_rate": 1.4591609589041095e-05,
      "loss": 0.3364,
      "step": 6000
    },
    {
      "epoch": 2.57277397260274,
      "grad_norm": 71.84607696533203,
      "learning_rate": 1.4565924657534246e-05,
      "loss": 0.4389,
      "step": 6010
    },
    {
      "epoch": 2.577054794520548,
      "grad_norm": 3.5749399662017822,
      "learning_rate": 1.4540239726027398e-05,
      "loss": 0.4435,
      "step": 6020
    },
    {
      "epoch": 2.581335616438356,
      "grad_norm": 6.95957088470459,
      "learning_rate": 1.4514554794520548e-05,
      "loss": 0.4064,
      "step": 6030
    },
    {
      "epoch": 2.5856164383561646,
      "grad_norm": 32.337677001953125,
      "learning_rate": 1.4488869863013699e-05,
      "loss": 0.4884,
      "step": 6040
    },
    {
      "epoch": 2.5898972602739727,
      "grad_norm": 14.259390830993652,
      "learning_rate": 1.4463184931506849e-05,
      "loss": 0.315,
      "step": 6050
    },
    {
      "epoch": 2.594178082191781,
      "grad_norm": 15.365755081176758,
      "learning_rate": 1.44375e-05,
      "loss": 0.3082,
      "step": 6060
    },
    {
      "epoch": 2.598458904109589,
      "grad_norm": 14.756214141845703,
      "learning_rate": 1.441181506849315e-05,
      "loss": 0.3758,
      "step": 6070
    },
    {
      "epoch": 2.602739726027397,
      "grad_norm": 9.131820678710938,
      "learning_rate": 1.4386130136986302e-05,
      "loss": 0.2125,
      "step": 6080
    },
    {
      "epoch": 2.6070205479452055,
      "grad_norm": 8.63611888885498,
      "learning_rate": 1.4360445205479451e-05,
      "loss": 0.5943,
      "step": 6090
    },
    {
      "epoch": 2.6113013698630136,
      "grad_norm": 0.5434450507164001,
      "learning_rate": 1.4334760273972603e-05,
      "loss": 0.265,
      "step": 6100
    },
    {
      "epoch": 2.615582191780822,
      "grad_norm": 48.881927490234375,
      "learning_rate": 1.4309075342465754e-05,
      "loss": 0.7705,
      "step": 6110
    },
    {
      "epoch": 2.6198630136986303,
      "grad_norm": 30.735191345214844,
      "learning_rate": 1.4283390410958904e-05,
      "loss": 0.3856,
      "step": 6120
    },
    {
      "epoch": 2.6241438356164384,
      "grad_norm": 7.842372894287109,
      "learning_rate": 1.4257705479452056e-05,
      "loss": 0.5184,
      "step": 6130
    },
    {
      "epoch": 2.6284246575342465,
      "grad_norm": 5.729582786560059,
      "learning_rate": 1.4232020547945205e-05,
      "loss": 0.311,
      "step": 6140
    },
    {
      "epoch": 2.6327054794520546,
      "grad_norm": 58.029170989990234,
      "learning_rate": 1.4206335616438357e-05,
      "loss": 0.2896,
      "step": 6150
    },
    {
      "epoch": 2.636986301369863,
      "grad_norm": 68.16889953613281,
      "learning_rate": 1.4180650684931507e-05,
      "loss": 0.4067,
      "step": 6160
    },
    {
      "epoch": 2.641267123287671,
      "grad_norm": 14.337040901184082,
      "learning_rate": 1.4154965753424658e-05,
      "loss": 0.2654,
      "step": 6170
    },
    {
      "epoch": 2.6455479452054793,
      "grad_norm": 22.358224868774414,
      "learning_rate": 1.412928082191781e-05,
      "loss": 0.4134,
      "step": 6180
    },
    {
      "epoch": 2.649828767123288,
      "grad_norm": 3.1232869625091553,
      "learning_rate": 1.410359589041096e-05,
      "loss": 0.2272,
      "step": 6190
    },
    {
      "epoch": 2.654109589041096,
      "grad_norm": 15.371111869812012,
      "learning_rate": 1.407791095890411e-05,
      "loss": 0.2948,
      "step": 6200
    },
    {
      "epoch": 2.658390410958904,
      "grad_norm": 6.1461873054504395,
      "learning_rate": 1.405222602739726e-05,
      "loss": 0.3548,
      "step": 6210
    },
    {
      "epoch": 2.662671232876712,
      "grad_norm": 12.958230972290039,
      "learning_rate": 1.4026541095890412e-05,
      "loss": 0.2452,
      "step": 6220
    },
    {
      "epoch": 2.6669520547945207,
      "grad_norm": 13.883332252502441,
      "learning_rate": 1.4000856164383562e-05,
      "loss": 0.3661,
      "step": 6230
    },
    {
      "epoch": 2.671232876712329,
      "grad_norm": 18.730741500854492,
      "learning_rate": 1.3975171232876713e-05,
      "loss": 0.4045,
      "step": 6240
    },
    {
      "epoch": 2.675513698630137,
      "grad_norm": 22.66792106628418,
      "learning_rate": 1.3949486301369863e-05,
      "loss": 0.7604,
      "step": 6250
    },
    {
      "epoch": 2.6797945205479454,
      "grad_norm": 12.541153907775879,
      "learning_rate": 1.3923801369863015e-05,
      "loss": 0.4365,
      "step": 6260
    },
    {
      "epoch": 2.6840753424657535,
      "grad_norm": 33.599945068359375,
      "learning_rate": 1.3898116438356166e-05,
      "loss": 0.3368,
      "step": 6270
    },
    {
      "epoch": 2.6883561643835616,
      "grad_norm": 7.811888217926025,
      "learning_rate": 1.3872431506849316e-05,
      "loss": 0.3929,
      "step": 6280
    },
    {
      "epoch": 2.6926369863013697,
      "grad_norm": 12.050020217895508,
      "learning_rate": 1.3846746575342467e-05,
      "loss": 0.2433,
      "step": 6290
    },
    {
      "epoch": 2.696917808219178,
      "grad_norm": 6.235482215881348,
      "learning_rate": 1.3821061643835617e-05,
      "loss": 0.3246,
      "step": 6300
    },
    {
      "epoch": 2.7011986301369864,
      "grad_norm": 2.5552730560302734,
      "learning_rate": 1.3795376712328769e-05,
      "loss": 0.2813,
      "step": 6310
    },
    {
      "epoch": 2.7054794520547945,
      "grad_norm": 34.8233757019043,
      "learning_rate": 1.3769691780821918e-05,
      "loss": 0.4047,
      "step": 6320
    },
    {
      "epoch": 2.709760273972603,
      "grad_norm": 2.2522239685058594,
      "learning_rate": 1.3744006849315068e-05,
      "loss": 0.2541,
      "step": 6330
    },
    {
      "epoch": 2.714041095890411,
      "grad_norm": 73.57962036132812,
      "learning_rate": 1.371832191780822e-05,
      "loss": 0.3109,
      "step": 6340
    },
    {
      "epoch": 2.718321917808219,
      "grad_norm": 5.327383041381836,
      "learning_rate": 1.369263698630137e-05,
      "loss": 0.4063,
      "step": 6350
    },
    {
      "epoch": 2.7226027397260273,
      "grad_norm": 47.393096923828125,
      "learning_rate": 1.366695205479452e-05,
      "loss": 0.2141,
      "step": 6360
    },
    {
      "epoch": 2.7268835616438354,
      "grad_norm": 95.91473388671875,
      "learning_rate": 1.364126712328767e-05,
      "loss": 0.3217,
      "step": 6370
    },
    {
      "epoch": 2.731164383561644,
      "grad_norm": 4.98040246963501,
      "learning_rate": 1.3615582191780822e-05,
      "loss": 0.4745,
      "step": 6380
    },
    {
      "epoch": 2.735445205479452,
      "grad_norm": 4.033075332641602,
      "learning_rate": 1.3589897260273972e-05,
      "loss": 0.4663,
      "step": 6390
    },
    {
      "epoch": 2.73972602739726,
      "grad_norm": 59.075706481933594,
      "learning_rate": 1.3564212328767123e-05,
      "loss": 0.4925,
      "step": 6400
    },
    {
      "epoch": 2.7440068493150687,
      "grad_norm": 2.0034759044647217,
      "learning_rate": 1.3538527397260273e-05,
      "loss": 0.3554,
      "step": 6410
    },
    {
      "epoch": 2.7482876712328768,
      "grad_norm": 14.203645706176758,
      "learning_rate": 1.3512842465753425e-05,
      "loss": 0.3135,
      "step": 6420
    },
    {
      "epoch": 2.752568493150685,
      "grad_norm": 28.168960571289062,
      "learning_rate": 1.3487157534246576e-05,
      "loss": 0.3593,
      "step": 6430
    },
    {
      "epoch": 2.756849315068493,
      "grad_norm": 8.687153816223145,
      "learning_rate": 1.3461472602739726e-05,
      "loss": 0.2939,
      "step": 6440
    },
    {
      "epoch": 2.7611301369863015,
      "grad_norm": 178.3568878173828,
      "learning_rate": 1.3435787671232877e-05,
      "loss": 0.2925,
      "step": 6450
    },
    {
      "epoch": 2.7654109589041096,
      "grad_norm": 10.218193054199219,
      "learning_rate": 1.3410102739726027e-05,
      "loss": 0.2169,
      "step": 6460
    },
    {
      "epoch": 2.7696917808219177,
      "grad_norm": 9.835856437683105,
      "learning_rate": 1.3384417808219179e-05,
      "loss": 0.3026,
      "step": 6470
    },
    {
      "epoch": 2.7739726027397262,
      "grad_norm": 10.289078712463379,
      "learning_rate": 1.3358732876712328e-05,
      "loss": 0.3038,
      "step": 6480
    },
    {
      "epoch": 2.7782534246575343,
      "grad_norm": 14.575322151184082,
      "learning_rate": 1.333304794520548e-05,
      "loss": 0.4191,
      "step": 6490
    },
    {
      "epoch": 2.7825342465753424,
      "grad_norm": 18.783327102661133,
      "learning_rate": 1.3307363013698631e-05,
      "loss": 0.3779,
      "step": 6500
    },
    {
      "epoch": 2.7868150684931505,
      "grad_norm": 9.678206443786621,
      "learning_rate": 1.3281678082191781e-05,
      "loss": 0.4244,
      "step": 6510
    },
    {
      "epoch": 2.791095890410959,
      "grad_norm": 10.929518699645996,
      "learning_rate": 1.3255993150684933e-05,
      "loss": 0.276,
      "step": 6520
    },
    {
      "epoch": 2.795376712328767,
      "grad_norm": 13.22543716430664,
      "learning_rate": 1.3230308219178082e-05,
      "loss": 0.3198,
      "step": 6530
    },
    {
      "epoch": 2.7996575342465753,
      "grad_norm": 11.663180351257324,
      "learning_rate": 1.3204623287671234e-05,
      "loss": 0.3545,
      "step": 6540
    },
    {
      "epoch": 2.803938356164384,
      "grad_norm": 20.666854858398438,
      "learning_rate": 1.3178938356164384e-05,
      "loss": 0.3197,
      "step": 6550
    },
    {
      "epoch": 2.808219178082192,
      "grad_norm": 52.60772705078125,
      "learning_rate": 1.3153253424657535e-05,
      "loss": 0.3719,
      "step": 6560
    },
    {
      "epoch": 2.8125,
      "grad_norm": 4.957720756530762,
      "learning_rate": 1.3127568493150685e-05,
      "loss": 0.3335,
      "step": 6570
    },
    {
      "epoch": 2.816780821917808,
      "grad_norm": 16.60493278503418,
      "learning_rate": 1.3101883561643836e-05,
      "loss": 0.2479,
      "step": 6580
    },
    {
      "epoch": 2.821061643835616,
      "grad_norm": 66.47907257080078,
      "learning_rate": 1.3076198630136988e-05,
      "loss": 0.6352,
      "step": 6590
    },
    {
      "epoch": 2.8253424657534247,
      "grad_norm": 39.71760559082031,
      "learning_rate": 1.3050513698630138e-05,
      "loss": 0.2677,
      "step": 6600
    },
    {
      "epoch": 2.829623287671233,
      "grad_norm": 1.683408260345459,
      "learning_rate": 1.3024828767123289e-05,
      "loss": 0.3094,
      "step": 6610
    },
    {
      "epoch": 2.833904109589041,
      "grad_norm": 39.07011032104492,
      "learning_rate": 1.2999143835616439e-05,
      "loss": 0.406,
      "step": 6620
    },
    {
      "epoch": 2.8381849315068495,
      "grad_norm": 19.41545295715332,
      "learning_rate": 1.297345890410959e-05,
      "loss": 0.3937,
      "step": 6630
    },
    {
      "epoch": 2.8424657534246576,
      "grad_norm": 17.39725112915039,
      "learning_rate": 1.294777397260274e-05,
      "loss": 0.7544,
      "step": 6640
    },
    {
      "epoch": 2.8467465753424657,
      "grad_norm": 12.125307083129883,
      "learning_rate": 1.2922089041095892e-05,
      "loss": 0.4436,
      "step": 6650
    },
    {
      "epoch": 2.8510273972602738,
      "grad_norm": 16.698118209838867,
      "learning_rate": 1.2896404109589041e-05,
      "loss": 0.3357,
      "step": 6660
    },
    {
      "epoch": 2.8553082191780823,
      "grad_norm": 17.360824584960938,
      "learning_rate": 1.2870719178082191e-05,
      "loss": 0.5837,
      "step": 6670
    },
    {
      "epoch": 2.8595890410958904,
      "grad_norm": 1.2292284965515137,
      "learning_rate": 1.2845034246575343e-05,
      "loss": 0.4396,
      "step": 6680
    },
    {
      "epoch": 2.8638698630136985,
      "grad_norm": 11.362168312072754,
      "learning_rate": 1.2819349315068492e-05,
      "loss": 0.3662,
      "step": 6690
    },
    {
      "epoch": 2.868150684931507,
      "grad_norm": 9.516593933105469,
      "learning_rate": 1.2793664383561644e-05,
      "loss": 0.2031,
      "step": 6700
    },
    {
      "epoch": 2.872431506849315,
      "grad_norm": 13.51259994506836,
      "learning_rate": 1.2767979452054794e-05,
      "loss": 0.2392,
      "step": 6710
    },
    {
      "epoch": 2.8767123287671232,
      "grad_norm": 1.1329867839813232,
      "learning_rate": 1.2742294520547945e-05,
      "loss": 0.3997,
      "step": 6720
    },
    {
      "epoch": 2.8809931506849313,
      "grad_norm": 6.9017653465271,
      "learning_rate": 1.2716609589041095e-05,
      "loss": 0.2701,
      "step": 6730
    },
    {
      "epoch": 2.88527397260274,
      "grad_norm": 4.315032005310059,
      "learning_rate": 1.2690924657534246e-05,
      "loss": 0.3739,
      "step": 6740
    },
    {
      "epoch": 2.889554794520548,
      "grad_norm": 7.0909600257873535,
      "learning_rate": 1.2665239726027398e-05,
      "loss": 0.2293,
      "step": 6750
    },
    {
      "epoch": 2.893835616438356,
      "grad_norm": 7.25237512588501,
      "learning_rate": 1.2639554794520548e-05,
      "loss": 0.3554,
      "step": 6760
    },
    {
      "epoch": 2.8981164383561646,
      "grad_norm": 5.144253730773926,
      "learning_rate": 1.26138698630137e-05,
      "loss": 0.3319,
      "step": 6770
    },
    {
      "epoch": 2.9023972602739727,
      "grad_norm": 10.541754722595215,
      "learning_rate": 1.2588184931506849e-05,
      "loss": 0.2834,
      "step": 6780
    },
    {
      "epoch": 2.906678082191781,
      "grad_norm": 349.6491394042969,
      "learning_rate": 1.25625e-05,
      "loss": 0.3183,
      "step": 6790
    },
    {
      "epoch": 2.910958904109589,
      "grad_norm": 23.919036865234375,
      "learning_rate": 1.253681506849315e-05,
      "loss": 0.5665,
      "step": 6800
    },
    {
      "epoch": 2.915239726027397,
      "grad_norm": 30.442419052124023,
      "learning_rate": 1.2511130136986302e-05,
      "loss": 0.3193,
      "step": 6810
    },
    {
      "epoch": 2.9195205479452055,
      "grad_norm": 4.340581893920898,
      "learning_rate": 1.2485445205479451e-05,
      "loss": 0.4151,
      "step": 6820
    },
    {
      "epoch": 2.9238013698630136,
      "grad_norm": 25.994104385375977,
      "learning_rate": 1.2459760273972603e-05,
      "loss": 0.4564,
      "step": 6830
    },
    {
      "epoch": 2.928082191780822,
      "grad_norm": 64.38519287109375,
      "learning_rate": 1.2434075342465754e-05,
      "loss": 0.4864,
      "step": 6840
    },
    {
      "epoch": 2.9323630136986303,
      "grad_norm": 17.231943130493164,
      "learning_rate": 1.2408390410958904e-05,
      "loss": 0.4898,
      "step": 6850
    },
    {
      "epoch": 2.9366438356164384,
      "grad_norm": 3.990678310394287,
      "learning_rate": 1.2382705479452056e-05,
      "loss": 0.2025,
      "step": 6860
    },
    {
      "epoch": 2.9409246575342465,
      "grad_norm": 7.416327953338623,
      "learning_rate": 1.2357020547945205e-05,
      "loss": 0.3813,
      "step": 6870
    },
    {
      "epoch": 2.9452054794520546,
      "grad_norm": 16.415189743041992,
      "learning_rate": 1.2331335616438357e-05,
      "loss": 0.5509,
      "step": 6880
    },
    {
      "epoch": 2.949486301369863,
      "grad_norm": 0.2791307270526886,
      "learning_rate": 1.2305650684931507e-05,
      "loss": 0.3133,
      "step": 6890
    },
    {
      "epoch": 2.953767123287671,
      "grad_norm": 9.656376838684082,
      "learning_rate": 1.2279965753424658e-05,
      "loss": 0.5342,
      "step": 6900
    },
    {
      "epoch": 2.9580479452054793,
      "grad_norm": 13.278460502624512,
      "learning_rate": 1.225428082191781e-05,
      "loss": 0.3831,
      "step": 6910
    },
    {
      "epoch": 2.962328767123288,
      "grad_norm": 1.9387692213058472,
      "learning_rate": 1.222859589041096e-05,
      "loss": 0.3702,
      "step": 6920
    },
    {
      "epoch": 2.966609589041096,
      "grad_norm": 0.4600253701210022,
      "learning_rate": 1.2202910958904111e-05,
      "loss": 0.2236,
      "step": 6930
    },
    {
      "epoch": 2.970890410958904,
      "grad_norm": 11.301372528076172,
      "learning_rate": 1.217722602739726e-05,
      "loss": 0.2348,
      "step": 6940
    },
    {
      "epoch": 2.975171232876712,
      "grad_norm": 17.38274383544922,
      "learning_rate": 1.2151541095890412e-05,
      "loss": 0.4951,
      "step": 6950
    },
    {
      "epoch": 2.9794520547945207,
      "grad_norm": 4.354271411895752,
      "learning_rate": 1.2125856164383562e-05,
      "loss": 0.6209,
      "step": 6960
    },
    {
      "epoch": 2.983732876712329,
      "grad_norm": 11.591288566589355,
      "learning_rate": 1.2100171232876713e-05,
      "loss": 0.2922,
      "step": 6970
    },
    {
      "epoch": 2.988013698630137,
      "grad_norm": 6.630304336547852,
      "learning_rate": 1.2074486301369863e-05,
      "loss": 0.46,
      "step": 6980
    },
    {
      "epoch": 2.9922945205479454,
      "grad_norm": 10.017501831054688,
      "learning_rate": 1.2048801369863015e-05,
      "loss": 0.4073,
      "step": 6990
    },
    {
      "epoch": 2.9965753424657535,
      "grad_norm": 8.010895729064941,
      "learning_rate": 1.2023116438356166e-05,
      "loss": 0.2658,
      "step": 7000
    },
    {
      "epoch": 3.0,
      "eval_f1": 0.691335357376561,
      "eval_loss": 0.6322787404060364,
      "eval_precision": 0.689940389645045,
      "eval_recall": 0.702699212191657,
      "eval_runtime": 536.5879,
      "eval_samples_per_second": 8.33,
      "eval_steps_per_second": 1.042,
      "step": 7008
    }
  ],
  "logging_steps": 10,
  "max_steps": 11680,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 310387116897792.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
