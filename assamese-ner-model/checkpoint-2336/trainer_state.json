{
  "best_global_step": 2336,
  "best_metric": 0.7885168194770813,
  "best_model_checkpoint": "./assamese-ner-model\\checkpoint-2336",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 2336,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004280821917808219,
      "grad_norm": 9.143527030944824,
      "learning_rate": 2.9976883561643835e-05,
      "loss": 3.2571,
      "step": 10
    },
    {
      "epoch": 0.008561643835616438,
      "grad_norm": 7.429162979125977,
      "learning_rate": 2.9951198630136988e-05,
      "loss": 2.773,
      "step": 20
    },
    {
      "epoch": 0.012842465753424657,
      "grad_norm": 9.300058364868164,
      "learning_rate": 2.9925513698630138e-05,
      "loss": 2.5513,
      "step": 30
    },
    {
      "epoch": 0.017123287671232876,
      "grad_norm": 11.095643043518066,
      "learning_rate": 2.9899828767123288e-05,
      "loss": 2.2438,
      "step": 40
    },
    {
      "epoch": 0.021404109589041095,
      "grad_norm": 5.4018874168396,
      "learning_rate": 2.9874143835616437e-05,
      "loss": 2.0131,
      "step": 50
    },
    {
      "epoch": 0.025684931506849314,
      "grad_norm": 6.561296463012695,
      "learning_rate": 2.984845890410959e-05,
      "loss": 1.9834,
      "step": 60
    },
    {
      "epoch": 0.029965753424657533,
      "grad_norm": 9.898076057434082,
      "learning_rate": 2.982277397260274e-05,
      "loss": 1.8944,
      "step": 70
    },
    {
      "epoch": 0.03424657534246575,
      "grad_norm": 5.873605251312256,
      "learning_rate": 2.979708904109589e-05,
      "loss": 2.1073,
      "step": 80
    },
    {
      "epoch": 0.038527397260273974,
      "grad_norm": 9.62494945526123,
      "learning_rate": 2.977140410958904e-05,
      "loss": 1.642,
      "step": 90
    },
    {
      "epoch": 0.04280821917808219,
      "grad_norm": 7.739095687866211,
      "learning_rate": 2.9745719178082193e-05,
      "loss": 1.8328,
      "step": 100
    },
    {
      "epoch": 0.04708904109589041,
      "grad_norm": 10.027473449707031,
      "learning_rate": 2.9720034246575343e-05,
      "loss": 1.5973,
      "step": 110
    },
    {
      "epoch": 0.05136986301369863,
      "grad_norm": 9.248682975769043,
      "learning_rate": 2.9694349315068496e-05,
      "loss": 1.3256,
      "step": 120
    },
    {
      "epoch": 0.05565068493150685,
      "grad_norm": 11.150430679321289,
      "learning_rate": 2.9668664383561642e-05,
      "loss": 1.8203,
      "step": 130
    },
    {
      "epoch": 0.059931506849315065,
      "grad_norm": 7.573376178741455,
      "learning_rate": 2.9642979452054796e-05,
      "loss": 1.7003,
      "step": 140
    },
    {
      "epoch": 0.0642123287671233,
      "grad_norm": 27.757863998413086,
      "learning_rate": 2.9617294520547945e-05,
      "loss": 1.4981,
      "step": 150
    },
    {
      "epoch": 0.0684931506849315,
      "grad_norm": 3.9192657470703125,
      "learning_rate": 2.95916095890411e-05,
      "loss": 1.3755,
      "step": 160
    },
    {
      "epoch": 0.07277397260273973,
      "grad_norm": 15.962122917175293,
      "learning_rate": 2.9565924657534245e-05,
      "loss": 1.3564,
      "step": 170
    },
    {
      "epoch": 0.07705479452054795,
      "grad_norm": 6.152198791503906,
      "learning_rate": 2.9540239726027398e-05,
      "loss": 1.4281,
      "step": 180
    },
    {
      "epoch": 0.08133561643835617,
      "grad_norm": 12.189838409423828,
      "learning_rate": 2.9514554794520548e-05,
      "loss": 1.7825,
      "step": 190
    },
    {
      "epoch": 0.08561643835616438,
      "grad_norm": 15.862617492675781,
      "learning_rate": 2.94888698630137e-05,
      "loss": 1.4193,
      "step": 200
    },
    {
      "epoch": 0.0898972602739726,
      "grad_norm": 10.695959091186523,
      "learning_rate": 2.946318493150685e-05,
      "loss": 1.3413,
      "step": 210
    },
    {
      "epoch": 0.09417808219178082,
      "grad_norm": 8.591190338134766,
      "learning_rate": 2.94375e-05,
      "loss": 1.4734,
      "step": 220
    },
    {
      "epoch": 0.09845890410958905,
      "grad_norm": 23.265018463134766,
      "learning_rate": 2.941181506849315e-05,
      "loss": 1.3237,
      "step": 230
    },
    {
      "epoch": 0.10273972602739725,
      "grad_norm": 12.896937370300293,
      "learning_rate": 2.9386130136986304e-05,
      "loss": 1.281,
      "step": 240
    },
    {
      "epoch": 0.10702054794520548,
      "grad_norm": 7.430508613586426,
      "learning_rate": 2.9360445205479453e-05,
      "loss": 1.5752,
      "step": 250
    },
    {
      "epoch": 0.1113013698630137,
      "grad_norm": 9.0586519241333,
      "learning_rate": 2.9334760273972607e-05,
      "loss": 1.1468,
      "step": 260
    },
    {
      "epoch": 0.11558219178082192,
      "grad_norm": 9.041784286499023,
      "learning_rate": 2.9309075342465753e-05,
      "loss": 1.3621,
      "step": 270
    },
    {
      "epoch": 0.11986301369863013,
      "grad_norm": 17.757610321044922,
      "learning_rate": 2.9283390410958906e-05,
      "loss": 1.3347,
      "step": 280
    },
    {
      "epoch": 0.12414383561643835,
      "grad_norm": 6.512852668762207,
      "learning_rate": 2.9257705479452056e-05,
      "loss": 1.2847,
      "step": 290
    },
    {
      "epoch": 0.1284246575342466,
      "grad_norm": 11.371417999267578,
      "learning_rate": 2.923202054794521e-05,
      "loss": 1.3513,
      "step": 300
    },
    {
      "epoch": 0.1327054794520548,
      "grad_norm": 10.900997161865234,
      "learning_rate": 2.9206335616438355e-05,
      "loss": 1.3872,
      "step": 310
    },
    {
      "epoch": 0.136986301369863,
      "grad_norm": 25.528583526611328,
      "learning_rate": 2.9180650684931505e-05,
      "loss": 0.9956,
      "step": 320
    },
    {
      "epoch": 0.14126712328767124,
      "grad_norm": 11.505348205566406,
      "learning_rate": 2.915496575342466e-05,
      "loss": 1.3819,
      "step": 330
    },
    {
      "epoch": 0.14554794520547945,
      "grad_norm": 11.744062423706055,
      "learning_rate": 2.9129280821917808e-05,
      "loss": 1.1022,
      "step": 340
    },
    {
      "epoch": 0.14982876712328766,
      "grad_norm": 10.921093940734863,
      "learning_rate": 2.910359589041096e-05,
      "loss": 1.2909,
      "step": 350
    },
    {
      "epoch": 0.1541095890410959,
      "grad_norm": 28.27537727355957,
      "learning_rate": 2.9077910958904108e-05,
      "loss": 1.1084,
      "step": 360
    },
    {
      "epoch": 0.1583904109589041,
      "grad_norm": 14.745160102844238,
      "learning_rate": 2.905222602739726e-05,
      "loss": 1.3607,
      "step": 370
    },
    {
      "epoch": 0.16267123287671234,
      "grad_norm": 9.820703506469727,
      "learning_rate": 2.902654109589041e-05,
      "loss": 1.144,
      "step": 380
    },
    {
      "epoch": 0.16695205479452055,
      "grad_norm": 20.382030487060547,
      "learning_rate": 2.9000856164383564e-05,
      "loss": 1.1846,
      "step": 390
    },
    {
      "epoch": 0.17123287671232876,
      "grad_norm": 11.614411354064941,
      "learning_rate": 2.897517123287671e-05,
      "loss": 1.1864,
      "step": 400
    },
    {
      "epoch": 0.175513698630137,
      "grad_norm": 18.76705551147461,
      "learning_rate": 2.8949486301369863e-05,
      "loss": 1.229,
      "step": 410
    },
    {
      "epoch": 0.1797945205479452,
      "grad_norm": 14.084985733032227,
      "learning_rate": 2.8923801369863013e-05,
      "loss": 0.9564,
      "step": 420
    },
    {
      "epoch": 0.1840753424657534,
      "grad_norm": 15.900201797485352,
      "learning_rate": 2.8898116438356166e-05,
      "loss": 1.0985,
      "step": 430
    },
    {
      "epoch": 0.18835616438356165,
      "grad_norm": 13.179973602294922,
      "learning_rate": 2.8872431506849316e-05,
      "loss": 0.9747,
      "step": 440
    },
    {
      "epoch": 0.19263698630136986,
      "grad_norm": 26.49134635925293,
      "learning_rate": 2.8846746575342466e-05,
      "loss": 1.2553,
      "step": 450
    },
    {
      "epoch": 0.1969178082191781,
      "grad_norm": 20.5519962310791,
      "learning_rate": 2.8821061643835616e-05,
      "loss": 0.8543,
      "step": 460
    },
    {
      "epoch": 0.2011986301369863,
      "grad_norm": 21.370920181274414,
      "learning_rate": 2.879537671232877e-05,
      "loss": 1.1098,
      "step": 470
    },
    {
      "epoch": 0.2054794520547945,
      "grad_norm": 15.452706336975098,
      "learning_rate": 2.876969178082192e-05,
      "loss": 1.1207,
      "step": 480
    },
    {
      "epoch": 0.20976027397260275,
      "grad_norm": 41.02529525756836,
      "learning_rate": 2.874400684931507e-05,
      "loss": 0.8128,
      "step": 490
    },
    {
      "epoch": 0.21404109589041095,
      "grad_norm": 23.58036231994629,
      "learning_rate": 2.8718321917808218e-05,
      "loss": 1.0838,
      "step": 500
    },
    {
      "epoch": 0.2183219178082192,
      "grad_norm": 17.155776977539062,
      "learning_rate": 2.869263698630137e-05,
      "loss": 0.8445,
      "step": 510
    },
    {
      "epoch": 0.2226027397260274,
      "grad_norm": 15.116822242736816,
      "learning_rate": 2.866695205479452e-05,
      "loss": 1.2592,
      "step": 520
    },
    {
      "epoch": 0.2268835616438356,
      "grad_norm": 6.698291301727295,
      "learning_rate": 2.8641267123287674e-05,
      "loss": 1.0454,
      "step": 530
    },
    {
      "epoch": 0.23116438356164384,
      "grad_norm": 27.467140197753906,
      "learning_rate": 2.861558219178082e-05,
      "loss": 0.8242,
      "step": 540
    },
    {
      "epoch": 0.23544520547945205,
      "grad_norm": 16.7839412689209,
      "learning_rate": 2.8589897260273974e-05,
      "loss": 1.0546,
      "step": 550
    },
    {
      "epoch": 0.23972602739726026,
      "grad_norm": 8.238468170166016,
      "learning_rate": 2.8564212328767124e-05,
      "loss": 0.988,
      "step": 560
    },
    {
      "epoch": 0.2440068493150685,
      "grad_norm": 22.661073684692383,
      "learning_rate": 2.8538527397260277e-05,
      "loss": 0.9039,
      "step": 570
    },
    {
      "epoch": 0.2482876712328767,
      "grad_norm": 15.265829086303711,
      "learning_rate": 2.8512842465753427e-05,
      "loss": 1.0021,
      "step": 580
    },
    {
      "epoch": 0.2525684931506849,
      "grad_norm": 49.56181335449219,
      "learning_rate": 2.8487157534246576e-05,
      "loss": 1.146,
      "step": 590
    },
    {
      "epoch": 0.2568493150684932,
      "grad_norm": 14.604925155639648,
      "learning_rate": 2.8461472602739726e-05,
      "loss": 0.9398,
      "step": 600
    },
    {
      "epoch": 0.2611301369863014,
      "grad_norm": 8.990811347961426,
      "learning_rate": 2.843578767123288e-05,
      "loss": 0.7949,
      "step": 610
    },
    {
      "epoch": 0.2654109589041096,
      "grad_norm": 24.059226989746094,
      "learning_rate": 2.841010273972603e-05,
      "loss": 1.1607,
      "step": 620
    },
    {
      "epoch": 0.2696917808219178,
      "grad_norm": 8.618311882019043,
      "learning_rate": 2.838441780821918e-05,
      "loss": 1.0555,
      "step": 630
    },
    {
      "epoch": 0.273972602739726,
      "grad_norm": 9.342193603515625,
      "learning_rate": 2.835873287671233e-05,
      "loss": 1.0055,
      "step": 640
    },
    {
      "epoch": 0.2782534246575342,
      "grad_norm": 11.598215103149414,
      "learning_rate": 2.8333047945205482e-05,
      "loss": 1.1484,
      "step": 650
    },
    {
      "epoch": 0.2825342465753425,
      "grad_norm": 25.86655616760254,
      "learning_rate": 2.8307363013698632e-05,
      "loss": 0.9345,
      "step": 660
    },
    {
      "epoch": 0.2868150684931507,
      "grad_norm": 29.468690872192383,
      "learning_rate": 2.828167808219178e-05,
      "loss": 1.0117,
      "step": 670
    },
    {
      "epoch": 0.2910958904109589,
      "grad_norm": 21.54947280883789,
      "learning_rate": 2.825599315068493e-05,
      "loss": 1.032,
      "step": 680
    },
    {
      "epoch": 0.2953767123287671,
      "grad_norm": 13.336138725280762,
      "learning_rate": 2.823030821917808e-05,
      "loss": 1.0047,
      "step": 690
    },
    {
      "epoch": 0.2996575342465753,
      "grad_norm": 36.05213165283203,
      "learning_rate": 2.8204623287671234e-05,
      "loss": 1.0083,
      "step": 700
    },
    {
      "epoch": 0.3039383561643836,
      "grad_norm": 36.2824821472168,
      "learning_rate": 2.8178938356164384e-05,
      "loss": 0.8792,
      "step": 710
    },
    {
      "epoch": 0.3082191780821918,
      "grad_norm": 15.812623977661133,
      "learning_rate": 2.8153253424657534e-05,
      "loss": 0.9901,
      "step": 720
    },
    {
      "epoch": 0.3125,
      "grad_norm": 13.622113227844238,
      "learning_rate": 2.8127568493150684e-05,
      "loss": 1.08,
      "step": 730
    },
    {
      "epoch": 0.3167808219178082,
      "grad_norm": 19.393569946289062,
      "learning_rate": 2.8101883561643837e-05,
      "loss": 0.8416,
      "step": 740
    },
    {
      "epoch": 0.3210616438356164,
      "grad_norm": 9.216547966003418,
      "learning_rate": 2.8076198630136987e-05,
      "loss": 0.9563,
      "step": 750
    },
    {
      "epoch": 0.3253424657534247,
      "grad_norm": 64.56256103515625,
      "learning_rate": 2.805051369863014e-05,
      "loss": 0.9758,
      "step": 760
    },
    {
      "epoch": 0.3296232876712329,
      "grad_norm": 16.644256591796875,
      "learning_rate": 2.8024828767123286e-05,
      "loss": 0.9189,
      "step": 770
    },
    {
      "epoch": 0.3339041095890411,
      "grad_norm": 22.02782440185547,
      "learning_rate": 2.799914383561644e-05,
      "loss": 1.0309,
      "step": 780
    },
    {
      "epoch": 0.3381849315068493,
      "grad_norm": 14.417555809020996,
      "learning_rate": 2.797345890410959e-05,
      "loss": 1.0168,
      "step": 790
    },
    {
      "epoch": 0.3424657534246575,
      "grad_norm": 17.383155822753906,
      "learning_rate": 2.7947773972602742e-05,
      "loss": 0.8795,
      "step": 800
    },
    {
      "epoch": 0.3467465753424658,
      "grad_norm": 16.478063583374023,
      "learning_rate": 2.792208904109589e-05,
      "loss": 0.9981,
      "step": 810
    },
    {
      "epoch": 0.351027397260274,
      "grad_norm": 46.852333068847656,
      "learning_rate": 2.7896404109589042e-05,
      "loss": 0.8396,
      "step": 820
    },
    {
      "epoch": 0.3553082191780822,
      "grad_norm": 16.336999893188477,
      "learning_rate": 2.787071917808219e-05,
      "loss": 0.8869,
      "step": 830
    },
    {
      "epoch": 0.3595890410958904,
      "grad_norm": 18.43584442138672,
      "learning_rate": 2.7845034246575345e-05,
      "loss": 1.0838,
      "step": 840
    },
    {
      "epoch": 0.3638698630136986,
      "grad_norm": 27.185283660888672,
      "learning_rate": 2.7819349315068495e-05,
      "loss": 0.7127,
      "step": 850
    },
    {
      "epoch": 0.3681506849315068,
      "grad_norm": 9.711454391479492,
      "learning_rate": 2.7793664383561644e-05,
      "loss": 0.6737,
      "step": 860
    },
    {
      "epoch": 0.3724315068493151,
      "grad_norm": 18.34222412109375,
      "learning_rate": 2.7767979452054794e-05,
      "loss": 1.108,
      "step": 870
    },
    {
      "epoch": 0.3767123287671233,
      "grad_norm": 9.437512397766113,
      "learning_rate": 2.7742294520547947e-05,
      "loss": 0.9617,
      "step": 880
    },
    {
      "epoch": 0.3809931506849315,
      "grad_norm": 6.7784905433654785,
      "learning_rate": 2.7716609589041097e-05,
      "loss": 0.894,
      "step": 890
    },
    {
      "epoch": 0.3852739726027397,
      "grad_norm": 14.5828218460083,
      "learning_rate": 2.7690924657534247e-05,
      "loss": 0.8916,
      "step": 900
    },
    {
      "epoch": 0.3895547945205479,
      "grad_norm": 13.952213287353516,
      "learning_rate": 2.7665239726027397e-05,
      "loss": 1.2107,
      "step": 910
    },
    {
      "epoch": 0.3938356164383562,
      "grad_norm": 23.894481658935547,
      "learning_rate": 2.763955479452055e-05,
      "loss": 1.0384,
      "step": 920
    },
    {
      "epoch": 0.3981164383561644,
      "grad_norm": 12.81005573272705,
      "learning_rate": 2.76138698630137e-05,
      "loss": 0.8794,
      "step": 930
    },
    {
      "epoch": 0.4023972602739726,
      "grad_norm": 11.605198860168457,
      "learning_rate": 2.7588184931506853e-05,
      "loss": 1.0815,
      "step": 940
    },
    {
      "epoch": 0.4066780821917808,
      "grad_norm": 19.173080444335938,
      "learning_rate": 2.75625e-05,
      "loss": 0.879,
      "step": 950
    },
    {
      "epoch": 0.410958904109589,
      "grad_norm": 16.12763786315918,
      "learning_rate": 2.7536815068493152e-05,
      "loss": 1.2316,
      "step": 960
    },
    {
      "epoch": 0.4152397260273973,
      "grad_norm": 11.04924488067627,
      "learning_rate": 2.7511130136986302e-05,
      "loss": 1.0372,
      "step": 970
    },
    {
      "epoch": 0.4195205479452055,
      "grad_norm": 12.3121337890625,
      "learning_rate": 2.7485445205479455e-05,
      "loss": 0.8087,
      "step": 980
    },
    {
      "epoch": 0.4238013698630137,
      "grad_norm": 31.46891212463379,
      "learning_rate": 2.7459760273972605e-05,
      "loss": 0.9003,
      "step": 990
    },
    {
      "epoch": 0.4280821917808219,
      "grad_norm": 12.546882629394531,
      "learning_rate": 2.7434075342465755e-05,
      "loss": 1.07,
      "step": 1000
    },
    {
      "epoch": 0.4323630136986301,
      "grad_norm": 8.71003532409668,
      "learning_rate": 2.7408390410958905e-05,
      "loss": 0.9106,
      "step": 1010
    },
    {
      "epoch": 0.4366438356164384,
      "grad_norm": 42.478759765625,
      "learning_rate": 2.7382705479452054e-05,
      "loss": 0.8865,
      "step": 1020
    },
    {
      "epoch": 0.4409246575342466,
      "grad_norm": 15.516157150268555,
      "learning_rate": 2.7357020547945208e-05,
      "loss": 1.078,
      "step": 1030
    },
    {
      "epoch": 0.4452054794520548,
      "grad_norm": 20.25251579284668,
      "learning_rate": 2.7331335616438354e-05,
      "loss": 0.9342,
      "step": 1040
    },
    {
      "epoch": 0.449486301369863,
      "grad_norm": 5.450655937194824,
      "learning_rate": 2.7305650684931507e-05,
      "loss": 0.7869,
      "step": 1050
    },
    {
      "epoch": 0.4537671232876712,
      "grad_norm": 26.11355972290039,
      "learning_rate": 2.7279965753424657e-05,
      "loss": 1.1147,
      "step": 1060
    },
    {
      "epoch": 0.4580479452054795,
      "grad_norm": 11.541396141052246,
      "learning_rate": 2.725428082191781e-05,
      "loss": 0.9773,
      "step": 1070
    },
    {
      "epoch": 0.4623287671232877,
      "grad_norm": 9.537392616271973,
      "learning_rate": 2.722859589041096e-05,
      "loss": 0.6934,
      "step": 1080
    },
    {
      "epoch": 0.4666095890410959,
      "grad_norm": 22.873441696166992,
      "learning_rate": 2.720291095890411e-05,
      "loss": 0.9617,
      "step": 1090
    },
    {
      "epoch": 0.4708904109589041,
      "grad_norm": 10.116670608520508,
      "learning_rate": 2.717722602739726e-05,
      "loss": 0.9711,
      "step": 1100
    },
    {
      "epoch": 0.4751712328767123,
      "grad_norm": 20.031970977783203,
      "learning_rate": 2.7151541095890413e-05,
      "loss": 1.0581,
      "step": 1110
    },
    {
      "epoch": 0.4794520547945205,
      "grad_norm": 19.42740821838379,
      "learning_rate": 2.7125856164383562e-05,
      "loss": 0.8627,
      "step": 1120
    },
    {
      "epoch": 0.4837328767123288,
      "grad_norm": 12.646263122558594,
      "learning_rate": 2.7100171232876712e-05,
      "loss": 0.8751,
      "step": 1130
    },
    {
      "epoch": 0.488013698630137,
      "grad_norm": 4.523874759674072,
      "learning_rate": 2.7074486301369862e-05,
      "loss": 0.8219,
      "step": 1140
    },
    {
      "epoch": 0.4922945205479452,
      "grad_norm": 13.249631881713867,
      "learning_rate": 2.7048801369863015e-05,
      "loss": 0.7833,
      "step": 1150
    },
    {
      "epoch": 0.4965753424657534,
      "grad_norm": 7.592014789581299,
      "learning_rate": 2.7023116438356165e-05,
      "loss": 0.8157,
      "step": 1160
    },
    {
      "epoch": 0.5008561643835616,
      "grad_norm": 34.3780403137207,
      "learning_rate": 2.6997431506849318e-05,
      "loss": 0.9072,
      "step": 1170
    },
    {
      "epoch": 0.5051369863013698,
      "grad_norm": 30.691129684448242,
      "learning_rate": 2.6971746575342464e-05,
      "loss": 0.6879,
      "step": 1180
    },
    {
      "epoch": 0.509417808219178,
      "grad_norm": 6.798182487487793,
      "learning_rate": 2.6946061643835618e-05,
      "loss": 0.8668,
      "step": 1190
    },
    {
      "epoch": 0.5136986301369864,
      "grad_norm": 39.035675048828125,
      "learning_rate": 2.6920376712328767e-05,
      "loss": 0.8607,
      "step": 1200
    },
    {
      "epoch": 0.5179794520547946,
      "grad_norm": 14.542919158935547,
      "learning_rate": 2.689469178082192e-05,
      "loss": 0.811,
      "step": 1210
    },
    {
      "epoch": 0.5222602739726028,
      "grad_norm": 8.363486289978027,
      "learning_rate": 2.6869006849315067e-05,
      "loss": 0.6759,
      "step": 1220
    },
    {
      "epoch": 0.526541095890411,
      "grad_norm": 6.7858123779296875,
      "learning_rate": 2.684332191780822e-05,
      "loss": 0.6917,
      "step": 1230
    },
    {
      "epoch": 0.5308219178082192,
      "grad_norm": 32.5490837097168,
      "learning_rate": 2.681763698630137e-05,
      "loss": 0.9429,
      "step": 1240
    },
    {
      "epoch": 0.5351027397260274,
      "grad_norm": 6.6626458168029785,
      "learning_rate": 2.6791952054794523e-05,
      "loss": 0.8783,
      "step": 1250
    },
    {
      "epoch": 0.5393835616438356,
      "grad_norm": 14.975225448608398,
      "learning_rate": 2.6766267123287673e-05,
      "loss": 0.7047,
      "step": 1260
    },
    {
      "epoch": 0.5436643835616438,
      "grad_norm": 13.297146797180176,
      "learning_rate": 2.6740582191780823e-05,
      "loss": 0.8957,
      "step": 1270
    },
    {
      "epoch": 0.547945205479452,
      "grad_norm": 12.45203685760498,
      "learning_rate": 2.6714897260273972e-05,
      "loss": 0.88,
      "step": 1280
    },
    {
      "epoch": 0.5522260273972602,
      "grad_norm": 4.13766622543335,
      "learning_rate": 2.6689212328767126e-05,
      "loss": 0.6168,
      "step": 1290
    },
    {
      "epoch": 0.5565068493150684,
      "grad_norm": 45.81571578979492,
      "learning_rate": 2.6663527397260275e-05,
      "loss": 1.0284,
      "step": 1300
    },
    {
      "epoch": 0.5607876712328768,
      "grad_norm": 8.625859260559082,
      "learning_rate": 2.663784246575343e-05,
      "loss": 0.5634,
      "step": 1310
    },
    {
      "epoch": 0.565068493150685,
      "grad_norm": 20.825454711914062,
      "learning_rate": 2.6612157534246575e-05,
      "loss": 0.9129,
      "step": 1320
    },
    {
      "epoch": 0.5693493150684932,
      "grad_norm": 12.970701217651367,
      "learning_rate": 2.6586472602739728e-05,
      "loss": 0.686,
      "step": 1330
    },
    {
      "epoch": 0.5736301369863014,
      "grad_norm": 21.17759895324707,
      "learning_rate": 2.6560787671232878e-05,
      "loss": 1.027,
      "step": 1340
    },
    {
      "epoch": 0.5779109589041096,
      "grad_norm": 4.656286716461182,
      "learning_rate": 2.6535102739726028e-05,
      "loss": 0.7063,
      "step": 1350
    },
    {
      "epoch": 0.5821917808219178,
      "grad_norm": 11.171497344970703,
      "learning_rate": 2.6509417808219177e-05,
      "loss": 0.8304,
      "step": 1360
    },
    {
      "epoch": 0.586472602739726,
      "grad_norm": 13.476147651672363,
      "learning_rate": 2.6483732876712327e-05,
      "loss": 0.7401,
      "step": 1370
    },
    {
      "epoch": 0.5907534246575342,
      "grad_norm": 11.683135032653809,
      "learning_rate": 2.645804794520548e-05,
      "loss": 0.8161,
      "step": 1380
    },
    {
      "epoch": 0.5950342465753424,
      "grad_norm": 7.547550678253174,
      "learning_rate": 2.643236301369863e-05,
      "loss": 0.8678,
      "step": 1390
    },
    {
      "epoch": 0.5993150684931506,
      "grad_norm": 4.967058181762695,
      "learning_rate": 2.6406678082191783e-05,
      "loss": 0.5675,
      "step": 1400
    },
    {
      "epoch": 0.603595890410959,
      "grad_norm": 8.383954048156738,
      "learning_rate": 2.638099315068493e-05,
      "loss": 0.7366,
      "step": 1410
    },
    {
      "epoch": 0.6078767123287672,
      "grad_norm": 3.407871723175049,
      "learning_rate": 2.6355308219178083e-05,
      "loss": 0.8091,
      "step": 1420
    },
    {
      "epoch": 0.6121575342465754,
      "grad_norm": 3.6211371421813965,
      "learning_rate": 2.6329623287671233e-05,
      "loss": 0.5425,
      "step": 1430
    },
    {
      "epoch": 0.6164383561643836,
      "grad_norm": 8.027620315551758,
      "learning_rate": 2.6303938356164386e-05,
      "loss": 1.2665,
      "step": 1440
    },
    {
      "epoch": 0.6207191780821918,
      "grad_norm": 10.842984199523926,
      "learning_rate": 2.6278253424657532e-05,
      "loss": 0.8933,
      "step": 1450
    },
    {
      "epoch": 0.625,
      "grad_norm": 17.257038116455078,
      "learning_rate": 2.6252568493150685e-05,
      "loss": 0.9975,
      "step": 1460
    },
    {
      "epoch": 0.6292808219178082,
      "grad_norm": 8.613531112670898,
      "learning_rate": 2.6226883561643835e-05,
      "loss": 0.9868,
      "step": 1470
    },
    {
      "epoch": 0.6335616438356164,
      "grad_norm": 12.613505363464355,
      "learning_rate": 2.620119863013699e-05,
      "loss": 0.8849,
      "step": 1480
    },
    {
      "epoch": 0.6378424657534246,
      "grad_norm": 15.471649169921875,
      "learning_rate": 2.6175513698630138e-05,
      "loss": 0.7888,
      "step": 1490
    },
    {
      "epoch": 0.6421232876712328,
      "grad_norm": 9.704048156738281,
      "learning_rate": 2.6149828767123288e-05,
      "loss": 0.6014,
      "step": 1500
    },
    {
      "epoch": 0.646404109589041,
      "grad_norm": 18.230037689208984,
      "learning_rate": 2.6124143835616438e-05,
      "loss": 0.7973,
      "step": 1510
    },
    {
      "epoch": 0.6506849315068494,
      "grad_norm": 13.136951446533203,
      "learning_rate": 2.609845890410959e-05,
      "loss": 0.54,
      "step": 1520
    },
    {
      "epoch": 0.6549657534246576,
      "grad_norm": 37.58016586303711,
      "learning_rate": 2.607277397260274e-05,
      "loss": 0.9804,
      "step": 1530
    },
    {
      "epoch": 0.6592465753424658,
      "grad_norm": 11.665778160095215,
      "learning_rate": 2.604708904109589e-05,
      "loss": 0.7955,
      "step": 1540
    },
    {
      "epoch": 0.663527397260274,
      "grad_norm": 15.988519668579102,
      "learning_rate": 2.602140410958904e-05,
      "loss": 1.2159,
      "step": 1550
    },
    {
      "epoch": 0.6678082191780822,
      "grad_norm": 12.905730247497559,
      "learning_rate": 2.5995719178082193e-05,
      "loss": 0.9387,
      "step": 1560
    },
    {
      "epoch": 0.6720890410958904,
      "grad_norm": 5.530625343322754,
      "learning_rate": 2.5970034246575343e-05,
      "loss": 0.7292,
      "step": 1570
    },
    {
      "epoch": 0.6763698630136986,
      "grad_norm": 24.208515167236328,
      "learning_rate": 2.5944349315068496e-05,
      "loss": 0.7722,
      "step": 1580
    },
    {
      "epoch": 0.6806506849315068,
      "grad_norm": 14.226933479309082,
      "learning_rate": 2.5918664383561643e-05,
      "loss": 0.7271,
      "step": 1590
    },
    {
      "epoch": 0.684931506849315,
      "grad_norm": 15.042016983032227,
      "learning_rate": 2.5892979452054796e-05,
      "loss": 0.7905,
      "step": 1600
    },
    {
      "epoch": 0.6892123287671232,
      "grad_norm": 35.65140914916992,
      "learning_rate": 2.5867294520547946e-05,
      "loss": 0.5681,
      "step": 1610
    },
    {
      "epoch": 0.6934931506849316,
      "grad_norm": 10.235920906066895,
      "learning_rate": 2.58416095890411e-05,
      "loss": 0.8,
      "step": 1620
    },
    {
      "epoch": 0.6977739726027398,
      "grad_norm": 27.296335220336914,
      "learning_rate": 2.5815924657534245e-05,
      "loss": 0.9603,
      "step": 1630
    },
    {
      "epoch": 0.702054794520548,
      "grad_norm": 25.194801330566406,
      "learning_rate": 2.57902397260274e-05,
      "loss": 0.82,
      "step": 1640
    },
    {
      "epoch": 0.7063356164383562,
      "grad_norm": 36.33634567260742,
      "learning_rate": 2.5764554794520548e-05,
      "loss": 0.7717,
      "step": 1650
    },
    {
      "epoch": 0.7106164383561644,
      "grad_norm": 15.009870529174805,
      "learning_rate": 2.57388698630137e-05,
      "loss": 0.7878,
      "step": 1660
    },
    {
      "epoch": 0.7148972602739726,
      "grad_norm": 34.94595718383789,
      "learning_rate": 2.571318493150685e-05,
      "loss": 0.6897,
      "step": 1670
    },
    {
      "epoch": 0.7191780821917808,
      "grad_norm": 3.3885648250579834,
      "learning_rate": 2.56875e-05,
      "loss": 0.6894,
      "step": 1680
    },
    {
      "epoch": 0.723458904109589,
      "grad_norm": 18.04899024963379,
      "learning_rate": 2.566181506849315e-05,
      "loss": 0.6855,
      "step": 1690
    },
    {
      "epoch": 0.7277397260273972,
      "grad_norm": 23.512062072753906,
      "learning_rate": 2.56361301369863e-05,
      "loss": 0.8258,
      "step": 1700
    },
    {
      "epoch": 0.7320205479452054,
      "grad_norm": 13.760478973388672,
      "learning_rate": 2.5610445205479454e-05,
      "loss": 0.8224,
      "step": 1710
    },
    {
      "epoch": 0.7363013698630136,
      "grad_norm": 17.022178649902344,
      "learning_rate": 2.5584760273972603e-05,
      "loss": 0.8214,
      "step": 1720
    },
    {
      "epoch": 0.740582191780822,
      "grad_norm": 93.08991241455078,
      "learning_rate": 2.5559075342465753e-05,
      "loss": 0.6138,
      "step": 1730
    },
    {
      "epoch": 0.7448630136986302,
      "grad_norm": 12.598793983459473,
      "learning_rate": 2.5533390410958903e-05,
      "loss": 0.6085,
      "step": 1740
    },
    {
      "epoch": 0.7491438356164384,
      "grad_norm": 10.264090538024902,
      "learning_rate": 2.5507705479452056e-05,
      "loss": 0.4778,
      "step": 1750
    },
    {
      "epoch": 0.7534246575342466,
      "grad_norm": 16.948760986328125,
      "learning_rate": 2.5482020547945206e-05,
      "loss": 0.7543,
      "step": 1760
    },
    {
      "epoch": 0.7577054794520548,
      "grad_norm": 16.17952537536621,
      "learning_rate": 2.5456335616438356e-05,
      "loss": 0.676,
      "step": 1770
    },
    {
      "epoch": 0.761986301369863,
      "grad_norm": 16.751449584960938,
      "learning_rate": 2.5430650684931506e-05,
      "loss": 0.6894,
      "step": 1780
    },
    {
      "epoch": 0.7662671232876712,
      "grad_norm": 10.731999397277832,
      "learning_rate": 2.540496575342466e-05,
      "loss": 0.6445,
      "step": 1790
    },
    {
      "epoch": 0.7705479452054794,
      "grad_norm": 5.810342788696289,
      "learning_rate": 2.537928082191781e-05,
      "loss": 0.79,
      "step": 1800
    },
    {
      "epoch": 0.7748287671232876,
      "grad_norm": 23.382064819335938,
      "learning_rate": 2.535359589041096e-05,
      "loss": 0.9898,
      "step": 1810
    },
    {
      "epoch": 0.7791095890410958,
      "grad_norm": 10.761639595031738,
      "learning_rate": 2.5327910958904108e-05,
      "loss": 0.5624,
      "step": 1820
    },
    {
      "epoch": 0.7833904109589042,
      "grad_norm": 13.678400993347168,
      "learning_rate": 2.530222602739726e-05,
      "loss": 0.9474,
      "step": 1830
    },
    {
      "epoch": 0.7876712328767124,
      "grad_norm": 13.515304565429688,
      "learning_rate": 2.527654109589041e-05,
      "loss": 0.7889,
      "step": 1840
    },
    {
      "epoch": 0.7919520547945206,
      "grad_norm": 20.154905319213867,
      "learning_rate": 2.5250856164383564e-05,
      "loss": 0.8551,
      "step": 1850
    },
    {
      "epoch": 0.7962328767123288,
      "grad_norm": 16.845243453979492,
      "learning_rate": 2.522517123287671e-05,
      "loss": 0.7068,
      "step": 1860
    },
    {
      "epoch": 0.800513698630137,
      "grad_norm": 34.205780029296875,
      "learning_rate": 2.5199486301369864e-05,
      "loss": 0.805,
      "step": 1870
    },
    {
      "epoch": 0.8047945205479452,
      "grad_norm": 13.016587257385254,
      "learning_rate": 2.5173801369863014e-05,
      "loss": 0.6715,
      "step": 1880
    },
    {
      "epoch": 0.8090753424657534,
      "grad_norm": 17.59514617919922,
      "learning_rate": 2.5148116438356167e-05,
      "loss": 0.7308,
      "step": 1890
    },
    {
      "epoch": 0.8133561643835616,
      "grad_norm": 21.68423843383789,
      "learning_rate": 2.5122431506849317e-05,
      "loss": 0.9619,
      "step": 1900
    },
    {
      "epoch": 0.8176369863013698,
      "grad_norm": 10.85733413696289,
      "learning_rate": 2.5096746575342466e-05,
      "loss": 0.4836,
      "step": 1910
    },
    {
      "epoch": 0.821917808219178,
      "grad_norm": 17.5113468170166,
      "learning_rate": 2.5071061643835616e-05,
      "loss": 0.7225,
      "step": 1920
    },
    {
      "epoch": 0.8261986301369864,
      "grad_norm": 20.78669548034668,
      "learning_rate": 2.504537671232877e-05,
      "loss": 0.7372,
      "step": 1930
    },
    {
      "epoch": 0.8304794520547946,
      "grad_norm": 12.682047843933105,
      "learning_rate": 2.501969178082192e-05,
      "loss": 0.781,
      "step": 1940
    },
    {
      "epoch": 0.8347602739726028,
      "grad_norm": 18.531436920166016,
      "learning_rate": 2.499400684931507e-05,
      "loss": 0.8725,
      "step": 1950
    },
    {
      "epoch": 0.839041095890411,
      "grad_norm": 33.07869338989258,
      "learning_rate": 2.496832191780822e-05,
      "loss": 0.7145,
      "step": 1960
    },
    {
      "epoch": 0.8433219178082192,
      "grad_norm": 56.62294006347656,
      "learning_rate": 2.4942636986301372e-05,
      "loss": 0.5252,
      "step": 1970
    },
    {
      "epoch": 0.8476027397260274,
      "grad_norm": 64.34467315673828,
      "learning_rate": 2.491695205479452e-05,
      "loss": 0.8005,
      "step": 1980
    },
    {
      "epoch": 0.8518835616438356,
      "grad_norm": 12.431035041809082,
      "learning_rate": 2.4891267123287675e-05,
      "loss": 0.5862,
      "step": 1990
    },
    {
      "epoch": 0.8561643835616438,
      "grad_norm": 31.489168167114258,
      "learning_rate": 2.486558219178082e-05,
      "loss": 0.8181,
      "step": 2000
    },
    {
      "epoch": 0.860445205479452,
      "grad_norm": 20.041730880737305,
      "learning_rate": 2.4839897260273974e-05,
      "loss": 0.6091,
      "step": 2010
    },
    {
      "epoch": 0.8647260273972602,
      "grad_norm": 8.799732208251953,
      "learning_rate": 2.4814212328767124e-05,
      "loss": 0.5877,
      "step": 2020
    },
    {
      "epoch": 0.8690068493150684,
      "grad_norm": 13.108522415161133,
      "learning_rate": 2.4788527397260274e-05,
      "loss": 0.6862,
      "step": 2030
    },
    {
      "epoch": 0.8732876712328768,
      "grad_norm": 21.110349655151367,
      "learning_rate": 2.4762842465753427e-05,
      "loss": 0.6593,
      "step": 2040
    },
    {
      "epoch": 0.877568493150685,
      "grad_norm": 4.349354267120361,
      "learning_rate": 2.4737157534246573e-05,
      "loss": 0.68,
      "step": 2050
    },
    {
      "epoch": 0.8818493150684932,
      "grad_norm": 17.407764434814453,
      "learning_rate": 2.4711472602739727e-05,
      "loss": 0.7587,
      "step": 2060
    },
    {
      "epoch": 0.8861301369863014,
      "grad_norm": 10.936676025390625,
      "learning_rate": 2.4685787671232876e-05,
      "loss": 0.7355,
      "step": 2070
    },
    {
      "epoch": 0.8904109589041096,
      "grad_norm": 36.851112365722656,
      "learning_rate": 2.466010273972603e-05,
      "loss": 0.8058,
      "step": 2080
    },
    {
      "epoch": 0.8946917808219178,
      "grad_norm": 26.359825134277344,
      "learning_rate": 2.4634417808219176e-05,
      "loss": 0.8067,
      "step": 2090
    },
    {
      "epoch": 0.898972602739726,
      "grad_norm": 4.808035373687744,
      "learning_rate": 2.460873287671233e-05,
      "loss": 0.7556,
      "step": 2100
    },
    {
      "epoch": 0.9032534246575342,
      "grad_norm": 29.35092544555664,
      "learning_rate": 2.458304794520548e-05,
      "loss": 0.7164,
      "step": 2110
    },
    {
      "epoch": 0.9075342465753424,
      "grad_norm": 19.436264038085938,
      "learning_rate": 2.4557363013698632e-05,
      "loss": 0.6005,
      "step": 2120
    },
    {
      "epoch": 0.9118150684931506,
      "grad_norm": 12.285465240478516,
      "learning_rate": 2.4531678082191782e-05,
      "loss": 0.5932,
      "step": 2130
    },
    {
      "epoch": 0.916095890410959,
      "grad_norm": 25.76729965209961,
      "learning_rate": 2.450599315068493e-05,
      "loss": 0.6441,
      "step": 2140
    },
    {
      "epoch": 0.9203767123287672,
      "grad_norm": 27.039709091186523,
      "learning_rate": 2.448030821917808e-05,
      "loss": 0.8027,
      "step": 2150
    },
    {
      "epoch": 0.9246575342465754,
      "grad_norm": 28.40961265563965,
      "learning_rate": 2.4454623287671235e-05,
      "loss": 0.5723,
      "step": 2160
    },
    {
      "epoch": 0.9289383561643836,
      "grad_norm": 28.89515495300293,
      "learning_rate": 2.4428938356164384e-05,
      "loss": 0.726,
      "step": 2170
    },
    {
      "epoch": 0.9332191780821918,
      "grad_norm": 15.21774959564209,
      "learning_rate": 2.4403253424657534e-05,
      "loss": 0.5119,
      "step": 2180
    },
    {
      "epoch": 0.9375,
      "grad_norm": 31.885351181030273,
      "learning_rate": 2.4377568493150684e-05,
      "loss": 0.6956,
      "step": 2190
    },
    {
      "epoch": 0.9417808219178082,
      "grad_norm": 41.878997802734375,
      "learning_rate": 2.4351883561643837e-05,
      "loss": 0.4734,
      "step": 2200
    },
    {
      "epoch": 0.9460616438356164,
      "grad_norm": 12.264182090759277,
      "learning_rate": 2.4326198630136987e-05,
      "loss": 0.526,
      "step": 2210
    },
    {
      "epoch": 0.9503424657534246,
      "grad_norm": 6.523041248321533,
      "learning_rate": 2.430051369863014e-05,
      "loss": 0.7543,
      "step": 2220
    },
    {
      "epoch": 0.9546232876712328,
      "grad_norm": 19.857051849365234,
      "learning_rate": 2.4274828767123286e-05,
      "loss": 0.4575,
      "step": 2230
    },
    {
      "epoch": 0.958904109589041,
      "grad_norm": 41.966434478759766,
      "learning_rate": 2.424914383561644e-05,
      "loss": 0.71,
      "step": 2240
    },
    {
      "epoch": 0.9631849315068494,
      "grad_norm": 18.047285079956055,
      "learning_rate": 2.422345890410959e-05,
      "loss": 0.4611,
      "step": 2250
    },
    {
      "epoch": 0.9674657534246576,
      "grad_norm": 32.25387191772461,
      "learning_rate": 2.4197773972602743e-05,
      "loss": 0.6311,
      "step": 2260
    },
    {
      "epoch": 0.9717465753424658,
      "grad_norm": 7.983370780944824,
      "learning_rate": 2.417208904109589e-05,
      "loss": 0.6455,
      "step": 2270
    },
    {
      "epoch": 0.976027397260274,
      "grad_norm": 15.092449188232422,
      "learning_rate": 2.4146404109589042e-05,
      "loss": 0.5441,
      "step": 2280
    },
    {
      "epoch": 0.9803082191780822,
      "grad_norm": 4.433277130126953,
      "learning_rate": 2.4120719178082192e-05,
      "loss": 0.8646,
      "step": 2290
    },
    {
      "epoch": 0.9845890410958904,
      "grad_norm": 2.8443455696105957,
      "learning_rate": 2.4095034246575345e-05,
      "loss": 0.653,
      "step": 2300
    },
    {
      "epoch": 0.9888698630136986,
      "grad_norm": 0.9373922944068909,
      "learning_rate": 2.4069349315068495e-05,
      "loss": 0.5245,
      "step": 2310
    },
    {
      "epoch": 0.9931506849315068,
      "grad_norm": 54.9177131652832,
      "learning_rate": 2.4043664383561645e-05,
      "loss": 0.5716,
      "step": 2320
    },
    {
      "epoch": 0.997431506849315,
      "grad_norm": 17.051746368408203,
      "learning_rate": 2.4017979452054794e-05,
      "loss": 0.5881,
      "step": 2330
    },
    {
      "epoch": 1.0,
      "eval_f1": 0.640155605090126,
      "eval_loss": 0.7885168194770813,
      "eval_precision": 0.65989952624744,
      "eval_recall": 0.6545266692496449,
      "eval_runtime": 596.1184,
      "eval_samples_per_second": 7.499,
      "eval_steps_per_second": 0.938,
      "step": 2336
    }
  ],
  "logging_steps": 10,
  "max_steps": 11680,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 103462372299264.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
