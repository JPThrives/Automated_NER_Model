{
  "best_global_step": 1640,
  "best_metric": 1.0088167190551758,
  "best_model_checkpoint": "./assamese-ner-model\\checkpoint-1640",
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 1640,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.024390243902439025,
      "grad_norm": 6.806490898132324,
      "learning_rate": 2.9835365853658537e-05,
      "loss": 3.0954,
      "step": 10
    },
    {
      "epoch": 0.04878048780487805,
      "grad_norm": 5.178570747375488,
      "learning_rate": 2.9652439024390243e-05,
      "loss": 2.5991,
      "step": 20
    },
    {
      "epoch": 0.07317073170731707,
      "grad_norm": 6.849449634552002,
      "learning_rate": 2.946951219512195e-05,
      "loss": 2.3584,
      "step": 30
    },
    {
      "epoch": 0.0975609756097561,
      "grad_norm": 3.9619998931884766,
      "learning_rate": 2.928658536585366e-05,
      "loss": 2.1533,
      "step": 40
    },
    {
      "epoch": 0.12195121951219512,
      "grad_norm": 21.48830795288086,
      "learning_rate": 2.9103658536585366e-05,
      "loss": 2.0662,
      "step": 50
    },
    {
      "epoch": 0.14634146341463414,
      "grad_norm": 6.740002155303955,
      "learning_rate": 2.8920731707317072e-05,
      "loss": 1.9827,
      "step": 60
    },
    {
      "epoch": 0.17073170731707318,
      "grad_norm": 16.31853485107422,
      "learning_rate": 2.873780487804878e-05,
      "loss": 1.8839,
      "step": 70
    },
    {
      "epoch": 0.1951219512195122,
      "grad_norm": 9.550885200500488,
      "learning_rate": 2.855487804878049e-05,
      "loss": 1.9446,
      "step": 80
    },
    {
      "epoch": 0.21951219512195122,
      "grad_norm": 6.670747756958008,
      "learning_rate": 2.8371951219512195e-05,
      "loss": 1.8155,
      "step": 90
    },
    {
      "epoch": 0.24390243902439024,
      "grad_norm": 7.361683368682861,
      "learning_rate": 2.81890243902439e-05,
      "loss": 1.7771,
      "step": 100
    },
    {
      "epoch": 0.2682926829268293,
      "grad_norm": 11.538498878479004,
      "learning_rate": 2.800609756097561e-05,
      "loss": 1.7501,
      "step": 110
    },
    {
      "epoch": 0.2926829268292683,
      "grad_norm": 9.348010063171387,
      "learning_rate": 2.782317073170732e-05,
      "loss": 1.6399,
      "step": 120
    },
    {
      "epoch": 0.3170731707317073,
      "grad_norm": 13.538579940795898,
      "learning_rate": 2.7640243902439027e-05,
      "loss": 1.784,
      "step": 130
    },
    {
      "epoch": 0.34146341463414637,
      "grad_norm": 8.807812690734863,
      "learning_rate": 2.7457317073170733e-05,
      "loss": 1.7968,
      "step": 140
    },
    {
      "epoch": 0.36585365853658536,
      "grad_norm": 16.737699508666992,
      "learning_rate": 2.727439024390244e-05,
      "loss": 1.5405,
      "step": 150
    },
    {
      "epoch": 0.3902439024390244,
      "grad_norm": 10.272539138793945,
      "learning_rate": 2.709146341463415e-05,
      "loss": 1.5086,
      "step": 160
    },
    {
      "epoch": 0.4146341463414634,
      "grad_norm": 11.049367904663086,
      "learning_rate": 2.6908536585365856e-05,
      "loss": 1.4831,
      "step": 170
    },
    {
      "epoch": 0.43902439024390244,
      "grad_norm": 10.026756286621094,
      "learning_rate": 2.6725609756097562e-05,
      "loss": 1.3933,
      "step": 180
    },
    {
      "epoch": 0.4634146341463415,
      "grad_norm": 5.130154132843018,
      "learning_rate": 2.6542682926829268e-05,
      "loss": 1.529,
      "step": 190
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 13.891378402709961,
      "learning_rate": 2.6359756097560978e-05,
      "loss": 1.4415,
      "step": 200
    },
    {
      "epoch": 0.5121951219512195,
      "grad_norm": 16.403783798217773,
      "learning_rate": 2.6176829268292684e-05,
      "loss": 1.4468,
      "step": 210
    },
    {
      "epoch": 0.5365853658536586,
      "grad_norm": 9.644128799438477,
      "learning_rate": 2.599390243902439e-05,
      "loss": 1.4506,
      "step": 220
    },
    {
      "epoch": 0.5609756097560976,
      "grad_norm": 43.84064865112305,
      "learning_rate": 2.5810975609756097e-05,
      "loss": 1.204,
      "step": 230
    },
    {
      "epoch": 0.5853658536585366,
      "grad_norm": 11.659941673278809,
      "learning_rate": 2.5628048780487803e-05,
      "loss": 1.3033,
      "step": 240
    },
    {
      "epoch": 0.6097560975609756,
      "grad_norm": 17.009349822998047,
      "learning_rate": 2.5445121951219513e-05,
      "loss": 1.2675,
      "step": 250
    },
    {
      "epoch": 0.6341463414634146,
      "grad_norm": 20.25924301147461,
      "learning_rate": 2.526219512195122e-05,
      "loss": 1.2092,
      "step": 260
    },
    {
      "epoch": 0.6585365853658537,
      "grad_norm": 12.54345417022705,
      "learning_rate": 2.5079268292682926e-05,
      "loss": 1.3723,
      "step": 270
    },
    {
      "epoch": 0.6829268292682927,
      "grad_norm": 21.613412857055664,
      "learning_rate": 2.4896341463414632e-05,
      "loss": 1.0481,
      "step": 280
    },
    {
      "epoch": 0.7073170731707317,
      "grad_norm": 7.058538436889648,
      "learning_rate": 2.4713414634146342e-05,
      "loss": 1.2186,
      "step": 290
    },
    {
      "epoch": 0.7317073170731707,
      "grad_norm": 15.101284980773926,
      "learning_rate": 2.4530487804878048e-05,
      "loss": 1.2348,
      "step": 300
    },
    {
      "epoch": 0.7560975609756098,
      "grad_norm": 33.639976501464844,
      "learning_rate": 2.4347560975609758e-05,
      "loss": 1.086,
      "step": 310
    },
    {
      "epoch": 0.7804878048780488,
      "grad_norm": 18.268835067749023,
      "learning_rate": 2.4164634146341464e-05,
      "loss": 1.3811,
      "step": 320
    },
    {
      "epoch": 0.8048780487804879,
      "grad_norm": 12.419136047363281,
      "learning_rate": 2.3981707317073174e-05,
      "loss": 0.99,
      "step": 330
    },
    {
      "epoch": 0.8292682926829268,
      "grad_norm": 14.32109260559082,
      "learning_rate": 2.379878048780488e-05,
      "loss": 1.2299,
      "step": 340
    },
    {
      "epoch": 0.8536585365853658,
      "grad_norm": 14.149701118469238,
      "learning_rate": 2.3615853658536587e-05,
      "loss": 1.1281,
      "step": 350
    },
    {
      "epoch": 0.8780487804878049,
      "grad_norm": 16.090816497802734,
      "learning_rate": 2.3432926829268293e-05,
      "loss": 1.2012,
      "step": 360
    },
    {
      "epoch": 0.9024390243902439,
      "grad_norm": 7.1827545166015625,
      "learning_rate": 2.3250000000000003e-05,
      "loss": 1.0064,
      "step": 370
    },
    {
      "epoch": 0.926829268292683,
      "grad_norm": 25.350797653198242,
      "learning_rate": 2.306707317073171e-05,
      "loss": 1.099,
      "step": 380
    },
    {
      "epoch": 0.9512195121951219,
      "grad_norm": 12.590739250183105,
      "learning_rate": 2.2884146341463415e-05,
      "loss": 1.09,
      "step": 390
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 17.025577545166016,
      "learning_rate": 2.2701219512195122e-05,
      "loss": 1.0079,
      "step": 400
    },
    {
      "epoch": 1.0,
      "grad_norm": 20.882299423217773,
      "learning_rate": 2.251829268292683e-05,
      "loss": 1.0585,
      "step": 410
    },
    {
      "epoch": 1.0,
      "eval_f1": 0.5494229355504264,
      "eval_loss": 1.2746955156326294,
      "eval_precision": 0.5772371676477878,
      "eval_recall": 0.5423606082548877,
      "eval_runtime": 115.007,
      "eval_samples_per_second": 7.287,
      "eval_steps_per_second": 0.913,
      "step": 410
    },
    {
      "epoch": 1.024390243902439,
      "grad_norm": 11.970000267028809,
      "learning_rate": 2.2335365853658538e-05,
      "loss": 0.9372,
      "step": 420
    },
    {
      "epoch": 1.048780487804878,
      "grad_norm": 5.787439346313477,
      "learning_rate": 2.2152439024390244e-05,
      "loss": 0.9075,
      "step": 430
    },
    {
      "epoch": 1.0731707317073171,
      "grad_norm": 12.780766487121582,
      "learning_rate": 2.196951219512195e-05,
      "loss": 1.1243,
      "step": 440
    },
    {
      "epoch": 1.0975609756097562,
      "grad_norm": 19.16437530517578,
      "learning_rate": 2.1786585365853657e-05,
      "loss": 1.0595,
      "step": 450
    },
    {
      "epoch": 1.1219512195121952,
      "grad_norm": 10.130958557128906,
      "learning_rate": 2.1603658536585367e-05,
      "loss": 0.9586,
      "step": 460
    },
    {
      "epoch": 1.146341463414634,
      "grad_norm": 10.170520782470703,
      "learning_rate": 2.1420731707317073e-05,
      "loss": 0.8314,
      "step": 470
    },
    {
      "epoch": 1.170731707317073,
      "grad_norm": 14.072538375854492,
      "learning_rate": 2.123780487804878e-05,
      "loss": 0.7359,
      "step": 480
    },
    {
      "epoch": 1.1951219512195121,
      "grad_norm": 14.70962142944336,
      "learning_rate": 2.1054878048780486e-05,
      "loss": 0.8459,
      "step": 490
    },
    {
      "epoch": 1.2195121951219512,
      "grad_norm": 60.16694641113281,
      "learning_rate": 2.0871951219512195e-05,
      "loss": 1.0276,
      "step": 500
    },
    {
      "epoch": 1.2439024390243902,
      "grad_norm": 34.79148483276367,
      "learning_rate": 2.06890243902439e-05,
      "loss": 0.8705,
      "step": 510
    },
    {
      "epoch": 1.2682926829268293,
      "grad_norm": 14.02302360534668,
      "learning_rate": 2.050609756097561e-05,
      "loss": 0.9037,
      "step": 520
    },
    {
      "epoch": 1.2926829268292683,
      "grad_norm": 16.247819900512695,
      "learning_rate": 2.0323170731707318e-05,
      "loss": 0.9502,
      "step": 530
    },
    {
      "epoch": 1.3170731707317074,
      "grad_norm": 14.70374870300293,
      "learning_rate": 2.0140243902439027e-05,
      "loss": 0.7468,
      "step": 540
    },
    {
      "epoch": 1.3414634146341464,
      "grad_norm": 7.403873920440674,
      "learning_rate": 1.9957317073170734e-05,
      "loss": 0.8474,
      "step": 550
    },
    {
      "epoch": 1.3658536585365852,
      "grad_norm": 13.034966468811035,
      "learning_rate": 1.977439024390244e-05,
      "loss": 0.8123,
      "step": 560
    },
    {
      "epoch": 1.3902439024390243,
      "grad_norm": 7.810897350311279,
      "learning_rate": 1.9591463414634146e-05,
      "loss": 0.8831,
      "step": 570
    },
    {
      "epoch": 1.4146341463414633,
      "grad_norm": 13.052536964416504,
      "learning_rate": 1.9408536585365856e-05,
      "loss": 0.9425,
      "step": 580
    },
    {
      "epoch": 1.4390243902439024,
      "grad_norm": 6.177195072174072,
      "learning_rate": 1.9225609756097563e-05,
      "loss": 1.0625,
      "step": 590
    },
    {
      "epoch": 1.4634146341463414,
      "grad_norm": 29.98554039001465,
      "learning_rate": 1.904268292682927e-05,
      "loss": 0.9397,
      "step": 600
    },
    {
      "epoch": 1.4878048780487805,
      "grad_norm": 15.040423393249512,
      "learning_rate": 1.8859756097560975e-05,
      "loss": 0.9428,
      "step": 610
    },
    {
      "epoch": 1.5121951219512195,
      "grad_norm": 21.019847869873047,
      "learning_rate": 1.8676829268292685e-05,
      "loss": 0.8748,
      "step": 620
    },
    {
      "epoch": 1.5365853658536586,
      "grad_norm": 9.834309577941895,
      "learning_rate": 1.849390243902439e-05,
      "loss": 0.8529,
      "step": 630
    },
    {
      "epoch": 1.5609756097560976,
      "grad_norm": 6.31193208694458,
      "learning_rate": 1.8310975609756098e-05,
      "loss": 0.7329,
      "step": 640
    },
    {
      "epoch": 1.5853658536585367,
      "grad_norm": 18.246091842651367,
      "learning_rate": 1.8128048780487804e-05,
      "loss": 0.8568,
      "step": 650
    },
    {
      "epoch": 1.6097560975609757,
      "grad_norm": 20.857250213623047,
      "learning_rate": 1.794512195121951e-05,
      "loss": 0.7689,
      "step": 660
    },
    {
      "epoch": 1.6341463414634148,
      "grad_norm": 12.395569801330566,
      "learning_rate": 1.776219512195122e-05,
      "loss": 0.6363,
      "step": 670
    },
    {
      "epoch": 1.6585365853658538,
      "grad_norm": 22.315855026245117,
      "learning_rate": 1.7579268292682926e-05,
      "loss": 0.6848,
      "step": 680
    },
    {
      "epoch": 1.6829268292682928,
      "grad_norm": 18.118322372436523,
      "learning_rate": 1.7396341463414633e-05,
      "loss": 0.8065,
      "step": 690
    },
    {
      "epoch": 1.7073170731707317,
      "grad_norm": 8.332937240600586,
      "learning_rate": 1.721341463414634e-05,
      "loss": 0.942,
      "step": 700
    },
    {
      "epoch": 1.7317073170731707,
      "grad_norm": 10.008410453796387,
      "learning_rate": 1.703048780487805e-05,
      "loss": 0.8726,
      "step": 710
    },
    {
      "epoch": 1.7560975609756098,
      "grad_norm": 5.9549994468688965,
      "learning_rate": 1.684756097560976e-05,
      "loss": 0.8538,
      "step": 720
    },
    {
      "epoch": 1.7804878048780488,
      "grad_norm": 19.855783462524414,
      "learning_rate": 1.6664634146341465e-05,
      "loss": 0.8684,
      "step": 730
    },
    {
      "epoch": 1.8048780487804879,
      "grad_norm": 8.915218353271484,
      "learning_rate": 1.648170731707317e-05,
      "loss": 0.7405,
      "step": 740
    },
    {
      "epoch": 1.8292682926829267,
      "grad_norm": 14.2027587890625,
      "learning_rate": 1.629878048780488e-05,
      "loss": 0.9261,
      "step": 750
    },
    {
      "epoch": 1.8536585365853657,
      "grad_norm": 16.217857360839844,
      "learning_rate": 1.6115853658536587e-05,
      "loss": 0.8021,
      "step": 760
    },
    {
      "epoch": 1.8780487804878048,
      "grad_norm": 5.440675735473633,
      "learning_rate": 1.5932926829268294e-05,
      "loss": 0.6502,
      "step": 770
    },
    {
      "epoch": 1.9024390243902438,
      "grad_norm": 26.296707153320312,
      "learning_rate": 1.575e-05,
      "loss": 0.7008,
      "step": 780
    },
    {
      "epoch": 1.9268292682926829,
      "grad_norm": 23.358501434326172,
      "learning_rate": 1.556707317073171e-05,
      "loss": 0.8148,
      "step": 790
    },
    {
      "epoch": 1.951219512195122,
      "grad_norm": 10.8901948928833,
      "learning_rate": 1.5384146341463416e-05,
      "loss": 0.811,
      "step": 800
    },
    {
      "epoch": 1.975609756097561,
      "grad_norm": 18.95917510986328,
      "learning_rate": 1.5201219512195122e-05,
      "loss": 0.8747,
      "step": 810
    },
    {
      "epoch": 2.0,
      "grad_norm": 8.61365795135498,
      "learning_rate": 1.501829268292683e-05,
      "loss": 0.7689,
      "step": 820
    },
    {
      "epoch": 2.0,
      "eval_f1": 0.5697672245247429,
      "eval_loss": 1.150826096534729,
      "eval_precision": 0.5937139102278998,
      "eval_recall": 0.5679459328988655,
      "eval_runtime": 115.3117,
      "eval_samples_per_second": 7.267,
      "eval_steps_per_second": 0.911,
      "step": 820
    },
    {
      "epoch": 2.024390243902439,
      "grad_norm": 6.715840816497803,
      "learning_rate": 1.4835365853658537e-05,
      "loss": 0.6421,
      "step": 830
    },
    {
      "epoch": 2.048780487804878,
      "grad_norm": 13.301779747009277,
      "learning_rate": 1.4652439024390245e-05,
      "loss": 0.6319,
      "step": 840
    },
    {
      "epoch": 2.073170731707317,
      "grad_norm": 6.477333068847656,
      "learning_rate": 1.4469512195121951e-05,
      "loss": 0.6931,
      "step": 850
    },
    {
      "epoch": 2.097560975609756,
      "grad_norm": 8.34264850616455,
      "learning_rate": 1.4286585365853657e-05,
      "loss": 0.6892,
      "step": 860
    },
    {
      "epoch": 2.1219512195121952,
      "grad_norm": 10.900664329528809,
      "learning_rate": 1.4103658536585366e-05,
      "loss": 0.5719,
      "step": 870
    },
    {
      "epoch": 2.1463414634146343,
      "grad_norm": 7.771868705749512,
      "learning_rate": 1.3920731707317074e-05,
      "loss": 0.4993,
      "step": 880
    },
    {
      "epoch": 2.1707317073170733,
      "grad_norm": 13.967663764953613,
      "learning_rate": 1.3737804878048782e-05,
      "loss": 0.6155,
      "step": 890
    },
    {
      "epoch": 2.1951219512195124,
      "grad_norm": 12.739470481872559,
      "learning_rate": 1.3554878048780488e-05,
      "loss": 0.5759,
      "step": 900
    },
    {
      "epoch": 2.2195121951219514,
      "grad_norm": 19.207687377929688,
      "learning_rate": 1.3371951219512196e-05,
      "loss": 0.6671,
      "step": 910
    },
    {
      "epoch": 2.2439024390243905,
      "grad_norm": 19.0821475982666,
      "learning_rate": 1.3189024390243902e-05,
      "loss": 0.6747,
      "step": 920
    },
    {
      "epoch": 2.2682926829268295,
      "grad_norm": 14.267452239990234,
      "learning_rate": 1.300609756097561e-05,
      "loss": 0.7393,
      "step": 930
    },
    {
      "epoch": 2.292682926829268,
      "grad_norm": 17.852628707885742,
      "learning_rate": 1.2823170731707317e-05,
      "loss": 0.7255,
      "step": 940
    },
    {
      "epoch": 2.317073170731707,
      "grad_norm": 13.893001556396484,
      "learning_rate": 1.2640243902439025e-05,
      "loss": 0.5974,
      "step": 950
    },
    {
      "epoch": 2.341463414634146,
      "grad_norm": 8.185132026672363,
      "learning_rate": 1.2457317073170731e-05,
      "loss": 0.666,
      "step": 960
    },
    {
      "epoch": 2.3658536585365852,
      "grad_norm": 8.152981758117676,
      "learning_rate": 1.2274390243902439e-05,
      "loss": 0.6125,
      "step": 970
    },
    {
      "epoch": 2.3902439024390243,
      "grad_norm": 8.551187515258789,
      "learning_rate": 1.2091463414634147e-05,
      "loss": 0.6581,
      "step": 980
    },
    {
      "epoch": 2.4146341463414633,
      "grad_norm": 16.64451789855957,
      "learning_rate": 1.1908536585365855e-05,
      "loss": 0.7352,
      "step": 990
    },
    {
      "epoch": 2.4390243902439024,
      "grad_norm": 9.507303237915039,
      "learning_rate": 1.1725609756097562e-05,
      "loss": 0.8007,
      "step": 1000
    },
    {
      "epoch": 2.4634146341463414,
      "grad_norm": 19.096637725830078,
      "learning_rate": 1.154268292682927e-05,
      "loss": 0.6021,
      "step": 1010
    },
    {
      "epoch": 2.4878048780487805,
      "grad_norm": 22.359750747680664,
      "learning_rate": 1.1359756097560976e-05,
      "loss": 0.6386,
      "step": 1020
    },
    {
      "epoch": 2.5121951219512195,
      "grad_norm": 9.334784507751465,
      "learning_rate": 1.1176829268292684e-05,
      "loss": 0.5712,
      "step": 1030
    },
    {
      "epoch": 2.5365853658536586,
      "grad_norm": 11.843677520751953,
      "learning_rate": 1.099390243902439e-05,
      "loss": 0.5975,
      "step": 1040
    },
    {
      "epoch": 2.5609756097560976,
      "grad_norm": 15.30622386932373,
      "learning_rate": 1.0810975609756097e-05,
      "loss": 0.65,
      "step": 1050
    },
    {
      "epoch": 2.5853658536585367,
      "grad_norm": 11.1730375289917,
      "learning_rate": 1.0628048780487805e-05,
      "loss": 0.6665,
      "step": 1060
    },
    {
      "epoch": 2.6097560975609757,
      "grad_norm": 8.31334400177002,
      "learning_rate": 1.0445121951219511e-05,
      "loss": 0.633,
      "step": 1070
    },
    {
      "epoch": 2.6341463414634148,
      "grad_norm": 40.82326889038086,
      "learning_rate": 1.026219512195122e-05,
      "loss": 0.7266,
      "step": 1080
    },
    {
      "epoch": 2.658536585365854,
      "grad_norm": 7.279654026031494,
      "learning_rate": 1.0079268292682927e-05,
      "loss": 0.6688,
      "step": 1090
    },
    {
      "epoch": 2.682926829268293,
      "grad_norm": 13.106986045837402,
      "learning_rate": 9.896341463414635e-06,
      "loss": 0.6307,
      "step": 1100
    },
    {
      "epoch": 2.7073170731707314,
      "grad_norm": 7.49027156829834,
      "learning_rate": 9.713414634146341e-06,
      "loss": 0.5265,
      "step": 1110
    },
    {
      "epoch": 2.7317073170731705,
      "grad_norm": 12.040759086608887,
      "learning_rate": 9.53048780487805e-06,
      "loss": 0.4849,
      "step": 1120
    },
    {
      "epoch": 2.7560975609756095,
      "grad_norm": 8.902502059936523,
      "learning_rate": 9.347560975609756e-06,
      "loss": 0.617,
      "step": 1130
    },
    {
      "epoch": 2.7804878048780486,
      "grad_norm": 23.830108642578125,
      "learning_rate": 9.164634146341464e-06,
      "loss": 0.5333,
      "step": 1140
    },
    {
      "epoch": 2.8048780487804876,
      "grad_norm": 17.478782653808594,
      "learning_rate": 8.98170731707317e-06,
      "loss": 0.5752,
      "step": 1150
    },
    {
      "epoch": 2.8292682926829267,
      "grad_norm": 9.030675888061523,
      "learning_rate": 8.798780487804878e-06,
      "loss": 0.5479,
      "step": 1160
    },
    {
      "epoch": 2.8536585365853657,
      "grad_norm": 18.662771224975586,
      "learning_rate": 8.615853658536585e-06,
      "loss": 0.5644,
      "step": 1170
    },
    {
      "epoch": 2.8780487804878048,
      "grad_norm": 23.385509490966797,
      "learning_rate": 8.432926829268294e-06,
      "loss": 0.6266,
      "step": 1180
    },
    {
      "epoch": 2.902439024390244,
      "grad_norm": 11.912230491638184,
      "learning_rate": 8.25e-06,
      "loss": 0.6193,
      "step": 1190
    },
    {
      "epoch": 2.926829268292683,
      "grad_norm": 25.09102439880371,
      "learning_rate": 8.067073170731709e-06,
      "loss": 0.6077,
      "step": 1200
    },
    {
      "epoch": 2.951219512195122,
      "grad_norm": 18.108539581298828,
      "learning_rate": 7.884146341463415e-06,
      "loss": 0.5417,
      "step": 1210
    },
    {
      "epoch": 2.975609756097561,
      "grad_norm": 2.6286394596099854,
      "learning_rate": 7.701219512195123e-06,
      "loss": 0.4998,
      "step": 1220
    },
    {
      "epoch": 3.0,
      "grad_norm": 7.039989471435547,
      "learning_rate": 7.518292682926829e-06,
      "loss": 0.6273,
      "step": 1230
    },
    {
      "epoch": 3.0,
      "eval_f1": 0.6174461287249529,
      "eval_loss": 1.0335557460784912,
      "eval_precision": 0.6258755200510389,
      "eval_recall": 0.6159787593531257,
      "eval_runtime": 122.8252,
      "eval_samples_per_second": 6.823,
      "eval_steps_per_second": 0.855,
      "step": 1230
    },
    {
      "epoch": 3.024390243902439,
      "grad_norm": 28.549453735351562,
      "learning_rate": 7.3353658536585366e-06,
      "loss": 0.5493,
      "step": 1240
    },
    {
      "epoch": 3.048780487804878,
      "grad_norm": 6.627293109893799,
      "learning_rate": 7.152439024390244e-06,
      "loss": 0.511,
      "step": 1250
    },
    {
      "epoch": 3.073170731707317,
      "grad_norm": 7.209502220153809,
      "learning_rate": 6.969512195121952e-06,
      "loss": 0.5795,
      "step": 1260
    },
    {
      "epoch": 3.097560975609756,
      "grad_norm": 4.345218181610107,
      "learning_rate": 6.786585365853659e-06,
      "loss": 0.4099,
      "step": 1270
    },
    {
      "epoch": 3.1219512195121952,
      "grad_norm": 27.088008880615234,
      "learning_rate": 6.603658536585366e-06,
      "loss": 0.5194,
      "step": 1280
    },
    {
      "epoch": 3.1463414634146343,
      "grad_norm": 20.787752151489258,
      "learning_rate": 6.420731707317073e-06,
      "loss": 0.467,
      "step": 1290
    },
    {
      "epoch": 3.1707317073170733,
      "grad_norm": 18.01169776916504,
      "learning_rate": 6.2378048780487806e-06,
      "loss": 0.5141,
      "step": 1300
    },
    {
      "epoch": 3.1951219512195124,
      "grad_norm": 6.0897088050842285,
      "learning_rate": 6.054878048780489e-06,
      "loss": 0.4309,
      "step": 1310
    },
    {
      "epoch": 3.2195121951219514,
      "grad_norm": 8.961873054504395,
      "learning_rate": 5.871951219512196e-06,
      "loss": 0.4686,
      "step": 1320
    },
    {
      "epoch": 3.2439024390243905,
      "grad_norm": 9.4314603805542,
      "learning_rate": 5.689024390243903e-06,
      "loss": 0.4948,
      "step": 1330
    },
    {
      "epoch": 3.2682926829268295,
      "grad_norm": 9.526607513427734,
      "learning_rate": 5.50609756097561e-06,
      "loss": 0.4013,
      "step": 1340
    },
    {
      "epoch": 3.292682926829268,
      "grad_norm": 12.46185302734375,
      "learning_rate": 5.323170731707317e-06,
      "loss": 0.4545,
      "step": 1350
    },
    {
      "epoch": 3.317073170731707,
      "grad_norm": 7.441787242889404,
      "learning_rate": 5.1402439024390245e-06,
      "loss": 0.4743,
      "step": 1360
    },
    {
      "epoch": 3.341463414634146,
      "grad_norm": 10.799345016479492,
      "learning_rate": 4.957317073170732e-06,
      "loss": 0.5323,
      "step": 1370
    },
    {
      "epoch": 3.3658536585365852,
      "grad_norm": 14.359907150268555,
      "learning_rate": 4.774390243902439e-06,
      "loss": 0.4985,
      "step": 1380
    },
    {
      "epoch": 3.3902439024390243,
      "grad_norm": 7.326841831207275,
      "learning_rate": 4.591463414634146e-06,
      "loss": 0.4785,
      "step": 1390
    },
    {
      "epoch": 3.4146341463414633,
      "grad_norm": 15.325827598571777,
      "learning_rate": 4.408536585365853e-06,
      "loss": 0.4399,
      "step": 1400
    },
    {
      "epoch": 3.4390243902439024,
      "grad_norm": 10.886738777160645,
      "learning_rate": 4.2256097560975605e-06,
      "loss": 0.497,
      "step": 1410
    },
    {
      "epoch": 3.4634146341463414,
      "grad_norm": 9.591415405273438,
      "learning_rate": 4.0426829268292685e-06,
      "loss": 0.627,
      "step": 1420
    },
    {
      "epoch": 3.4878048780487805,
      "grad_norm": 15.495943069458008,
      "learning_rate": 3.859756097560976e-06,
      "loss": 0.5244,
      "step": 1430
    },
    {
      "epoch": 3.5121951219512195,
      "grad_norm": 17.311372756958008,
      "learning_rate": 3.676829268292683e-06,
      "loss": 0.5637,
      "step": 1440
    },
    {
      "epoch": 3.5365853658536586,
      "grad_norm": 6.448296546936035,
      "learning_rate": 3.4939024390243905e-06,
      "loss": 0.5048,
      "step": 1450
    },
    {
      "epoch": 3.5609756097560976,
      "grad_norm": 6.077893257141113,
      "learning_rate": 3.3109756097560977e-06,
      "loss": 0.5595,
      "step": 1460
    },
    {
      "epoch": 3.5853658536585367,
      "grad_norm": 9.212390899658203,
      "learning_rate": 3.128048780487805e-06,
      "loss": 0.4562,
      "step": 1470
    },
    {
      "epoch": 3.6097560975609757,
      "grad_norm": 9.06646728515625,
      "learning_rate": 2.9451219512195125e-06,
      "loss": 0.5062,
      "step": 1480
    },
    {
      "epoch": 3.6341463414634148,
      "grad_norm": 8.162152290344238,
      "learning_rate": 2.7621951219512197e-06,
      "loss": 0.4454,
      "step": 1490
    },
    {
      "epoch": 3.658536585365854,
      "grad_norm": 4.1686577796936035,
      "learning_rate": 2.579268292682927e-06,
      "loss": 0.4322,
      "step": 1500
    },
    {
      "epoch": 3.682926829268293,
      "grad_norm": 12.823263168334961,
      "learning_rate": 2.396341463414634e-06,
      "loss": 0.5151,
      "step": 1510
    },
    {
      "epoch": 3.7073170731707314,
      "grad_norm": 9.397972106933594,
      "learning_rate": 2.2134146341463412e-06,
      "loss": 0.4832,
      "step": 1520
    },
    {
      "epoch": 3.7317073170731705,
      "grad_norm": 8.10063648223877,
      "learning_rate": 2.030487804878049e-06,
      "loss": 0.5023,
      "step": 1530
    },
    {
      "epoch": 3.7560975609756095,
      "grad_norm": 13.44108772277832,
      "learning_rate": 1.847560975609756e-06,
      "loss": 0.5429,
      "step": 1540
    },
    {
      "epoch": 3.7804878048780486,
      "grad_norm": 21.249168395996094,
      "learning_rate": 1.6646341463414634e-06,
      "loss": 0.5654,
      "step": 1550
    },
    {
      "epoch": 3.8048780487804876,
      "grad_norm": 2.81365966796875,
      "learning_rate": 1.4817073170731708e-06,
      "loss": 0.3695,
      "step": 1560
    },
    {
      "epoch": 3.8292682926829267,
      "grad_norm": 23.344005584716797,
      "learning_rate": 1.298780487804878e-06,
      "loss": 0.5475,
      "step": 1570
    },
    {
      "epoch": 3.8536585365853657,
      "grad_norm": 8.077929496765137,
      "learning_rate": 1.1158536585365854e-06,
      "loss": 0.4136,
      "step": 1580
    },
    {
      "epoch": 3.8780487804878048,
      "grad_norm": 10.019097328186035,
      "learning_rate": 9.329268292682927e-07,
      "loss": 0.4713,
      "step": 1590
    },
    {
      "epoch": 3.902439024390244,
      "grad_norm": 11.623896598815918,
      "learning_rate": 7.5e-07,
      "loss": 0.5552,
      "step": 1600
    },
    {
      "epoch": 3.926829268292683,
      "grad_norm": 9.770620346069336,
      "learning_rate": 5.670731707317073e-07,
      "loss": 0.6291,
      "step": 1610
    },
    {
      "epoch": 3.951219512195122,
      "grad_norm": 8.462898254394531,
      "learning_rate": 3.841463414634146e-07,
      "loss": 0.7231,
      "step": 1620
    },
    {
      "epoch": 3.975609756097561,
      "grad_norm": 21.182422637939453,
      "learning_rate": 2.0121951219512196e-07,
      "loss": 0.4873,
      "step": 1630
    },
    {
      "epoch": 4.0,
      "grad_norm": 6.061005115509033,
      "learning_rate": 1.829268292682927e-08,
      "loss": 0.4517,
      "step": 1640
    },
    {
      "epoch": 4.0,
      "eval_f1": 0.6184850264762732,
      "eval_loss": 1.0088167190551758,
      "eval_precision": 0.6243287044788045,
      "eval_recall": 0.6191165821868212,
      "eval_runtime": 121.939,
      "eval_samples_per_second": 6.872,
      "eval_steps_per_second": 0.861,
      "step": 1640
    }
  ],
  "logging_steps": 10,
  "max_steps": 1640,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 72504516547584.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
