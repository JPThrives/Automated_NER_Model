{
  "best_global_step": 1341,
  "best_metric": 0.6622161269187927,
  "best_model_checkpoint": "./assamese-ner-model\\checkpoint-1341",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 1341,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02237136465324385,
      "grad_norm": 26.499937057495117,
      "learning_rate": 2.9798657718120806e-05,
      "loss": 0.9909,
      "step": 10
    },
    {
      "epoch": 0.0447427293064877,
      "grad_norm": 11.344620704650879,
      "learning_rate": 2.957494407158837e-05,
      "loss": 1.0869,
      "step": 20
    },
    {
      "epoch": 0.06711409395973154,
      "grad_norm": 6.6401753425598145,
      "learning_rate": 2.935123042505593e-05,
      "loss": 1.0506,
      "step": 30
    },
    {
      "epoch": 0.0894854586129754,
      "grad_norm": 9.887673377990723,
      "learning_rate": 2.912751677852349e-05,
      "loss": 0.9501,
      "step": 40
    },
    {
      "epoch": 0.11185682326621924,
      "grad_norm": 6.668510913848877,
      "learning_rate": 2.8903803131991055e-05,
      "loss": 0.7875,
      "step": 50
    },
    {
      "epoch": 0.1342281879194631,
      "grad_norm": 24.745676040649414,
      "learning_rate": 2.8680089485458614e-05,
      "loss": 0.8398,
      "step": 60
    },
    {
      "epoch": 0.15659955257270694,
      "grad_norm": 20.566390991210938,
      "learning_rate": 2.8456375838926174e-05,
      "loss": 0.9209,
      "step": 70
    },
    {
      "epoch": 0.1789709172259508,
      "grad_norm": 8.219826698303223,
      "learning_rate": 2.8232662192393736e-05,
      "loss": 0.8561,
      "step": 80
    },
    {
      "epoch": 0.20134228187919462,
      "grad_norm": 17.740718841552734,
      "learning_rate": 2.8008948545861296e-05,
      "loss": 0.9202,
      "step": 90
    },
    {
      "epoch": 0.22371364653243847,
      "grad_norm": 10.854516983032227,
      "learning_rate": 2.778523489932886e-05,
      "loss": 0.9006,
      "step": 100
    },
    {
      "epoch": 0.24608501118568232,
      "grad_norm": 9.866681098937988,
      "learning_rate": 2.7561521252796422e-05,
      "loss": 0.9766,
      "step": 110
    },
    {
      "epoch": 0.2684563758389262,
      "grad_norm": 12.054339408874512,
      "learning_rate": 2.733780760626398e-05,
      "loss": 0.9599,
      "step": 120
    },
    {
      "epoch": 0.29082774049217003,
      "grad_norm": 6.589640140533447,
      "learning_rate": 2.7114093959731544e-05,
      "loss": 0.9436,
      "step": 130
    },
    {
      "epoch": 0.3131991051454139,
      "grad_norm": 21.276220321655273,
      "learning_rate": 2.6890380313199107e-05,
      "loss": 0.8269,
      "step": 140
    },
    {
      "epoch": 0.33557046979865773,
      "grad_norm": 10.560347557067871,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.9424,
      "step": 150
    },
    {
      "epoch": 0.3579418344519016,
      "grad_norm": 14.2261323928833,
      "learning_rate": 2.644295302013423e-05,
      "loss": 0.8087,
      "step": 160
    },
    {
      "epoch": 0.38031319910514544,
      "grad_norm": 7.743072509765625,
      "learning_rate": 2.6219239373601792e-05,
      "loss": 0.8917,
      "step": 170
    },
    {
      "epoch": 0.40268456375838924,
      "grad_norm": 5.369715213775635,
      "learning_rate": 2.5995525727069352e-05,
      "loss": 0.8168,
      "step": 180
    },
    {
      "epoch": 0.4250559284116331,
      "grad_norm": 12.913552284240723,
      "learning_rate": 2.577181208053691e-05,
      "loss": 0.8208,
      "step": 190
    },
    {
      "epoch": 0.44742729306487694,
      "grad_norm": 8.381195068359375,
      "learning_rate": 2.5548098434004474e-05,
      "loss": 0.8849,
      "step": 200
    },
    {
      "epoch": 0.4697986577181208,
      "grad_norm": 8.252007484436035,
      "learning_rate": 2.5324384787472037e-05,
      "loss": 0.6517,
      "step": 210
    },
    {
      "epoch": 0.49217002237136465,
      "grad_norm": 18.743104934692383,
      "learning_rate": 2.5100671140939597e-05,
      "loss": 0.757,
      "step": 220
    },
    {
      "epoch": 0.5145413870246085,
      "grad_norm": 5.1574015617370605,
      "learning_rate": 2.487695749440716e-05,
      "loss": 0.7866,
      "step": 230
    },
    {
      "epoch": 0.5369127516778524,
      "grad_norm": 5.609720706939697,
      "learning_rate": 2.4653243847874723e-05,
      "loss": 0.944,
      "step": 240
    },
    {
      "epoch": 0.5592841163310962,
      "grad_norm": 11.792844772338867,
      "learning_rate": 2.4429530201342282e-05,
      "loss": 0.7304,
      "step": 250
    },
    {
      "epoch": 0.5816554809843401,
      "grad_norm": 9.030939102172852,
      "learning_rate": 2.4205816554809845e-05,
      "loss": 0.8696,
      "step": 260
    },
    {
      "epoch": 0.6040268456375839,
      "grad_norm": 18.235719680786133,
      "learning_rate": 2.3982102908277408e-05,
      "loss": 0.8007,
      "step": 270
    },
    {
      "epoch": 0.6263982102908278,
      "grad_norm": 14.27563190460205,
      "learning_rate": 2.3758389261744967e-05,
      "loss": 0.9865,
      "step": 280
    },
    {
      "epoch": 0.6487695749440716,
      "grad_norm": 17.80423927307129,
      "learning_rate": 2.353467561521253e-05,
      "loss": 0.6994,
      "step": 290
    },
    {
      "epoch": 0.6711409395973155,
      "grad_norm": 9.92613697052002,
      "learning_rate": 2.331096196868009e-05,
      "loss": 0.7833,
      "step": 300
    },
    {
      "epoch": 0.6935123042505593,
      "grad_norm": 9.939031600952148,
      "learning_rate": 2.308724832214765e-05,
      "loss": 0.7982,
      "step": 310
    },
    {
      "epoch": 0.7158836689038032,
      "grad_norm": 15.133773803710938,
      "learning_rate": 2.2863534675615212e-05,
      "loss": 0.81,
      "step": 320
    },
    {
      "epoch": 0.738255033557047,
      "grad_norm": 9.298298835754395,
      "learning_rate": 2.2639821029082775e-05,
      "loss": 0.7959,
      "step": 330
    },
    {
      "epoch": 0.7606263982102909,
      "grad_norm": 8.24637222290039,
      "learning_rate": 2.2416107382550335e-05,
      "loss": 0.7327,
      "step": 340
    },
    {
      "epoch": 0.7829977628635347,
      "grad_norm": 7.558612823486328,
      "learning_rate": 2.2192393736017897e-05,
      "loss": 0.8175,
      "step": 350
    },
    {
      "epoch": 0.8053691275167785,
      "grad_norm": 5.0307793617248535,
      "learning_rate": 2.196868008948546e-05,
      "loss": 0.7298,
      "step": 360
    },
    {
      "epoch": 0.8277404921700223,
      "grad_norm": 8.012625694274902,
      "learning_rate": 2.174496644295302e-05,
      "loss": 0.8453,
      "step": 370
    },
    {
      "epoch": 0.8501118568232662,
      "grad_norm": 5.265918254852295,
      "learning_rate": 2.1521252796420583e-05,
      "loss": 0.6734,
      "step": 380
    },
    {
      "epoch": 0.87248322147651,
      "grad_norm": 18.38582992553711,
      "learning_rate": 2.1297539149888146e-05,
      "loss": 0.6773,
      "step": 390
    },
    {
      "epoch": 0.8948545861297539,
      "grad_norm": 17.508047103881836,
      "learning_rate": 2.1073825503355705e-05,
      "loss": 0.7571,
      "step": 400
    },
    {
      "epoch": 0.9172259507829977,
      "grad_norm": 12.790425300598145,
      "learning_rate": 2.0850111856823265e-05,
      "loss": 0.8513,
      "step": 410
    },
    {
      "epoch": 0.9395973154362416,
      "grad_norm": 9.74666976928711,
      "learning_rate": 2.0626398210290828e-05,
      "loss": 0.6361,
      "step": 420
    },
    {
      "epoch": 0.9619686800894854,
      "grad_norm": 9.930702209472656,
      "learning_rate": 2.040268456375839e-05,
      "loss": 0.7861,
      "step": 430
    },
    {
      "epoch": 0.9843400447427293,
      "grad_norm": 10.96072006225586,
      "learning_rate": 2.017897091722595e-05,
      "loss": 0.92,
      "step": 440
    },
    {
      "epoch": 1.0,
      "eval_f1": 0.6841703850749439,
      "eval_loss": 0.7616462707519531,
      "eval_precision": 0.6851652297218331,
      "eval_recall": 0.6943521594684385,
      "eval_runtime": 110.3601,
      "eval_samples_per_second": 8.101,
      "eval_steps_per_second": 1.015,
      "step": 447
    },
    {
      "epoch": 1.0067114093959733,
      "grad_norm": 6.807394981384277,
      "learning_rate": 1.9955257270693513e-05,
      "loss": 0.7463,
      "step": 450
    },
    {
      "epoch": 1.029082774049217,
      "grad_norm": 8.720099449157715,
      "learning_rate": 1.9731543624161076e-05,
      "loss": 0.6276,
      "step": 460
    },
    {
      "epoch": 1.0514541387024607,
      "grad_norm": 7.530154228210449,
      "learning_rate": 1.9507829977628635e-05,
      "loss": 0.6466,
      "step": 470
    },
    {
      "epoch": 1.0738255033557047,
      "grad_norm": 8.642014503479004,
      "learning_rate": 1.9284116331096198e-05,
      "loss": 0.6368,
      "step": 480
    },
    {
      "epoch": 1.0961968680089484,
      "grad_norm": 6.184327602386475,
      "learning_rate": 1.906040268456376e-05,
      "loss": 0.7538,
      "step": 490
    },
    {
      "epoch": 1.1185682326621924,
      "grad_norm": 7.637544631958008,
      "learning_rate": 1.883668903803132e-05,
      "loss": 0.731,
      "step": 500
    },
    {
      "epoch": 1.1409395973154361,
      "grad_norm": 20.88919448852539,
      "learning_rate": 1.8612975391498883e-05,
      "loss": 0.5934,
      "step": 510
    },
    {
      "epoch": 1.1633109619686801,
      "grad_norm": 9.643326759338379,
      "learning_rate": 1.8389261744966443e-05,
      "loss": 0.5581,
      "step": 520
    },
    {
      "epoch": 1.1856823266219239,
      "grad_norm": 12.893446922302246,
      "learning_rate": 1.8165548098434002e-05,
      "loss": 0.5595,
      "step": 530
    },
    {
      "epoch": 1.2080536912751678,
      "grad_norm": 9.271039962768555,
      "learning_rate": 1.7941834451901565e-05,
      "loss": 0.7617,
      "step": 540
    },
    {
      "epoch": 1.2304250559284116,
      "grad_norm": 10.80196762084961,
      "learning_rate": 1.7718120805369128e-05,
      "loss": 0.6209,
      "step": 550
    },
    {
      "epoch": 1.2527964205816555,
      "grad_norm": 23.700345993041992,
      "learning_rate": 1.7494407158836688e-05,
      "loss": 0.5714,
      "step": 560
    },
    {
      "epoch": 1.2751677852348993,
      "grad_norm": 17.695178985595703,
      "learning_rate": 1.727069351230425e-05,
      "loss": 0.8336,
      "step": 570
    },
    {
      "epoch": 1.2975391498881432,
      "grad_norm": 5.1130146980285645,
      "learning_rate": 1.7046979865771814e-05,
      "loss": 0.7021,
      "step": 580
    },
    {
      "epoch": 1.319910514541387,
      "grad_norm": 6.642660140991211,
      "learning_rate": 1.6823266219239373e-05,
      "loss": 0.7751,
      "step": 590
    },
    {
      "epoch": 1.342281879194631,
      "grad_norm": 9.293599128723145,
      "learning_rate": 1.6599552572706936e-05,
      "loss": 0.5398,
      "step": 600
    },
    {
      "epoch": 1.3646532438478747,
      "grad_norm": 31.807357788085938,
      "learning_rate": 1.63758389261745e-05,
      "loss": 0.5863,
      "step": 610
    },
    {
      "epoch": 1.3870246085011186,
      "grad_norm": 15.702994346618652,
      "learning_rate": 1.615212527964206e-05,
      "loss": 0.6246,
      "step": 620
    },
    {
      "epoch": 1.4093959731543624,
      "grad_norm": 5.1996564865112305,
      "learning_rate": 1.592841163310962e-05,
      "loss": 0.6804,
      "step": 630
    },
    {
      "epoch": 1.4317673378076063,
      "grad_norm": 12.61574649810791,
      "learning_rate": 1.570469798657718e-05,
      "loss": 0.5805,
      "step": 640
    },
    {
      "epoch": 1.45413870246085,
      "grad_norm": 5.479684829711914,
      "learning_rate": 1.548098434004474e-05,
      "loss": 0.5617,
      "step": 650
    },
    {
      "epoch": 1.476510067114094,
      "grad_norm": 8.064306259155273,
      "learning_rate": 1.5257270693512305e-05,
      "loss": 0.6422,
      "step": 660
    },
    {
      "epoch": 1.4988814317673378,
      "grad_norm": 24.569438934326172,
      "learning_rate": 1.5033557046979866e-05,
      "loss": 0.857,
      "step": 670
    },
    {
      "epoch": 1.5212527964205815,
      "grad_norm": 14.310860633850098,
      "learning_rate": 1.4809843400447427e-05,
      "loss": 0.651,
      "step": 680
    },
    {
      "epoch": 1.5436241610738255,
      "grad_norm": 12.460115432739258,
      "learning_rate": 1.4586129753914989e-05,
      "loss": 0.7642,
      "step": 690
    },
    {
      "epoch": 1.5659955257270695,
      "grad_norm": 11.478301048278809,
      "learning_rate": 1.4362416107382551e-05,
      "loss": 0.6357,
      "step": 700
    },
    {
      "epoch": 1.5883668903803132,
      "grad_norm": 28.89933204650879,
      "learning_rate": 1.4138702460850113e-05,
      "loss": 0.6196,
      "step": 710
    },
    {
      "epoch": 1.610738255033557,
      "grad_norm": 17.997299194335938,
      "learning_rate": 1.3914988814317674e-05,
      "loss": 0.6066,
      "step": 720
    },
    {
      "epoch": 1.633109619686801,
      "grad_norm": 7.156940460205078,
      "learning_rate": 1.3691275167785235e-05,
      "loss": 0.4192,
      "step": 730
    },
    {
      "epoch": 1.6554809843400449,
      "grad_norm": 14.202323913574219,
      "learning_rate": 1.3467561521252796e-05,
      "loss": 0.5341,
      "step": 740
    },
    {
      "epoch": 1.6778523489932886,
      "grad_norm": 9.792184829711914,
      "learning_rate": 1.3243847874720359e-05,
      "loss": 0.5651,
      "step": 750
    },
    {
      "epoch": 1.7002237136465324,
      "grad_norm": 2.5776896476745605,
      "learning_rate": 1.302013422818792e-05,
      "loss": 0.5375,
      "step": 760
    },
    {
      "epoch": 1.7225950782997763,
      "grad_norm": 25.1643009185791,
      "learning_rate": 1.2796420581655482e-05,
      "loss": 0.5959,
      "step": 770
    },
    {
      "epoch": 1.7449664429530203,
      "grad_norm": 10.579537391662598,
      "learning_rate": 1.2572706935123043e-05,
      "loss": 0.9996,
      "step": 780
    },
    {
      "epoch": 1.767337807606264,
      "grad_norm": 7.081393718719482,
      "learning_rate": 1.2348993288590604e-05,
      "loss": 0.6244,
      "step": 790
    },
    {
      "epoch": 1.7897091722595078,
      "grad_norm": 5.128015518188477,
      "learning_rate": 1.2125279642058165e-05,
      "loss": 0.4716,
      "step": 800
    },
    {
      "epoch": 1.8120805369127517,
      "grad_norm": 14.629246711730957,
      "learning_rate": 1.1901565995525728e-05,
      "loss": 0.6178,
      "step": 810
    },
    {
      "epoch": 1.8344519015659957,
      "grad_norm": 8.020458221435547,
      "learning_rate": 1.167785234899329e-05,
      "loss": 0.6974,
      "step": 820
    },
    {
      "epoch": 1.8568232662192394,
      "grad_norm": 7.949923992156982,
      "learning_rate": 1.1454138702460849e-05,
      "loss": 0.6423,
      "step": 830
    },
    {
      "epoch": 1.8791946308724832,
      "grad_norm": 7.564052104949951,
      "learning_rate": 1.1230425055928412e-05,
      "loss": 0.6103,
      "step": 840
    },
    {
      "epoch": 1.901565995525727,
      "grad_norm": 15.185393333435059,
      "learning_rate": 1.1006711409395973e-05,
      "loss": 0.6099,
      "step": 850
    },
    {
      "epoch": 1.9239373601789709,
      "grad_norm": 13.474766731262207,
      "learning_rate": 1.0782997762863536e-05,
      "loss": 0.6922,
      "step": 860
    },
    {
      "epoch": 1.9463087248322148,
      "grad_norm": 23.76938247680664,
      "learning_rate": 1.0559284116331097e-05,
      "loss": 0.7207,
      "step": 870
    },
    {
      "epoch": 1.9686800894854586,
      "grad_norm": 7.233656406402588,
      "learning_rate": 1.0335570469798658e-05,
      "loss": 0.5827,
      "step": 880
    },
    {
      "epoch": 1.9910514541387023,
      "grad_norm": 14.950921058654785,
      "learning_rate": 1.011185682326622e-05,
      "loss": 0.4613,
      "step": 890
    },
    {
      "epoch": 2.0,
      "eval_f1": 0.7015882858241294,
      "eval_loss": 0.7034929394721985,
      "eval_precision": 0.7022843267193887,
      "eval_recall": 0.7222591362126246,
      "eval_runtime": 103.505,
      "eval_samples_per_second": 8.637,
      "eval_steps_per_second": 1.082,
      "step": 894
    },
    {
      "epoch": 2.0134228187919465,
      "grad_norm": 7.4073944091796875,
      "learning_rate": 9.88814317673378e-06,
      "loss": 0.5731,
      "step": 900
    },
    {
      "epoch": 2.0357941834451903,
      "grad_norm": 8.927010536193848,
      "learning_rate": 9.664429530201342e-06,
      "loss": 0.4476,
      "step": 910
    },
    {
      "epoch": 2.058165548098434,
      "grad_norm": 2.9854607582092285,
      "learning_rate": 9.440715883668905e-06,
      "loss": 0.447,
      "step": 920
    },
    {
      "epoch": 2.0805369127516777,
      "grad_norm": 7.576559066772461,
      "learning_rate": 9.217002237136466e-06,
      "loss": 0.4317,
      "step": 930
    },
    {
      "epoch": 2.1029082774049215,
      "grad_norm": 6.488811492919922,
      "learning_rate": 8.993288590604027e-06,
      "loss": 0.5509,
      "step": 940
    },
    {
      "epoch": 2.1252796420581657,
      "grad_norm": 6.645498752593994,
      "learning_rate": 8.769574944071588e-06,
      "loss": 0.4836,
      "step": 950
    },
    {
      "epoch": 2.1476510067114094,
      "grad_norm": 5.41803503036499,
      "learning_rate": 8.54586129753915e-06,
      "loss": 0.7169,
      "step": 960
    },
    {
      "epoch": 2.170022371364653,
      "grad_norm": 17.963945388793945,
      "learning_rate": 8.322147651006712e-06,
      "loss": 0.5954,
      "step": 970
    },
    {
      "epoch": 2.192393736017897,
      "grad_norm": 42.34425354003906,
      "learning_rate": 8.098434004474274e-06,
      "loss": 0.5954,
      "step": 980
    },
    {
      "epoch": 2.214765100671141,
      "grad_norm": 15.060530662536621,
      "learning_rate": 7.874720357941835e-06,
      "loss": 0.3903,
      "step": 990
    },
    {
      "epoch": 2.237136465324385,
      "grad_norm": 3.230893611907959,
      "learning_rate": 7.651006711409396e-06,
      "loss": 0.4576,
      "step": 1000
    },
    {
      "epoch": 2.2595078299776286,
      "grad_norm": 12.749349594116211,
      "learning_rate": 7.427293064876958e-06,
      "loss": 0.506,
      "step": 1010
    },
    {
      "epoch": 2.2818791946308723,
      "grad_norm": 28.902179718017578,
      "learning_rate": 7.203579418344519e-06,
      "loss": 0.5836,
      "step": 1020
    },
    {
      "epoch": 2.3042505592841165,
      "grad_norm": 11.830079078674316,
      "learning_rate": 6.9798657718120805e-06,
      "loss": 0.6159,
      "step": 1030
    },
    {
      "epoch": 2.3266219239373602,
      "grad_norm": 18.880918502807617,
      "learning_rate": 6.7561521252796425e-06,
      "loss": 0.4625,
      "step": 1040
    },
    {
      "epoch": 2.348993288590604,
      "grad_norm": 7.17169189453125,
      "learning_rate": 6.532438478747204e-06,
      "loss": 0.4509,
      "step": 1050
    },
    {
      "epoch": 2.3713646532438477,
      "grad_norm": 6.367412090301514,
      "learning_rate": 6.308724832214766e-06,
      "loss": 0.6623,
      "step": 1060
    },
    {
      "epoch": 2.393736017897092,
      "grad_norm": 11.67835807800293,
      "learning_rate": 6.085011185682326e-06,
      "loss": 0.5136,
      "step": 1070
    },
    {
      "epoch": 2.4161073825503356,
      "grad_norm": 51.911102294921875,
      "learning_rate": 5.861297539149888e-06,
      "loss": 0.6121,
      "step": 1080
    },
    {
      "epoch": 2.4384787472035794,
      "grad_norm": 20.27164077758789,
      "learning_rate": 5.63758389261745e-06,
      "loss": 0.5322,
      "step": 1090
    },
    {
      "epoch": 2.460850111856823,
      "grad_norm": 5.921002388000488,
      "learning_rate": 5.4138702460850106e-06,
      "loss": 0.5605,
      "step": 1100
    },
    {
      "epoch": 2.4832214765100673,
      "grad_norm": 14.09941291809082,
      "learning_rate": 5.190156599552573e-06,
      "loss": 0.5716,
      "step": 1110
    },
    {
      "epoch": 2.505592841163311,
      "grad_norm": 16.71769905090332,
      "learning_rate": 4.966442953020135e-06,
      "loss": 0.4019,
      "step": 1120
    },
    {
      "epoch": 2.527964205816555,
      "grad_norm": 22.121633529663086,
      "learning_rate": 4.742729306487696e-06,
      "loss": 0.3858,
      "step": 1130
    },
    {
      "epoch": 2.5503355704697985,
      "grad_norm": 9.727243423461914,
      "learning_rate": 4.519015659955257e-06,
      "loss": 0.5245,
      "step": 1140
    },
    {
      "epoch": 2.5727069351230423,
      "grad_norm": 33.29819107055664,
      "learning_rate": 4.295302013422819e-06,
      "loss": 0.3957,
      "step": 1150
    },
    {
      "epoch": 2.5950782997762865,
      "grad_norm": 7.666825294494629,
      "learning_rate": 4.07158836689038e-06,
      "loss": 0.5516,
      "step": 1160
    },
    {
      "epoch": 2.61744966442953,
      "grad_norm": 23.99205780029297,
      "learning_rate": 3.847874720357942e-06,
      "loss": 0.5163,
      "step": 1170
    },
    {
      "epoch": 2.639821029082774,
      "grad_norm": 9.57778263092041,
      "learning_rate": 3.6241610738255036e-06,
      "loss": 0.3864,
      "step": 1180
    },
    {
      "epoch": 2.662192393736018,
      "grad_norm": 2.7382845878601074,
      "learning_rate": 3.400447427293065e-06,
      "loss": 0.3922,
      "step": 1190
    },
    {
      "epoch": 2.684563758389262,
      "grad_norm": 31.808565139770508,
      "learning_rate": 3.1767337807606264e-06,
      "loss": 0.7282,
      "step": 1200
    },
    {
      "epoch": 2.7069351230425056,
      "grad_norm": 76.23480987548828,
      "learning_rate": 2.953020134228188e-06,
      "loss": 0.5961,
      "step": 1210
    },
    {
      "epoch": 2.7293064876957494,
      "grad_norm": 14.061819076538086,
      "learning_rate": 2.7293064876957493e-06,
      "loss": 0.4275,
      "step": 1220
    },
    {
      "epoch": 2.751677852348993,
      "grad_norm": 6.076058864593506,
      "learning_rate": 2.5055928411633113e-06,
      "loss": 0.4768,
      "step": 1230
    },
    {
      "epoch": 2.7740492170022373,
      "grad_norm": 2.2516326904296875,
      "learning_rate": 2.2818791946308725e-06,
      "loss": 0.4016,
      "step": 1240
    },
    {
      "epoch": 2.796420581655481,
      "grad_norm": 24.75690460205078,
      "learning_rate": 2.058165548098434e-06,
      "loss": 0.4058,
      "step": 1250
    },
    {
      "epoch": 2.8187919463087248,
      "grad_norm": 18.137590408325195,
      "learning_rate": 1.8344519015659955e-06,
      "loss": 0.4438,
      "step": 1260
    },
    {
      "epoch": 2.841163310961969,
      "grad_norm": 56.15564727783203,
      "learning_rate": 1.610738255033557e-06,
      "loss": 0.4783,
      "step": 1270
    },
    {
      "epoch": 2.8635346756152127,
      "grad_norm": 7.099587440490723,
      "learning_rate": 1.3870246085011186e-06,
      "loss": 0.4834,
      "step": 1280
    },
    {
      "epoch": 2.8859060402684564,
      "grad_norm": 10.120471000671387,
      "learning_rate": 1.1633109619686802e-06,
      "loss": 0.5308,
      "step": 1290
    },
    {
      "epoch": 2.9082774049217,
      "grad_norm": 23.60041618347168,
      "learning_rate": 9.395973154362416e-07,
      "loss": 0.4611,
      "step": 1300
    },
    {
      "epoch": 2.930648769574944,
      "grad_norm": 27.6268253326416,
      "learning_rate": 7.15883668903803e-07,
      "loss": 0.4748,
      "step": 1310
    },
    {
      "epoch": 2.953020134228188,
      "grad_norm": 7.077395439147949,
      "learning_rate": 4.921700223713647e-07,
      "loss": 0.5034,
      "step": 1320
    },
    {
      "epoch": 2.975391498881432,
      "grad_norm": 3.3658058643341064,
      "learning_rate": 2.684563758389262e-07,
      "loss": 0.5243,
      "step": 1330
    },
    {
      "epoch": 2.9977628635346756,
      "grad_norm": 8.619452476501465,
      "learning_rate": 4.474272930648769e-08,
      "loss": 0.5139,
      "step": 1340
    },
    {
      "epoch": 3.0,
      "eval_f1": 0.7393064352031681,
      "eval_loss": 0.6622161269187927,
      "eval_precision": 0.7340125491968901,
      "eval_recall": 0.7501661129568107,
      "eval_runtime": 104.83,
      "eval_samples_per_second": 8.528,
      "eval_steps_per_second": 1.068,
      "step": 1341
    }
  ],
  "logging_steps": 10,
  "max_steps": 1341,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 59355477430272.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
