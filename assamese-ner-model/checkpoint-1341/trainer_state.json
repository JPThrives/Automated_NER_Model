{
  "best_global_step": 1341,
  "best_metric": 0.7666971683502197,
  "best_model_checkpoint": "./assamese-ner-model\\checkpoint-1341",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 1341,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02237136465324385,
      "grad_norm": 7.157212734222412,
      "learning_rate": 2.9848993288590606e-05,
      "loss": 2.8962,
      "step": 10
    },
    {
      "epoch": 0.0447427293064877,
      "grad_norm": 6.176828861236572,
      "learning_rate": 2.9681208053691275e-05,
      "loss": 2.5259,
      "step": 20
    },
    {
      "epoch": 0.06711409395973154,
      "grad_norm": 6.290245532989502,
      "learning_rate": 2.9513422818791945e-05,
      "loss": 2.5175,
      "step": 30
    },
    {
      "epoch": 0.0894854586129754,
      "grad_norm": 6.314141750335693,
      "learning_rate": 2.9345637583892618e-05,
      "loss": 1.8937,
      "step": 40
    },
    {
      "epoch": 0.11185682326621924,
      "grad_norm": 5.160433292388916,
      "learning_rate": 2.917785234899329e-05,
      "loss": 1.6995,
      "step": 50
    },
    {
      "epoch": 0.1342281879194631,
      "grad_norm": 6.162083148956299,
      "learning_rate": 2.9010067114093957e-05,
      "loss": 1.5155,
      "step": 60
    },
    {
      "epoch": 0.15659955257270694,
      "grad_norm": 7.120657920837402,
      "learning_rate": 2.884228187919463e-05,
      "loss": 1.7117,
      "step": 70
    },
    {
      "epoch": 0.1789709172259508,
      "grad_norm": 10.064047813415527,
      "learning_rate": 2.8674496644295303e-05,
      "loss": 1.6234,
      "step": 80
    },
    {
      "epoch": 0.20134228187919462,
      "grad_norm": 7.134056568145752,
      "learning_rate": 2.8506711409395973e-05,
      "loss": 1.4532,
      "step": 90
    },
    {
      "epoch": 0.22371364653243847,
      "grad_norm": 11.062542915344238,
      "learning_rate": 2.8338926174496646e-05,
      "loss": 1.4031,
      "step": 100
    },
    {
      "epoch": 0.24608501118568232,
      "grad_norm": 4.8246307373046875,
      "learning_rate": 2.8171140939597316e-05,
      "loss": 1.4896,
      "step": 110
    },
    {
      "epoch": 0.2684563758389262,
      "grad_norm": 8.579524040222168,
      "learning_rate": 2.800335570469799e-05,
      "loss": 1.3745,
      "step": 120
    },
    {
      "epoch": 0.29082774049217003,
      "grad_norm": 8.57605266571045,
      "learning_rate": 2.783557046979866e-05,
      "loss": 1.4817,
      "step": 130
    },
    {
      "epoch": 0.3131991051454139,
      "grad_norm": 6.495886325836182,
      "learning_rate": 2.766778523489933e-05,
      "loss": 1.1454,
      "step": 140
    },
    {
      "epoch": 0.33557046979865773,
      "grad_norm": 10.61954402923584,
      "learning_rate": 2.75e-05,
      "loss": 1.4575,
      "step": 150
    },
    {
      "epoch": 0.3579418344519016,
      "grad_norm": 6.849227428436279,
      "learning_rate": 2.733221476510067e-05,
      "loss": 1.2853,
      "step": 160
    },
    {
      "epoch": 0.38031319910514544,
      "grad_norm": 3.900722026824951,
      "learning_rate": 2.7164429530201344e-05,
      "loss": 1.2888,
      "step": 170
    },
    {
      "epoch": 0.40268456375838924,
      "grad_norm": 9.42455768585205,
      "learning_rate": 2.6996644295302017e-05,
      "loss": 1.3254,
      "step": 180
    },
    {
      "epoch": 0.4250559284116331,
      "grad_norm": 7.289499759674072,
      "learning_rate": 2.6828859060402683e-05,
      "loss": 1.2277,
      "step": 190
    },
    {
      "epoch": 0.44742729306487694,
      "grad_norm": 7.697993755340576,
      "learning_rate": 2.6661073825503356e-05,
      "loss": 1.3245,
      "step": 200
    },
    {
      "epoch": 0.4697986577181208,
      "grad_norm": 8.294083595275879,
      "learning_rate": 2.649328859060403e-05,
      "loss": 1.0054,
      "step": 210
    },
    {
      "epoch": 0.49217002237136465,
      "grad_norm": 5.122470855712891,
      "learning_rate": 2.63255033557047e-05,
      "loss": 1.191,
      "step": 220
    },
    {
      "epoch": 0.5145413870246085,
      "grad_norm": 8.426281929016113,
      "learning_rate": 2.6157718120805368e-05,
      "loss": 1.2034,
      "step": 230
    },
    {
      "epoch": 0.5369127516778524,
      "grad_norm": 29.226863861083984,
      "learning_rate": 2.598993288590604e-05,
      "loss": 1.394,
      "step": 240
    },
    {
      "epoch": 0.5592841163310962,
      "grad_norm": 9.634117126464844,
      "learning_rate": 2.582214765100671e-05,
      "loss": 1.2749,
      "step": 250
    },
    {
      "epoch": 0.5816554809843401,
      "grad_norm": 7.35527229309082,
      "learning_rate": 2.5654362416107384e-05,
      "loss": 1.1729,
      "step": 260
    },
    {
      "epoch": 0.6040268456375839,
      "grad_norm": 7.573207855224609,
      "learning_rate": 2.5486577181208054e-05,
      "loss": 1.3343,
      "step": 270
    },
    {
      "epoch": 0.6263982102908278,
      "grad_norm": 13.957225799560547,
      "learning_rate": 2.5318791946308723e-05,
      "loss": 1.2757,
      "step": 280
    },
    {
      "epoch": 0.6487695749440716,
      "grad_norm": 7.72498893737793,
      "learning_rate": 2.5151006711409396e-05,
      "loss": 0.9984,
      "step": 290
    },
    {
      "epoch": 0.6711409395973155,
      "grad_norm": 14.369996070861816,
      "learning_rate": 2.498322147651007e-05,
      "loss": 1.1547,
      "step": 300
    },
    {
      "epoch": 0.6935123042505593,
      "grad_norm": 12.68978214263916,
      "learning_rate": 2.4815436241610742e-05,
      "loss": 1.074,
      "step": 310
    },
    {
      "epoch": 0.7158836689038032,
      "grad_norm": 19.182003021240234,
      "learning_rate": 2.464765100671141e-05,
      "loss": 1.1721,
      "step": 320
    },
    {
      "epoch": 0.738255033557047,
      "grad_norm": 5.345863342285156,
      "learning_rate": 2.447986577181208e-05,
      "loss": 1.0534,
      "step": 330
    },
    {
      "epoch": 0.7606263982102909,
      "grad_norm": 9.118544578552246,
      "learning_rate": 2.4312080536912754e-05,
      "loss": 1.1372,
      "step": 340
    },
    {
      "epoch": 0.7829977628635347,
      "grad_norm": 9.682320594787598,
      "learning_rate": 2.4144295302013424e-05,
      "loss": 1.2717,
      "step": 350
    },
    {
      "epoch": 0.8053691275167785,
      "grad_norm": 16.167531967163086,
      "learning_rate": 2.3976510067114094e-05,
      "loss": 1.0906,
      "step": 360
    },
    {
      "epoch": 0.8277404921700223,
      "grad_norm": 12.754473686218262,
      "learning_rate": 2.3808724832214767e-05,
      "loss": 1.2125,
      "step": 370
    },
    {
      "epoch": 0.8501118568232662,
      "grad_norm": 10.417766571044922,
      "learning_rate": 2.3640939597315436e-05,
      "loss": 1.0429,
      "step": 380
    },
    {
      "epoch": 0.87248322147651,
      "grad_norm": 6.782989025115967,
      "learning_rate": 2.347315436241611e-05,
      "loss": 0.9883,
      "step": 390
    },
    {
      "epoch": 0.8948545861297539,
      "grad_norm": 11.977624893188477,
      "learning_rate": 2.330536912751678e-05,
      "loss": 1.0944,
      "step": 400
    },
    {
      "epoch": 0.9172259507829977,
      "grad_norm": 16.361587524414062,
      "learning_rate": 2.313758389261745e-05,
      "loss": 1.1683,
      "step": 410
    },
    {
      "epoch": 0.9395973154362416,
      "grad_norm": 8.478983879089355,
      "learning_rate": 2.2969798657718122e-05,
      "loss": 0.9486,
      "step": 420
    },
    {
      "epoch": 0.9619686800894854,
      "grad_norm": 17.60906410217285,
      "learning_rate": 2.2802013422818795e-05,
      "loss": 1.1541,
      "step": 430
    },
    {
      "epoch": 0.9843400447427293,
      "grad_norm": 10.970022201538086,
      "learning_rate": 2.263422818791946e-05,
      "loss": 1.2914,
      "step": 440
    },
    {
      "epoch": 1.0,
      "eval_f1": 0.6043505062846339,
      "eval_loss": 1.0173254013061523,
      "eval_precision": 0.6084737342840406,
      "eval_recall": 0.6205980066445183,
      "eval_runtime": 135.1561,
      "eval_samples_per_second": 6.615,
      "eval_steps_per_second": 0.829,
      "step": 447
    },
    {
      "epoch": 1.0067114093959733,
      "grad_norm": 11.354462623596191,
      "learning_rate": 2.2466442953020134e-05,
      "loss": 0.9932,
      "step": 450
    },
    {
      "epoch": 1.029082774049217,
      "grad_norm": 5.543514251708984,
      "learning_rate": 2.2298657718120807e-05,
      "loss": 0.8589,
      "step": 460
    },
    {
      "epoch": 1.0514541387024607,
      "grad_norm": 3.9696362018585205,
      "learning_rate": 2.2130872483221477e-05,
      "loss": 0.9245,
      "step": 470
    },
    {
      "epoch": 1.0738255033557047,
      "grad_norm": 11.148492813110352,
      "learning_rate": 2.1963087248322146e-05,
      "loss": 0.8976,
      "step": 480
    },
    {
      "epoch": 1.0961968680089484,
      "grad_norm": 5.530459880828857,
      "learning_rate": 2.179530201342282e-05,
      "loss": 0.9569,
      "step": 490
    },
    {
      "epoch": 1.1185682326621924,
      "grad_norm": 13.021883964538574,
      "learning_rate": 2.1627516778523492e-05,
      "loss": 1.1345,
      "step": 500
    },
    {
      "epoch": 1.1409395973154361,
      "grad_norm": 39.57381057739258,
      "learning_rate": 2.1459731543624162e-05,
      "loss": 0.9332,
      "step": 510
    },
    {
      "epoch": 1.1633109619686801,
      "grad_norm": 16.392417907714844,
      "learning_rate": 2.129194630872483e-05,
      "loss": 0.8678,
      "step": 520
    },
    {
      "epoch": 1.1856823266219239,
      "grad_norm": 16.07318115234375,
      "learning_rate": 2.1124161073825505e-05,
      "loss": 0.8696,
      "step": 530
    },
    {
      "epoch": 1.2080536912751678,
      "grad_norm": 12.171980857849121,
      "learning_rate": 2.0956375838926174e-05,
      "loss": 1.0565,
      "step": 540
    },
    {
      "epoch": 1.2304250559284116,
      "grad_norm": 9.174825668334961,
      "learning_rate": 2.0788590604026847e-05,
      "loss": 0.9,
      "step": 550
    },
    {
      "epoch": 1.2527964205816555,
      "grad_norm": 14.54086971282959,
      "learning_rate": 2.062080536912752e-05,
      "loss": 0.891,
      "step": 560
    },
    {
      "epoch": 1.2751677852348993,
      "grad_norm": 11.284557342529297,
      "learning_rate": 2.0453020134228187e-05,
      "loss": 1.172,
      "step": 570
    },
    {
      "epoch": 1.2975391498881432,
      "grad_norm": 9.837845802307129,
      "learning_rate": 2.028523489932886e-05,
      "loss": 0.9735,
      "step": 580
    },
    {
      "epoch": 1.319910514541387,
      "grad_norm": 8.07215690612793,
      "learning_rate": 2.0117449664429533e-05,
      "loss": 0.9691,
      "step": 590
    },
    {
      "epoch": 1.342281879194631,
      "grad_norm": 10.510406494140625,
      "learning_rate": 1.9949664429530202e-05,
      "loss": 0.8018,
      "step": 600
    },
    {
      "epoch": 1.3646532438478747,
      "grad_norm": 13.036903381347656,
      "learning_rate": 1.9781879194630872e-05,
      "loss": 0.8072,
      "step": 610
    },
    {
      "epoch": 1.3870246085011186,
      "grad_norm": 8.814906120300293,
      "learning_rate": 1.9614093959731545e-05,
      "loss": 0.8417,
      "step": 620
    },
    {
      "epoch": 1.4093959731543624,
      "grad_norm": 8.039392471313477,
      "learning_rate": 1.9446308724832214e-05,
      "loss": 0.9581,
      "step": 630
    },
    {
      "epoch": 1.4317673378076063,
      "grad_norm": 14.121169090270996,
      "learning_rate": 1.9278523489932887e-05,
      "loss": 0.8229,
      "step": 640
    },
    {
      "epoch": 1.45413870246085,
      "grad_norm": 16.9940242767334,
      "learning_rate": 1.9110738255033557e-05,
      "loss": 0.7882,
      "step": 650
    },
    {
      "epoch": 1.476510067114094,
      "grad_norm": 11.465024948120117,
      "learning_rate": 1.8942953020134227e-05,
      "loss": 0.9247,
      "step": 660
    },
    {
      "epoch": 1.4988814317673378,
      "grad_norm": 31.988805770874023,
      "learning_rate": 1.87751677852349e-05,
      "loss": 1.0752,
      "step": 670
    },
    {
      "epoch": 1.5212527964205815,
      "grad_norm": 10.798734664916992,
      "learning_rate": 1.8607382550335573e-05,
      "loss": 0.9343,
      "step": 680
    },
    {
      "epoch": 1.5436241610738255,
      "grad_norm": 15.875622749328613,
      "learning_rate": 1.843959731543624e-05,
      "loss": 1.0945,
      "step": 690
    },
    {
      "epoch": 1.5659955257270695,
      "grad_norm": 7.514532566070557,
      "learning_rate": 1.8271812080536912e-05,
      "loss": 1.0456,
      "step": 700
    },
    {
      "epoch": 1.5883668903803132,
      "grad_norm": 11.029094696044922,
      "learning_rate": 1.8104026845637585e-05,
      "loss": 0.8911,
      "step": 710
    },
    {
      "epoch": 1.610738255033557,
      "grad_norm": 13.491842269897461,
      "learning_rate": 1.7936241610738258e-05,
      "loss": 0.8506,
      "step": 720
    },
    {
      "epoch": 1.633109619686801,
      "grad_norm": 7.95211935043335,
      "learning_rate": 1.7768456375838924e-05,
      "loss": 0.7254,
      "step": 730
    },
    {
      "epoch": 1.6554809843400449,
      "grad_norm": 34.65481185913086,
      "learning_rate": 1.7600671140939597e-05,
      "loss": 0.864,
      "step": 740
    },
    {
      "epoch": 1.6778523489932886,
      "grad_norm": 11.670961380004883,
      "learning_rate": 1.743288590604027e-05,
      "loss": 0.817,
      "step": 750
    },
    {
      "epoch": 1.7002237136465324,
      "grad_norm": 29.27161979675293,
      "learning_rate": 1.726510067114094e-05,
      "loss": 0.7725,
      "step": 760
    },
    {
      "epoch": 1.7225950782997763,
      "grad_norm": 9.839641571044922,
      "learning_rate": 1.709731543624161e-05,
      "loss": 0.8442,
      "step": 770
    },
    {
      "epoch": 1.7449664429530203,
      "grad_norm": 9.149066925048828,
      "learning_rate": 1.6929530201342283e-05,
      "loss": 1.063,
      "step": 780
    },
    {
      "epoch": 1.767337807606264,
      "grad_norm": 9.665802001953125,
      "learning_rate": 1.6761744966442952e-05,
      "loss": 0.739,
      "step": 790
    },
    {
      "epoch": 1.7897091722595078,
      "grad_norm": 7.875019073486328,
      "learning_rate": 1.6593959731543625e-05,
      "loss": 0.7112,
      "step": 800
    },
    {
      "epoch": 1.8120805369127517,
      "grad_norm": 5.911330699920654,
      "learning_rate": 1.64261744966443e-05,
      "loss": 0.7599,
      "step": 810
    },
    {
      "epoch": 1.8344519015659957,
      "grad_norm": 6.861919403076172,
      "learning_rate": 1.6258389261744965e-05,
      "loss": 0.9531,
      "step": 820
    },
    {
      "epoch": 1.8568232662192394,
      "grad_norm": 15.08234691619873,
      "learning_rate": 1.6090604026845638e-05,
      "loss": 0.8382,
      "step": 830
    },
    {
      "epoch": 1.8791946308724832,
      "grad_norm": 8.711251258850098,
      "learning_rate": 1.592281879194631e-05,
      "loss": 0.8153,
      "step": 840
    },
    {
      "epoch": 1.901565995525727,
      "grad_norm": 9.151041030883789,
      "learning_rate": 1.575503355704698e-05,
      "loss": 0.7409,
      "step": 850
    },
    {
      "epoch": 1.9239373601789709,
      "grad_norm": 21.139633178710938,
      "learning_rate": 1.558724832214765e-05,
      "loss": 1.0023,
      "step": 860
    },
    {
      "epoch": 1.9463087248322148,
      "grad_norm": 12.869996070861816,
      "learning_rate": 1.5419463087248323e-05,
      "loss": 0.964,
      "step": 870
    },
    {
      "epoch": 1.9686800894854586,
      "grad_norm": 7.78395414352417,
      "learning_rate": 1.5251677852348994e-05,
      "loss": 0.7917,
      "step": 880
    },
    {
      "epoch": 1.9910514541387023,
      "grad_norm": 9.886857032775879,
      "learning_rate": 1.5083892617449666e-05,
      "loss": 0.6309,
      "step": 890
    },
    {
      "epoch": 2.0,
      "eval_f1": 0.6602455422468317,
      "eval_loss": 0.8669551610946655,
      "eval_precision": 0.6523089212777071,
      "eval_recall": 0.692358803986711,
      "eval_runtime": 130.8871,
      "eval_samples_per_second": 6.83,
      "eval_steps_per_second": 0.856,
      "step": 894
    },
    {
      "epoch": 2.0134228187919465,
      "grad_norm": 7.215494155883789,
      "learning_rate": 1.4916107382550337e-05,
      "loss": 0.7906,
      "step": 900
    },
    {
      "epoch": 2.0357941834451903,
      "grad_norm": 5.634564399719238,
      "learning_rate": 1.4748322147651007e-05,
      "loss": 0.761,
      "step": 910
    },
    {
      "epoch": 2.058165548098434,
      "grad_norm": 20.779586791992188,
      "learning_rate": 1.4580536912751678e-05,
      "loss": 0.6579,
      "step": 920
    },
    {
      "epoch": 2.0805369127516777,
      "grad_norm": 10.742440223693848,
      "learning_rate": 1.441275167785235e-05,
      "loss": 0.6386,
      "step": 930
    },
    {
      "epoch": 2.1029082774049215,
      "grad_norm": 8.43903923034668,
      "learning_rate": 1.424496644295302e-05,
      "loss": 0.7638,
      "step": 940
    },
    {
      "epoch": 2.1252796420581657,
      "grad_norm": 19.1334171295166,
      "learning_rate": 1.4077181208053692e-05,
      "loss": 0.7029,
      "step": 950
    },
    {
      "epoch": 2.1476510067114094,
      "grad_norm": 3.0524423122406006,
      "learning_rate": 1.3909395973154363e-05,
      "loss": 0.978,
      "step": 960
    },
    {
      "epoch": 2.170022371364653,
      "grad_norm": 15.839861869812012,
      "learning_rate": 1.3741610738255033e-05,
      "loss": 0.7235,
      "step": 970
    },
    {
      "epoch": 2.192393736017897,
      "grad_norm": 12.448015213012695,
      "learning_rate": 1.3573825503355706e-05,
      "loss": 0.7515,
      "step": 980
    },
    {
      "epoch": 2.214765100671141,
      "grad_norm": 8.058697700500488,
      "learning_rate": 1.3406040268456375e-05,
      "loss": 0.6217,
      "step": 990
    },
    {
      "epoch": 2.237136465324385,
      "grad_norm": 7.375150680541992,
      "learning_rate": 1.3238255033557047e-05,
      "loss": 0.7008,
      "step": 1000
    },
    {
      "epoch": 2.2595078299776286,
      "grad_norm": 13.356056213378906,
      "learning_rate": 1.3070469798657718e-05,
      "loss": 0.6514,
      "step": 1010
    },
    {
      "epoch": 2.2818791946308723,
      "grad_norm": 12.400814056396484,
      "learning_rate": 1.290268456375839e-05,
      "loss": 0.7722,
      "step": 1020
    },
    {
      "epoch": 2.3042505592841165,
      "grad_norm": 8.832700729370117,
      "learning_rate": 1.273489932885906e-05,
      "loss": 0.897,
      "step": 1030
    },
    {
      "epoch": 2.3266219239373602,
      "grad_norm": 10.940841674804688,
      "learning_rate": 1.2567114093959732e-05,
      "loss": 0.589,
      "step": 1040
    },
    {
      "epoch": 2.348993288590604,
      "grad_norm": 12.862110137939453,
      "learning_rate": 1.2399328859060403e-05,
      "loss": 0.6633,
      "step": 1050
    },
    {
      "epoch": 2.3713646532438477,
      "grad_norm": 7.077243328094482,
      "learning_rate": 1.2231543624161075e-05,
      "loss": 0.8926,
      "step": 1060
    },
    {
      "epoch": 2.393736017897092,
      "grad_norm": 27.0998592376709,
      "learning_rate": 1.2063758389261746e-05,
      "loss": 0.6407,
      "step": 1070
    },
    {
      "epoch": 2.4161073825503356,
      "grad_norm": 46.02029800415039,
      "learning_rate": 1.1895973154362416e-05,
      "loss": 0.7948,
      "step": 1080
    },
    {
      "epoch": 2.4384787472035794,
      "grad_norm": 28.452024459838867,
      "learning_rate": 1.1728187919463089e-05,
      "loss": 0.895,
      "step": 1090
    },
    {
      "epoch": 2.460850111856823,
      "grad_norm": 4.791912078857422,
      "learning_rate": 1.1560402684563758e-05,
      "loss": 0.7442,
      "step": 1100
    },
    {
      "epoch": 2.4832214765100673,
      "grad_norm": 30.137638092041016,
      "learning_rate": 1.139261744966443e-05,
      "loss": 0.7894,
      "step": 1110
    },
    {
      "epoch": 2.505592841163311,
      "grad_norm": 27.18743133544922,
      "learning_rate": 1.1224832214765101e-05,
      "loss": 0.5726,
      "step": 1120
    },
    {
      "epoch": 2.527964205816555,
      "grad_norm": 26.038005828857422,
      "learning_rate": 1.1057046979865772e-05,
      "loss": 0.6136,
      "step": 1130
    },
    {
      "epoch": 2.5503355704697985,
      "grad_norm": 14.915918350219727,
      "learning_rate": 1.0889261744966442e-05,
      "loss": 0.6462,
      "step": 1140
    },
    {
      "epoch": 2.5727069351230423,
      "grad_norm": 17.5335636138916,
      "learning_rate": 1.0721476510067115e-05,
      "loss": 0.4968,
      "step": 1150
    },
    {
      "epoch": 2.5950782997762865,
      "grad_norm": 10.35541820526123,
      "learning_rate": 1.0553691275167785e-05,
      "loss": 0.7937,
      "step": 1160
    },
    {
      "epoch": 2.61744966442953,
      "grad_norm": 17.452661514282227,
      "learning_rate": 1.0385906040268458e-05,
      "loss": 0.6387,
      "step": 1170
    },
    {
      "epoch": 2.639821029082774,
      "grad_norm": 17.41935920715332,
      "learning_rate": 1.0218120805369127e-05,
      "loss": 0.6518,
      "step": 1180
    },
    {
      "epoch": 2.662192393736018,
      "grad_norm": 4.451584815979004,
      "learning_rate": 1.0050335570469799e-05,
      "loss": 0.6161,
      "step": 1190
    },
    {
      "epoch": 2.684563758389262,
      "grad_norm": 25.335411071777344,
      "learning_rate": 9.88255033557047e-06,
      "loss": 0.853,
      "step": 1200
    },
    {
      "epoch": 2.7069351230425056,
      "grad_norm": 11.961751937866211,
      "learning_rate": 9.714765100671141e-06,
      "loss": 0.7882,
      "step": 1210
    },
    {
      "epoch": 2.7293064876957494,
      "grad_norm": 11.344894409179688,
      "learning_rate": 9.546979865771811e-06,
      "loss": 0.6131,
      "step": 1220
    },
    {
      "epoch": 2.751677852348993,
      "grad_norm": 6.470048904418945,
      "learning_rate": 9.379194630872484e-06,
      "loss": 0.6766,
      "step": 1230
    },
    {
      "epoch": 2.7740492170022373,
      "grad_norm": 21.995107650756836,
      "learning_rate": 9.211409395973154e-06,
      "loss": 0.5748,
      "step": 1240
    },
    {
      "epoch": 2.796420581655481,
      "grad_norm": 21.259807586669922,
      "learning_rate": 9.043624161073827e-06,
      "loss": 0.5948,
      "step": 1250
    },
    {
      "epoch": 2.8187919463087248,
      "grad_norm": 9.049982070922852,
      "learning_rate": 8.875838926174498e-06,
      "loss": 0.6905,
      "step": 1260
    },
    {
      "epoch": 2.841163310961969,
      "grad_norm": 29.27450942993164,
      "learning_rate": 8.708053691275167e-06,
      "loss": 0.6622,
      "step": 1270
    },
    {
      "epoch": 2.8635346756152127,
      "grad_norm": 4.443180084228516,
      "learning_rate": 8.54026845637584e-06,
      "loss": 0.6907,
      "step": 1280
    },
    {
      "epoch": 2.8859060402684564,
      "grad_norm": 9.375423431396484,
      "learning_rate": 8.37248322147651e-06,
      "loss": 0.6335,
      "step": 1290
    },
    {
      "epoch": 2.9082774049217,
      "grad_norm": 15.02713680267334,
      "learning_rate": 8.204697986577181e-06,
      "loss": 0.6284,
      "step": 1300
    },
    {
      "epoch": 2.930648769574944,
      "grad_norm": 19.734182357788086,
      "learning_rate": 8.036912751677853e-06,
      "loss": 0.6597,
      "step": 1310
    },
    {
      "epoch": 2.953020134228188,
      "grad_norm": 6.294586181640625,
      "learning_rate": 7.869127516778524e-06,
      "loss": 0.7776,
      "step": 1320
    },
    {
      "epoch": 2.975391498881432,
      "grad_norm": 11.588424682617188,
      "learning_rate": 7.701342281879194e-06,
      "loss": 0.6819,
      "step": 1330
    },
    {
      "epoch": 2.9977628635346756,
      "grad_norm": 30.317203521728516,
      "learning_rate": 7.533557046979867e-06,
      "loss": 0.6251,
      "step": 1340
    },
    {
      "epoch": 3.0,
      "eval_f1": 0.6974591793030971,
      "eval_loss": 0.7666971683502197,
      "eval_precision": 0.6879897703990999,
      "eval_recall": 0.720265780730897,
      "eval_runtime": 130.273,
      "eval_samples_per_second": 6.863,
      "eval_steps_per_second": 0.86,
      "step": 1341
    }
  ],
  "logging_steps": 10,
  "max_steps": 1788,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 59355477430272.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
