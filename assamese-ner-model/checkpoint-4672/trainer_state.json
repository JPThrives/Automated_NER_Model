{
  "best_global_step": 4672,
  "best_metric": 0.7407808303833008,
  "best_model_checkpoint": "./assamese-ner-model\\checkpoint-4672",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 4672,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004280821917808219,
      "grad_norm": 9.143527030944824,
      "learning_rate": 2.9976883561643835e-05,
      "loss": 3.2571,
      "step": 10
    },
    {
      "epoch": 0.008561643835616438,
      "grad_norm": 7.429162979125977,
      "learning_rate": 2.9951198630136988e-05,
      "loss": 2.773,
      "step": 20
    },
    {
      "epoch": 0.012842465753424657,
      "grad_norm": 9.300058364868164,
      "learning_rate": 2.9925513698630138e-05,
      "loss": 2.5513,
      "step": 30
    },
    {
      "epoch": 0.017123287671232876,
      "grad_norm": 11.095643043518066,
      "learning_rate": 2.9899828767123288e-05,
      "loss": 2.2438,
      "step": 40
    },
    {
      "epoch": 0.021404109589041095,
      "grad_norm": 5.4018874168396,
      "learning_rate": 2.9874143835616437e-05,
      "loss": 2.0131,
      "step": 50
    },
    {
      "epoch": 0.025684931506849314,
      "grad_norm": 6.561296463012695,
      "learning_rate": 2.984845890410959e-05,
      "loss": 1.9834,
      "step": 60
    },
    {
      "epoch": 0.029965753424657533,
      "grad_norm": 9.898076057434082,
      "learning_rate": 2.982277397260274e-05,
      "loss": 1.8944,
      "step": 70
    },
    {
      "epoch": 0.03424657534246575,
      "grad_norm": 5.873605251312256,
      "learning_rate": 2.979708904109589e-05,
      "loss": 2.1073,
      "step": 80
    },
    {
      "epoch": 0.038527397260273974,
      "grad_norm": 9.62494945526123,
      "learning_rate": 2.977140410958904e-05,
      "loss": 1.642,
      "step": 90
    },
    {
      "epoch": 0.04280821917808219,
      "grad_norm": 7.739095687866211,
      "learning_rate": 2.9745719178082193e-05,
      "loss": 1.8328,
      "step": 100
    },
    {
      "epoch": 0.04708904109589041,
      "grad_norm": 10.027473449707031,
      "learning_rate": 2.9720034246575343e-05,
      "loss": 1.5973,
      "step": 110
    },
    {
      "epoch": 0.05136986301369863,
      "grad_norm": 9.248682975769043,
      "learning_rate": 2.9694349315068496e-05,
      "loss": 1.3256,
      "step": 120
    },
    {
      "epoch": 0.05565068493150685,
      "grad_norm": 11.150430679321289,
      "learning_rate": 2.9668664383561642e-05,
      "loss": 1.8203,
      "step": 130
    },
    {
      "epoch": 0.059931506849315065,
      "grad_norm": 7.573376178741455,
      "learning_rate": 2.9642979452054796e-05,
      "loss": 1.7003,
      "step": 140
    },
    {
      "epoch": 0.0642123287671233,
      "grad_norm": 27.757863998413086,
      "learning_rate": 2.9617294520547945e-05,
      "loss": 1.4981,
      "step": 150
    },
    {
      "epoch": 0.0684931506849315,
      "grad_norm": 3.9192657470703125,
      "learning_rate": 2.95916095890411e-05,
      "loss": 1.3755,
      "step": 160
    },
    {
      "epoch": 0.07277397260273973,
      "grad_norm": 15.962122917175293,
      "learning_rate": 2.9565924657534245e-05,
      "loss": 1.3564,
      "step": 170
    },
    {
      "epoch": 0.07705479452054795,
      "grad_norm": 6.152198791503906,
      "learning_rate": 2.9540239726027398e-05,
      "loss": 1.4281,
      "step": 180
    },
    {
      "epoch": 0.08133561643835617,
      "grad_norm": 12.189838409423828,
      "learning_rate": 2.9514554794520548e-05,
      "loss": 1.7825,
      "step": 190
    },
    {
      "epoch": 0.08561643835616438,
      "grad_norm": 15.862617492675781,
      "learning_rate": 2.94888698630137e-05,
      "loss": 1.4193,
      "step": 200
    },
    {
      "epoch": 0.0898972602739726,
      "grad_norm": 10.695959091186523,
      "learning_rate": 2.946318493150685e-05,
      "loss": 1.3413,
      "step": 210
    },
    {
      "epoch": 0.09417808219178082,
      "grad_norm": 8.591190338134766,
      "learning_rate": 2.94375e-05,
      "loss": 1.4734,
      "step": 220
    },
    {
      "epoch": 0.09845890410958905,
      "grad_norm": 23.265018463134766,
      "learning_rate": 2.941181506849315e-05,
      "loss": 1.3237,
      "step": 230
    },
    {
      "epoch": 0.10273972602739725,
      "grad_norm": 12.896937370300293,
      "learning_rate": 2.9386130136986304e-05,
      "loss": 1.281,
      "step": 240
    },
    {
      "epoch": 0.10702054794520548,
      "grad_norm": 7.430508613586426,
      "learning_rate": 2.9360445205479453e-05,
      "loss": 1.5752,
      "step": 250
    },
    {
      "epoch": 0.1113013698630137,
      "grad_norm": 9.0586519241333,
      "learning_rate": 2.9334760273972607e-05,
      "loss": 1.1468,
      "step": 260
    },
    {
      "epoch": 0.11558219178082192,
      "grad_norm": 9.041784286499023,
      "learning_rate": 2.9309075342465753e-05,
      "loss": 1.3621,
      "step": 270
    },
    {
      "epoch": 0.11986301369863013,
      "grad_norm": 17.757610321044922,
      "learning_rate": 2.9283390410958906e-05,
      "loss": 1.3347,
      "step": 280
    },
    {
      "epoch": 0.12414383561643835,
      "grad_norm": 6.512852668762207,
      "learning_rate": 2.9257705479452056e-05,
      "loss": 1.2847,
      "step": 290
    },
    {
      "epoch": 0.1284246575342466,
      "grad_norm": 11.371417999267578,
      "learning_rate": 2.923202054794521e-05,
      "loss": 1.3513,
      "step": 300
    },
    {
      "epoch": 0.1327054794520548,
      "grad_norm": 10.900997161865234,
      "learning_rate": 2.9206335616438355e-05,
      "loss": 1.3872,
      "step": 310
    },
    {
      "epoch": 0.136986301369863,
      "grad_norm": 25.528583526611328,
      "learning_rate": 2.9180650684931505e-05,
      "loss": 0.9956,
      "step": 320
    },
    {
      "epoch": 0.14126712328767124,
      "grad_norm": 11.505348205566406,
      "learning_rate": 2.915496575342466e-05,
      "loss": 1.3819,
      "step": 330
    },
    {
      "epoch": 0.14554794520547945,
      "grad_norm": 11.744062423706055,
      "learning_rate": 2.9129280821917808e-05,
      "loss": 1.1022,
      "step": 340
    },
    {
      "epoch": 0.14982876712328766,
      "grad_norm": 10.921093940734863,
      "learning_rate": 2.910359589041096e-05,
      "loss": 1.2909,
      "step": 350
    },
    {
      "epoch": 0.1541095890410959,
      "grad_norm": 28.27537727355957,
      "learning_rate": 2.9077910958904108e-05,
      "loss": 1.1084,
      "step": 360
    },
    {
      "epoch": 0.1583904109589041,
      "grad_norm": 14.745160102844238,
      "learning_rate": 2.905222602739726e-05,
      "loss": 1.3607,
      "step": 370
    },
    {
      "epoch": 0.16267123287671234,
      "grad_norm": 9.820703506469727,
      "learning_rate": 2.902654109589041e-05,
      "loss": 1.144,
      "step": 380
    },
    {
      "epoch": 0.16695205479452055,
      "grad_norm": 20.382030487060547,
      "learning_rate": 2.9000856164383564e-05,
      "loss": 1.1846,
      "step": 390
    },
    {
      "epoch": 0.17123287671232876,
      "grad_norm": 11.614411354064941,
      "learning_rate": 2.897517123287671e-05,
      "loss": 1.1864,
      "step": 400
    },
    {
      "epoch": 0.175513698630137,
      "grad_norm": 18.76705551147461,
      "learning_rate": 2.8949486301369863e-05,
      "loss": 1.229,
      "step": 410
    },
    {
      "epoch": 0.1797945205479452,
      "grad_norm": 14.084985733032227,
      "learning_rate": 2.8923801369863013e-05,
      "loss": 0.9564,
      "step": 420
    },
    {
      "epoch": 0.1840753424657534,
      "grad_norm": 15.900201797485352,
      "learning_rate": 2.8898116438356166e-05,
      "loss": 1.0985,
      "step": 430
    },
    {
      "epoch": 0.18835616438356165,
      "grad_norm": 13.179973602294922,
      "learning_rate": 2.8872431506849316e-05,
      "loss": 0.9747,
      "step": 440
    },
    {
      "epoch": 0.19263698630136986,
      "grad_norm": 26.49134635925293,
      "learning_rate": 2.8846746575342466e-05,
      "loss": 1.2553,
      "step": 450
    },
    {
      "epoch": 0.1969178082191781,
      "grad_norm": 20.5519962310791,
      "learning_rate": 2.8821061643835616e-05,
      "loss": 0.8543,
      "step": 460
    },
    {
      "epoch": 0.2011986301369863,
      "grad_norm": 21.370920181274414,
      "learning_rate": 2.879537671232877e-05,
      "loss": 1.1098,
      "step": 470
    },
    {
      "epoch": 0.2054794520547945,
      "grad_norm": 15.452706336975098,
      "learning_rate": 2.876969178082192e-05,
      "loss": 1.1207,
      "step": 480
    },
    {
      "epoch": 0.20976027397260275,
      "grad_norm": 41.02529525756836,
      "learning_rate": 2.874400684931507e-05,
      "loss": 0.8128,
      "step": 490
    },
    {
      "epoch": 0.21404109589041095,
      "grad_norm": 23.58036231994629,
      "learning_rate": 2.8718321917808218e-05,
      "loss": 1.0838,
      "step": 500
    },
    {
      "epoch": 0.2183219178082192,
      "grad_norm": 17.155776977539062,
      "learning_rate": 2.869263698630137e-05,
      "loss": 0.8445,
      "step": 510
    },
    {
      "epoch": 0.2226027397260274,
      "grad_norm": 15.116822242736816,
      "learning_rate": 2.866695205479452e-05,
      "loss": 1.2592,
      "step": 520
    },
    {
      "epoch": 0.2268835616438356,
      "grad_norm": 6.698291301727295,
      "learning_rate": 2.8641267123287674e-05,
      "loss": 1.0454,
      "step": 530
    },
    {
      "epoch": 0.23116438356164384,
      "grad_norm": 27.467140197753906,
      "learning_rate": 2.861558219178082e-05,
      "loss": 0.8242,
      "step": 540
    },
    {
      "epoch": 0.23544520547945205,
      "grad_norm": 16.7839412689209,
      "learning_rate": 2.8589897260273974e-05,
      "loss": 1.0546,
      "step": 550
    },
    {
      "epoch": 0.23972602739726026,
      "grad_norm": 8.238468170166016,
      "learning_rate": 2.8564212328767124e-05,
      "loss": 0.988,
      "step": 560
    },
    {
      "epoch": 0.2440068493150685,
      "grad_norm": 22.661073684692383,
      "learning_rate": 2.8538527397260277e-05,
      "loss": 0.9039,
      "step": 570
    },
    {
      "epoch": 0.2482876712328767,
      "grad_norm": 15.265829086303711,
      "learning_rate": 2.8512842465753427e-05,
      "loss": 1.0021,
      "step": 580
    },
    {
      "epoch": 0.2525684931506849,
      "grad_norm": 49.56181335449219,
      "learning_rate": 2.8487157534246576e-05,
      "loss": 1.146,
      "step": 590
    },
    {
      "epoch": 0.2568493150684932,
      "grad_norm": 14.604925155639648,
      "learning_rate": 2.8461472602739726e-05,
      "loss": 0.9398,
      "step": 600
    },
    {
      "epoch": 0.2611301369863014,
      "grad_norm": 8.990811347961426,
      "learning_rate": 2.843578767123288e-05,
      "loss": 0.7949,
      "step": 610
    },
    {
      "epoch": 0.2654109589041096,
      "grad_norm": 24.059226989746094,
      "learning_rate": 2.841010273972603e-05,
      "loss": 1.1607,
      "step": 620
    },
    {
      "epoch": 0.2696917808219178,
      "grad_norm": 8.618311882019043,
      "learning_rate": 2.838441780821918e-05,
      "loss": 1.0555,
      "step": 630
    },
    {
      "epoch": 0.273972602739726,
      "grad_norm": 9.342193603515625,
      "learning_rate": 2.835873287671233e-05,
      "loss": 1.0055,
      "step": 640
    },
    {
      "epoch": 0.2782534246575342,
      "grad_norm": 11.598215103149414,
      "learning_rate": 2.8333047945205482e-05,
      "loss": 1.1484,
      "step": 650
    },
    {
      "epoch": 0.2825342465753425,
      "grad_norm": 25.86655616760254,
      "learning_rate": 2.8307363013698632e-05,
      "loss": 0.9345,
      "step": 660
    },
    {
      "epoch": 0.2868150684931507,
      "grad_norm": 29.468690872192383,
      "learning_rate": 2.828167808219178e-05,
      "loss": 1.0117,
      "step": 670
    },
    {
      "epoch": 0.2910958904109589,
      "grad_norm": 21.54947280883789,
      "learning_rate": 2.825599315068493e-05,
      "loss": 1.032,
      "step": 680
    },
    {
      "epoch": 0.2953767123287671,
      "grad_norm": 13.336138725280762,
      "learning_rate": 2.823030821917808e-05,
      "loss": 1.0047,
      "step": 690
    },
    {
      "epoch": 0.2996575342465753,
      "grad_norm": 36.05213165283203,
      "learning_rate": 2.8204623287671234e-05,
      "loss": 1.0083,
      "step": 700
    },
    {
      "epoch": 0.3039383561643836,
      "grad_norm": 36.2824821472168,
      "learning_rate": 2.8178938356164384e-05,
      "loss": 0.8792,
      "step": 710
    },
    {
      "epoch": 0.3082191780821918,
      "grad_norm": 15.812623977661133,
      "learning_rate": 2.8153253424657534e-05,
      "loss": 0.9901,
      "step": 720
    },
    {
      "epoch": 0.3125,
      "grad_norm": 13.622113227844238,
      "learning_rate": 2.8127568493150684e-05,
      "loss": 1.08,
      "step": 730
    },
    {
      "epoch": 0.3167808219178082,
      "grad_norm": 19.393569946289062,
      "learning_rate": 2.8101883561643837e-05,
      "loss": 0.8416,
      "step": 740
    },
    {
      "epoch": 0.3210616438356164,
      "grad_norm": 9.216547966003418,
      "learning_rate": 2.8076198630136987e-05,
      "loss": 0.9563,
      "step": 750
    },
    {
      "epoch": 0.3253424657534247,
      "grad_norm": 64.56256103515625,
      "learning_rate": 2.805051369863014e-05,
      "loss": 0.9758,
      "step": 760
    },
    {
      "epoch": 0.3296232876712329,
      "grad_norm": 16.644256591796875,
      "learning_rate": 2.8024828767123286e-05,
      "loss": 0.9189,
      "step": 770
    },
    {
      "epoch": 0.3339041095890411,
      "grad_norm": 22.02782440185547,
      "learning_rate": 2.799914383561644e-05,
      "loss": 1.0309,
      "step": 780
    },
    {
      "epoch": 0.3381849315068493,
      "grad_norm": 14.417555809020996,
      "learning_rate": 2.797345890410959e-05,
      "loss": 1.0168,
      "step": 790
    },
    {
      "epoch": 0.3424657534246575,
      "grad_norm": 17.383155822753906,
      "learning_rate": 2.7947773972602742e-05,
      "loss": 0.8795,
      "step": 800
    },
    {
      "epoch": 0.3467465753424658,
      "grad_norm": 16.478063583374023,
      "learning_rate": 2.792208904109589e-05,
      "loss": 0.9981,
      "step": 810
    },
    {
      "epoch": 0.351027397260274,
      "grad_norm": 46.852333068847656,
      "learning_rate": 2.7896404109589042e-05,
      "loss": 0.8396,
      "step": 820
    },
    {
      "epoch": 0.3553082191780822,
      "grad_norm": 16.336999893188477,
      "learning_rate": 2.787071917808219e-05,
      "loss": 0.8869,
      "step": 830
    },
    {
      "epoch": 0.3595890410958904,
      "grad_norm": 18.43584442138672,
      "learning_rate": 2.7845034246575345e-05,
      "loss": 1.0838,
      "step": 840
    },
    {
      "epoch": 0.3638698630136986,
      "grad_norm": 27.185283660888672,
      "learning_rate": 2.7819349315068495e-05,
      "loss": 0.7127,
      "step": 850
    },
    {
      "epoch": 0.3681506849315068,
      "grad_norm": 9.711454391479492,
      "learning_rate": 2.7793664383561644e-05,
      "loss": 0.6737,
      "step": 860
    },
    {
      "epoch": 0.3724315068493151,
      "grad_norm": 18.34222412109375,
      "learning_rate": 2.7767979452054794e-05,
      "loss": 1.108,
      "step": 870
    },
    {
      "epoch": 0.3767123287671233,
      "grad_norm": 9.437512397766113,
      "learning_rate": 2.7742294520547947e-05,
      "loss": 0.9617,
      "step": 880
    },
    {
      "epoch": 0.3809931506849315,
      "grad_norm": 6.7784905433654785,
      "learning_rate": 2.7716609589041097e-05,
      "loss": 0.894,
      "step": 890
    },
    {
      "epoch": 0.3852739726027397,
      "grad_norm": 14.5828218460083,
      "learning_rate": 2.7690924657534247e-05,
      "loss": 0.8916,
      "step": 900
    },
    {
      "epoch": 0.3895547945205479,
      "grad_norm": 13.952213287353516,
      "learning_rate": 2.7665239726027397e-05,
      "loss": 1.2107,
      "step": 910
    },
    {
      "epoch": 0.3938356164383562,
      "grad_norm": 23.894481658935547,
      "learning_rate": 2.763955479452055e-05,
      "loss": 1.0384,
      "step": 920
    },
    {
      "epoch": 0.3981164383561644,
      "grad_norm": 12.81005573272705,
      "learning_rate": 2.76138698630137e-05,
      "loss": 0.8794,
      "step": 930
    },
    {
      "epoch": 0.4023972602739726,
      "grad_norm": 11.605198860168457,
      "learning_rate": 2.7588184931506853e-05,
      "loss": 1.0815,
      "step": 940
    },
    {
      "epoch": 0.4066780821917808,
      "grad_norm": 19.173080444335938,
      "learning_rate": 2.75625e-05,
      "loss": 0.879,
      "step": 950
    },
    {
      "epoch": 0.410958904109589,
      "grad_norm": 16.12763786315918,
      "learning_rate": 2.7536815068493152e-05,
      "loss": 1.2316,
      "step": 960
    },
    {
      "epoch": 0.4152397260273973,
      "grad_norm": 11.04924488067627,
      "learning_rate": 2.7511130136986302e-05,
      "loss": 1.0372,
      "step": 970
    },
    {
      "epoch": 0.4195205479452055,
      "grad_norm": 12.3121337890625,
      "learning_rate": 2.7485445205479455e-05,
      "loss": 0.8087,
      "step": 980
    },
    {
      "epoch": 0.4238013698630137,
      "grad_norm": 31.46891212463379,
      "learning_rate": 2.7459760273972605e-05,
      "loss": 0.9003,
      "step": 990
    },
    {
      "epoch": 0.4280821917808219,
      "grad_norm": 12.546882629394531,
      "learning_rate": 2.7434075342465755e-05,
      "loss": 1.07,
      "step": 1000
    },
    {
      "epoch": 0.4323630136986301,
      "grad_norm": 8.71003532409668,
      "learning_rate": 2.7408390410958905e-05,
      "loss": 0.9106,
      "step": 1010
    },
    {
      "epoch": 0.4366438356164384,
      "grad_norm": 42.478759765625,
      "learning_rate": 2.7382705479452054e-05,
      "loss": 0.8865,
      "step": 1020
    },
    {
      "epoch": 0.4409246575342466,
      "grad_norm": 15.516157150268555,
      "learning_rate": 2.7357020547945208e-05,
      "loss": 1.078,
      "step": 1030
    },
    {
      "epoch": 0.4452054794520548,
      "grad_norm": 20.25251579284668,
      "learning_rate": 2.7331335616438354e-05,
      "loss": 0.9342,
      "step": 1040
    },
    {
      "epoch": 0.449486301369863,
      "grad_norm": 5.450655937194824,
      "learning_rate": 2.7305650684931507e-05,
      "loss": 0.7869,
      "step": 1050
    },
    {
      "epoch": 0.4537671232876712,
      "grad_norm": 26.11355972290039,
      "learning_rate": 2.7279965753424657e-05,
      "loss": 1.1147,
      "step": 1060
    },
    {
      "epoch": 0.4580479452054795,
      "grad_norm": 11.541396141052246,
      "learning_rate": 2.725428082191781e-05,
      "loss": 0.9773,
      "step": 1070
    },
    {
      "epoch": 0.4623287671232877,
      "grad_norm": 9.537392616271973,
      "learning_rate": 2.722859589041096e-05,
      "loss": 0.6934,
      "step": 1080
    },
    {
      "epoch": 0.4666095890410959,
      "grad_norm": 22.873441696166992,
      "learning_rate": 2.720291095890411e-05,
      "loss": 0.9617,
      "step": 1090
    },
    {
      "epoch": 0.4708904109589041,
      "grad_norm": 10.116670608520508,
      "learning_rate": 2.717722602739726e-05,
      "loss": 0.9711,
      "step": 1100
    },
    {
      "epoch": 0.4751712328767123,
      "grad_norm": 20.031970977783203,
      "learning_rate": 2.7151541095890413e-05,
      "loss": 1.0581,
      "step": 1110
    },
    {
      "epoch": 0.4794520547945205,
      "grad_norm": 19.42740821838379,
      "learning_rate": 2.7125856164383562e-05,
      "loss": 0.8627,
      "step": 1120
    },
    {
      "epoch": 0.4837328767123288,
      "grad_norm": 12.646263122558594,
      "learning_rate": 2.7100171232876712e-05,
      "loss": 0.8751,
      "step": 1130
    },
    {
      "epoch": 0.488013698630137,
      "grad_norm": 4.523874759674072,
      "learning_rate": 2.7074486301369862e-05,
      "loss": 0.8219,
      "step": 1140
    },
    {
      "epoch": 0.4922945205479452,
      "grad_norm": 13.249631881713867,
      "learning_rate": 2.7048801369863015e-05,
      "loss": 0.7833,
      "step": 1150
    },
    {
      "epoch": 0.4965753424657534,
      "grad_norm": 7.592014789581299,
      "learning_rate": 2.7023116438356165e-05,
      "loss": 0.8157,
      "step": 1160
    },
    {
      "epoch": 0.5008561643835616,
      "grad_norm": 34.3780403137207,
      "learning_rate": 2.6997431506849318e-05,
      "loss": 0.9072,
      "step": 1170
    },
    {
      "epoch": 0.5051369863013698,
      "grad_norm": 30.691129684448242,
      "learning_rate": 2.6971746575342464e-05,
      "loss": 0.6879,
      "step": 1180
    },
    {
      "epoch": 0.509417808219178,
      "grad_norm": 6.798182487487793,
      "learning_rate": 2.6946061643835618e-05,
      "loss": 0.8668,
      "step": 1190
    },
    {
      "epoch": 0.5136986301369864,
      "grad_norm": 39.035675048828125,
      "learning_rate": 2.6920376712328767e-05,
      "loss": 0.8607,
      "step": 1200
    },
    {
      "epoch": 0.5179794520547946,
      "grad_norm": 14.542919158935547,
      "learning_rate": 2.689469178082192e-05,
      "loss": 0.811,
      "step": 1210
    },
    {
      "epoch": 0.5222602739726028,
      "grad_norm": 8.363486289978027,
      "learning_rate": 2.6869006849315067e-05,
      "loss": 0.6759,
      "step": 1220
    },
    {
      "epoch": 0.526541095890411,
      "grad_norm": 6.7858123779296875,
      "learning_rate": 2.684332191780822e-05,
      "loss": 0.6917,
      "step": 1230
    },
    {
      "epoch": 0.5308219178082192,
      "grad_norm": 32.5490837097168,
      "learning_rate": 2.681763698630137e-05,
      "loss": 0.9429,
      "step": 1240
    },
    {
      "epoch": 0.5351027397260274,
      "grad_norm": 6.6626458168029785,
      "learning_rate": 2.6791952054794523e-05,
      "loss": 0.8783,
      "step": 1250
    },
    {
      "epoch": 0.5393835616438356,
      "grad_norm": 14.975225448608398,
      "learning_rate": 2.6766267123287673e-05,
      "loss": 0.7047,
      "step": 1260
    },
    {
      "epoch": 0.5436643835616438,
      "grad_norm": 13.297146797180176,
      "learning_rate": 2.6740582191780823e-05,
      "loss": 0.8957,
      "step": 1270
    },
    {
      "epoch": 0.547945205479452,
      "grad_norm": 12.45203685760498,
      "learning_rate": 2.6714897260273972e-05,
      "loss": 0.88,
      "step": 1280
    },
    {
      "epoch": 0.5522260273972602,
      "grad_norm": 4.13766622543335,
      "learning_rate": 2.6689212328767126e-05,
      "loss": 0.6168,
      "step": 1290
    },
    {
      "epoch": 0.5565068493150684,
      "grad_norm": 45.81571578979492,
      "learning_rate": 2.6663527397260275e-05,
      "loss": 1.0284,
      "step": 1300
    },
    {
      "epoch": 0.5607876712328768,
      "grad_norm": 8.625859260559082,
      "learning_rate": 2.663784246575343e-05,
      "loss": 0.5634,
      "step": 1310
    },
    {
      "epoch": 0.565068493150685,
      "grad_norm": 20.825454711914062,
      "learning_rate": 2.6612157534246575e-05,
      "loss": 0.9129,
      "step": 1320
    },
    {
      "epoch": 0.5693493150684932,
      "grad_norm": 12.970701217651367,
      "learning_rate": 2.6586472602739728e-05,
      "loss": 0.686,
      "step": 1330
    },
    {
      "epoch": 0.5736301369863014,
      "grad_norm": 21.17759895324707,
      "learning_rate": 2.6560787671232878e-05,
      "loss": 1.027,
      "step": 1340
    },
    {
      "epoch": 0.5779109589041096,
      "grad_norm": 4.656286716461182,
      "learning_rate": 2.6535102739726028e-05,
      "loss": 0.7063,
      "step": 1350
    },
    {
      "epoch": 0.5821917808219178,
      "grad_norm": 11.171497344970703,
      "learning_rate": 2.6509417808219177e-05,
      "loss": 0.8304,
      "step": 1360
    },
    {
      "epoch": 0.586472602739726,
      "grad_norm": 13.476147651672363,
      "learning_rate": 2.6483732876712327e-05,
      "loss": 0.7401,
      "step": 1370
    },
    {
      "epoch": 0.5907534246575342,
      "grad_norm": 11.683135032653809,
      "learning_rate": 2.645804794520548e-05,
      "loss": 0.8161,
      "step": 1380
    },
    {
      "epoch": 0.5950342465753424,
      "grad_norm": 7.547550678253174,
      "learning_rate": 2.643236301369863e-05,
      "loss": 0.8678,
      "step": 1390
    },
    {
      "epoch": 0.5993150684931506,
      "grad_norm": 4.967058181762695,
      "learning_rate": 2.6406678082191783e-05,
      "loss": 0.5675,
      "step": 1400
    },
    {
      "epoch": 0.603595890410959,
      "grad_norm": 8.383954048156738,
      "learning_rate": 2.638099315068493e-05,
      "loss": 0.7366,
      "step": 1410
    },
    {
      "epoch": 0.6078767123287672,
      "grad_norm": 3.407871723175049,
      "learning_rate": 2.6355308219178083e-05,
      "loss": 0.8091,
      "step": 1420
    },
    {
      "epoch": 0.6121575342465754,
      "grad_norm": 3.6211371421813965,
      "learning_rate": 2.6329623287671233e-05,
      "loss": 0.5425,
      "step": 1430
    },
    {
      "epoch": 0.6164383561643836,
      "grad_norm": 8.027620315551758,
      "learning_rate": 2.6303938356164386e-05,
      "loss": 1.2665,
      "step": 1440
    },
    {
      "epoch": 0.6207191780821918,
      "grad_norm": 10.842984199523926,
      "learning_rate": 2.6278253424657532e-05,
      "loss": 0.8933,
      "step": 1450
    },
    {
      "epoch": 0.625,
      "grad_norm": 17.257038116455078,
      "learning_rate": 2.6252568493150685e-05,
      "loss": 0.9975,
      "step": 1460
    },
    {
      "epoch": 0.6292808219178082,
      "grad_norm": 8.613531112670898,
      "learning_rate": 2.6226883561643835e-05,
      "loss": 0.9868,
      "step": 1470
    },
    {
      "epoch": 0.6335616438356164,
      "grad_norm": 12.613505363464355,
      "learning_rate": 2.620119863013699e-05,
      "loss": 0.8849,
      "step": 1480
    },
    {
      "epoch": 0.6378424657534246,
      "grad_norm": 15.471649169921875,
      "learning_rate": 2.6175513698630138e-05,
      "loss": 0.7888,
      "step": 1490
    },
    {
      "epoch": 0.6421232876712328,
      "grad_norm": 9.704048156738281,
      "learning_rate": 2.6149828767123288e-05,
      "loss": 0.6014,
      "step": 1500
    },
    {
      "epoch": 0.646404109589041,
      "grad_norm": 18.230037689208984,
      "learning_rate": 2.6124143835616438e-05,
      "loss": 0.7973,
      "step": 1510
    },
    {
      "epoch": 0.6506849315068494,
      "grad_norm": 13.136951446533203,
      "learning_rate": 2.609845890410959e-05,
      "loss": 0.54,
      "step": 1520
    },
    {
      "epoch": 0.6549657534246576,
      "grad_norm": 37.58016586303711,
      "learning_rate": 2.607277397260274e-05,
      "loss": 0.9804,
      "step": 1530
    },
    {
      "epoch": 0.6592465753424658,
      "grad_norm": 11.665778160095215,
      "learning_rate": 2.604708904109589e-05,
      "loss": 0.7955,
      "step": 1540
    },
    {
      "epoch": 0.663527397260274,
      "grad_norm": 15.988519668579102,
      "learning_rate": 2.602140410958904e-05,
      "loss": 1.2159,
      "step": 1550
    },
    {
      "epoch": 0.6678082191780822,
      "grad_norm": 12.905730247497559,
      "learning_rate": 2.5995719178082193e-05,
      "loss": 0.9387,
      "step": 1560
    },
    {
      "epoch": 0.6720890410958904,
      "grad_norm": 5.530625343322754,
      "learning_rate": 2.5970034246575343e-05,
      "loss": 0.7292,
      "step": 1570
    },
    {
      "epoch": 0.6763698630136986,
      "grad_norm": 24.208515167236328,
      "learning_rate": 2.5944349315068496e-05,
      "loss": 0.7722,
      "step": 1580
    },
    {
      "epoch": 0.6806506849315068,
      "grad_norm": 14.226933479309082,
      "learning_rate": 2.5918664383561643e-05,
      "loss": 0.7271,
      "step": 1590
    },
    {
      "epoch": 0.684931506849315,
      "grad_norm": 15.042016983032227,
      "learning_rate": 2.5892979452054796e-05,
      "loss": 0.7905,
      "step": 1600
    },
    {
      "epoch": 0.6892123287671232,
      "grad_norm": 35.65140914916992,
      "learning_rate": 2.5867294520547946e-05,
      "loss": 0.5681,
      "step": 1610
    },
    {
      "epoch": 0.6934931506849316,
      "grad_norm": 10.235920906066895,
      "learning_rate": 2.58416095890411e-05,
      "loss": 0.8,
      "step": 1620
    },
    {
      "epoch": 0.6977739726027398,
      "grad_norm": 27.296335220336914,
      "learning_rate": 2.5815924657534245e-05,
      "loss": 0.9603,
      "step": 1630
    },
    {
      "epoch": 0.702054794520548,
      "grad_norm": 25.194801330566406,
      "learning_rate": 2.57902397260274e-05,
      "loss": 0.82,
      "step": 1640
    },
    {
      "epoch": 0.7063356164383562,
      "grad_norm": 36.33634567260742,
      "learning_rate": 2.5764554794520548e-05,
      "loss": 0.7717,
      "step": 1650
    },
    {
      "epoch": 0.7106164383561644,
      "grad_norm": 15.009870529174805,
      "learning_rate": 2.57388698630137e-05,
      "loss": 0.7878,
      "step": 1660
    },
    {
      "epoch": 0.7148972602739726,
      "grad_norm": 34.94595718383789,
      "learning_rate": 2.571318493150685e-05,
      "loss": 0.6897,
      "step": 1670
    },
    {
      "epoch": 0.7191780821917808,
      "grad_norm": 3.3885648250579834,
      "learning_rate": 2.56875e-05,
      "loss": 0.6894,
      "step": 1680
    },
    {
      "epoch": 0.723458904109589,
      "grad_norm": 18.04899024963379,
      "learning_rate": 2.566181506849315e-05,
      "loss": 0.6855,
      "step": 1690
    },
    {
      "epoch": 0.7277397260273972,
      "grad_norm": 23.512062072753906,
      "learning_rate": 2.56361301369863e-05,
      "loss": 0.8258,
      "step": 1700
    },
    {
      "epoch": 0.7320205479452054,
      "grad_norm": 13.760478973388672,
      "learning_rate": 2.5610445205479454e-05,
      "loss": 0.8224,
      "step": 1710
    },
    {
      "epoch": 0.7363013698630136,
      "grad_norm": 17.022178649902344,
      "learning_rate": 2.5584760273972603e-05,
      "loss": 0.8214,
      "step": 1720
    },
    {
      "epoch": 0.740582191780822,
      "grad_norm": 93.08991241455078,
      "learning_rate": 2.5559075342465753e-05,
      "loss": 0.6138,
      "step": 1730
    },
    {
      "epoch": 0.7448630136986302,
      "grad_norm": 12.598793983459473,
      "learning_rate": 2.5533390410958903e-05,
      "loss": 0.6085,
      "step": 1740
    },
    {
      "epoch": 0.7491438356164384,
      "grad_norm": 10.264090538024902,
      "learning_rate": 2.5507705479452056e-05,
      "loss": 0.4778,
      "step": 1750
    },
    {
      "epoch": 0.7534246575342466,
      "grad_norm": 16.948760986328125,
      "learning_rate": 2.5482020547945206e-05,
      "loss": 0.7543,
      "step": 1760
    },
    {
      "epoch": 0.7577054794520548,
      "grad_norm": 16.17952537536621,
      "learning_rate": 2.5456335616438356e-05,
      "loss": 0.676,
      "step": 1770
    },
    {
      "epoch": 0.761986301369863,
      "grad_norm": 16.751449584960938,
      "learning_rate": 2.5430650684931506e-05,
      "loss": 0.6894,
      "step": 1780
    },
    {
      "epoch": 0.7662671232876712,
      "grad_norm": 10.731999397277832,
      "learning_rate": 2.540496575342466e-05,
      "loss": 0.6445,
      "step": 1790
    },
    {
      "epoch": 0.7705479452054794,
      "grad_norm": 5.810342788696289,
      "learning_rate": 2.537928082191781e-05,
      "loss": 0.79,
      "step": 1800
    },
    {
      "epoch": 0.7748287671232876,
      "grad_norm": 23.382064819335938,
      "learning_rate": 2.535359589041096e-05,
      "loss": 0.9898,
      "step": 1810
    },
    {
      "epoch": 0.7791095890410958,
      "grad_norm": 10.761639595031738,
      "learning_rate": 2.5327910958904108e-05,
      "loss": 0.5624,
      "step": 1820
    },
    {
      "epoch": 0.7833904109589042,
      "grad_norm": 13.678400993347168,
      "learning_rate": 2.530222602739726e-05,
      "loss": 0.9474,
      "step": 1830
    },
    {
      "epoch": 0.7876712328767124,
      "grad_norm": 13.515304565429688,
      "learning_rate": 2.527654109589041e-05,
      "loss": 0.7889,
      "step": 1840
    },
    {
      "epoch": 0.7919520547945206,
      "grad_norm": 20.154905319213867,
      "learning_rate": 2.5250856164383564e-05,
      "loss": 0.8551,
      "step": 1850
    },
    {
      "epoch": 0.7962328767123288,
      "grad_norm": 16.845243453979492,
      "learning_rate": 2.522517123287671e-05,
      "loss": 0.7068,
      "step": 1860
    },
    {
      "epoch": 0.800513698630137,
      "grad_norm": 34.205780029296875,
      "learning_rate": 2.5199486301369864e-05,
      "loss": 0.805,
      "step": 1870
    },
    {
      "epoch": 0.8047945205479452,
      "grad_norm": 13.016587257385254,
      "learning_rate": 2.5173801369863014e-05,
      "loss": 0.6715,
      "step": 1880
    },
    {
      "epoch": 0.8090753424657534,
      "grad_norm": 17.59514617919922,
      "learning_rate": 2.5148116438356167e-05,
      "loss": 0.7308,
      "step": 1890
    },
    {
      "epoch": 0.8133561643835616,
      "grad_norm": 21.68423843383789,
      "learning_rate": 2.5122431506849317e-05,
      "loss": 0.9619,
      "step": 1900
    },
    {
      "epoch": 0.8176369863013698,
      "grad_norm": 10.85733413696289,
      "learning_rate": 2.5096746575342466e-05,
      "loss": 0.4836,
      "step": 1910
    },
    {
      "epoch": 0.821917808219178,
      "grad_norm": 17.5113468170166,
      "learning_rate": 2.5071061643835616e-05,
      "loss": 0.7225,
      "step": 1920
    },
    {
      "epoch": 0.8261986301369864,
      "grad_norm": 20.78669548034668,
      "learning_rate": 2.504537671232877e-05,
      "loss": 0.7372,
      "step": 1930
    },
    {
      "epoch": 0.8304794520547946,
      "grad_norm": 12.682047843933105,
      "learning_rate": 2.501969178082192e-05,
      "loss": 0.781,
      "step": 1940
    },
    {
      "epoch": 0.8347602739726028,
      "grad_norm": 18.531436920166016,
      "learning_rate": 2.499400684931507e-05,
      "loss": 0.8725,
      "step": 1950
    },
    {
      "epoch": 0.839041095890411,
      "grad_norm": 33.07869338989258,
      "learning_rate": 2.496832191780822e-05,
      "loss": 0.7145,
      "step": 1960
    },
    {
      "epoch": 0.8433219178082192,
      "grad_norm": 56.62294006347656,
      "learning_rate": 2.4942636986301372e-05,
      "loss": 0.5252,
      "step": 1970
    },
    {
      "epoch": 0.8476027397260274,
      "grad_norm": 64.34467315673828,
      "learning_rate": 2.491695205479452e-05,
      "loss": 0.8005,
      "step": 1980
    },
    {
      "epoch": 0.8518835616438356,
      "grad_norm": 12.431035041809082,
      "learning_rate": 2.4891267123287675e-05,
      "loss": 0.5862,
      "step": 1990
    },
    {
      "epoch": 0.8561643835616438,
      "grad_norm": 31.489168167114258,
      "learning_rate": 2.486558219178082e-05,
      "loss": 0.8181,
      "step": 2000
    },
    {
      "epoch": 0.860445205479452,
      "grad_norm": 20.041730880737305,
      "learning_rate": 2.4839897260273974e-05,
      "loss": 0.6091,
      "step": 2010
    },
    {
      "epoch": 0.8647260273972602,
      "grad_norm": 8.799732208251953,
      "learning_rate": 2.4814212328767124e-05,
      "loss": 0.5877,
      "step": 2020
    },
    {
      "epoch": 0.8690068493150684,
      "grad_norm": 13.108522415161133,
      "learning_rate": 2.4788527397260274e-05,
      "loss": 0.6862,
      "step": 2030
    },
    {
      "epoch": 0.8732876712328768,
      "grad_norm": 21.110349655151367,
      "learning_rate": 2.4762842465753427e-05,
      "loss": 0.6593,
      "step": 2040
    },
    {
      "epoch": 0.877568493150685,
      "grad_norm": 4.349354267120361,
      "learning_rate": 2.4737157534246573e-05,
      "loss": 0.68,
      "step": 2050
    },
    {
      "epoch": 0.8818493150684932,
      "grad_norm": 17.407764434814453,
      "learning_rate": 2.4711472602739727e-05,
      "loss": 0.7587,
      "step": 2060
    },
    {
      "epoch": 0.8861301369863014,
      "grad_norm": 10.936676025390625,
      "learning_rate": 2.4685787671232876e-05,
      "loss": 0.7355,
      "step": 2070
    },
    {
      "epoch": 0.8904109589041096,
      "grad_norm": 36.851112365722656,
      "learning_rate": 2.466010273972603e-05,
      "loss": 0.8058,
      "step": 2080
    },
    {
      "epoch": 0.8946917808219178,
      "grad_norm": 26.359825134277344,
      "learning_rate": 2.4634417808219176e-05,
      "loss": 0.8067,
      "step": 2090
    },
    {
      "epoch": 0.898972602739726,
      "grad_norm": 4.808035373687744,
      "learning_rate": 2.460873287671233e-05,
      "loss": 0.7556,
      "step": 2100
    },
    {
      "epoch": 0.9032534246575342,
      "grad_norm": 29.35092544555664,
      "learning_rate": 2.458304794520548e-05,
      "loss": 0.7164,
      "step": 2110
    },
    {
      "epoch": 0.9075342465753424,
      "grad_norm": 19.436264038085938,
      "learning_rate": 2.4557363013698632e-05,
      "loss": 0.6005,
      "step": 2120
    },
    {
      "epoch": 0.9118150684931506,
      "grad_norm": 12.285465240478516,
      "learning_rate": 2.4531678082191782e-05,
      "loss": 0.5932,
      "step": 2130
    },
    {
      "epoch": 0.916095890410959,
      "grad_norm": 25.76729965209961,
      "learning_rate": 2.450599315068493e-05,
      "loss": 0.6441,
      "step": 2140
    },
    {
      "epoch": 0.9203767123287672,
      "grad_norm": 27.039709091186523,
      "learning_rate": 2.448030821917808e-05,
      "loss": 0.8027,
      "step": 2150
    },
    {
      "epoch": 0.9246575342465754,
      "grad_norm": 28.40961265563965,
      "learning_rate": 2.4454623287671235e-05,
      "loss": 0.5723,
      "step": 2160
    },
    {
      "epoch": 0.9289383561643836,
      "grad_norm": 28.89515495300293,
      "learning_rate": 2.4428938356164384e-05,
      "loss": 0.726,
      "step": 2170
    },
    {
      "epoch": 0.9332191780821918,
      "grad_norm": 15.21774959564209,
      "learning_rate": 2.4403253424657534e-05,
      "loss": 0.5119,
      "step": 2180
    },
    {
      "epoch": 0.9375,
      "grad_norm": 31.885351181030273,
      "learning_rate": 2.4377568493150684e-05,
      "loss": 0.6956,
      "step": 2190
    },
    {
      "epoch": 0.9417808219178082,
      "grad_norm": 41.878997802734375,
      "learning_rate": 2.4351883561643837e-05,
      "loss": 0.4734,
      "step": 2200
    },
    {
      "epoch": 0.9460616438356164,
      "grad_norm": 12.264182090759277,
      "learning_rate": 2.4326198630136987e-05,
      "loss": 0.526,
      "step": 2210
    },
    {
      "epoch": 0.9503424657534246,
      "grad_norm": 6.523041248321533,
      "learning_rate": 2.430051369863014e-05,
      "loss": 0.7543,
      "step": 2220
    },
    {
      "epoch": 0.9546232876712328,
      "grad_norm": 19.857051849365234,
      "learning_rate": 2.4274828767123286e-05,
      "loss": 0.4575,
      "step": 2230
    },
    {
      "epoch": 0.958904109589041,
      "grad_norm": 41.966434478759766,
      "learning_rate": 2.424914383561644e-05,
      "loss": 0.71,
      "step": 2240
    },
    {
      "epoch": 0.9631849315068494,
      "grad_norm": 18.047285079956055,
      "learning_rate": 2.422345890410959e-05,
      "loss": 0.4611,
      "step": 2250
    },
    {
      "epoch": 0.9674657534246576,
      "grad_norm": 32.25387191772461,
      "learning_rate": 2.4197773972602743e-05,
      "loss": 0.6311,
      "step": 2260
    },
    {
      "epoch": 0.9717465753424658,
      "grad_norm": 7.983370780944824,
      "learning_rate": 2.417208904109589e-05,
      "loss": 0.6455,
      "step": 2270
    },
    {
      "epoch": 0.976027397260274,
      "grad_norm": 15.092449188232422,
      "learning_rate": 2.4146404109589042e-05,
      "loss": 0.5441,
      "step": 2280
    },
    {
      "epoch": 0.9803082191780822,
      "grad_norm": 4.433277130126953,
      "learning_rate": 2.4120719178082192e-05,
      "loss": 0.8646,
      "step": 2290
    },
    {
      "epoch": 0.9845890410958904,
      "grad_norm": 2.8443455696105957,
      "learning_rate": 2.4095034246575345e-05,
      "loss": 0.653,
      "step": 2300
    },
    {
      "epoch": 0.9888698630136986,
      "grad_norm": 0.9373922944068909,
      "learning_rate": 2.4069349315068495e-05,
      "loss": 0.5245,
      "step": 2310
    },
    {
      "epoch": 0.9931506849315068,
      "grad_norm": 54.9177131652832,
      "learning_rate": 2.4043664383561645e-05,
      "loss": 0.5716,
      "step": 2320
    },
    {
      "epoch": 0.997431506849315,
      "grad_norm": 17.051746368408203,
      "learning_rate": 2.4017979452054794e-05,
      "loss": 0.5881,
      "step": 2330
    },
    {
      "epoch": 1.0,
      "eval_f1": 0.640155605090126,
      "eval_loss": 0.7885168194770813,
      "eval_precision": 0.65989952624744,
      "eval_recall": 0.6545266692496449,
      "eval_runtime": 596.1184,
      "eval_samples_per_second": 7.499,
      "eval_steps_per_second": 0.938,
      "step": 2336
    },
    {
      "epoch": 1.0017123287671232,
      "grad_norm": 14.434311866760254,
      "learning_rate": 2.3992294520547948e-05,
      "loss": 0.5605,
      "step": 2340
    },
    {
      "epoch": 1.0059931506849316,
      "grad_norm": 1.0917059183120728,
      "learning_rate": 2.3966609589041097e-05,
      "loss": 0.7174,
      "step": 2350
    },
    {
      "epoch": 1.0102739726027397,
      "grad_norm": 10.919930458068848,
      "learning_rate": 2.3940924657534247e-05,
      "loss": 0.5105,
      "step": 2360
    },
    {
      "epoch": 1.014554794520548,
      "grad_norm": 28.403804779052734,
      "learning_rate": 2.3915239726027397e-05,
      "loss": 0.6549,
      "step": 2370
    },
    {
      "epoch": 1.018835616438356,
      "grad_norm": 26.01487922668457,
      "learning_rate": 2.3889554794520547e-05,
      "loss": 0.5837,
      "step": 2380
    },
    {
      "epoch": 1.0231164383561644,
      "grad_norm": 8.452924728393555,
      "learning_rate": 2.38638698630137e-05,
      "loss": 0.4745,
      "step": 2390
    },
    {
      "epoch": 1.0273972602739727,
      "grad_norm": 20.746458053588867,
      "learning_rate": 2.383818493150685e-05,
      "loss": 0.4122,
      "step": 2400
    },
    {
      "epoch": 1.0316780821917808,
      "grad_norm": 25.048707962036133,
      "learning_rate": 2.38125e-05,
      "loss": 0.8255,
      "step": 2410
    },
    {
      "epoch": 1.0359589041095891,
      "grad_norm": 6.413450717926025,
      "learning_rate": 2.378681506849315e-05,
      "loss": 0.5403,
      "step": 2420
    },
    {
      "epoch": 1.0402397260273972,
      "grad_norm": 11.969742774963379,
      "learning_rate": 2.3761130136986302e-05,
      "loss": 0.6837,
      "step": 2430
    },
    {
      "epoch": 1.0445205479452055,
      "grad_norm": 12.856775283813477,
      "learning_rate": 2.3735445205479452e-05,
      "loss": 0.508,
      "step": 2440
    },
    {
      "epoch": 1.0488013698630136,
      "grad_norm": 98.75533294677734,
      "learning_rate": 2.3709760273972605e-05,
      "loss": 0.5482,
      "step": 2450
    },
    {
      "epoch": 1.053082191780822,
      "grad_norm": 14.54837417602539,
      "learning_rate": 2.3684075342465752e-05,
      "loss": 0.543,
      "step": 2460
    },
    {
      "epoch": 1.05736301369863,
      "grad_norm": 13.33887004852295,
      "learning_rate": 2.3658390410958905e-05,
      "loss": 0.5926,
      "step": 2470
    },
    {
      "epoch": 1.0616438356164384,
      "grad_norm": 8.95486831665039,
      "learning_rate": 2.3632705479452055e-05,
      "loss": 0.5339,
      "step": 2480
    },
    {
      "epoch": 1.0659246575342465,
      "grad_norm": 14.323339462280273,
      "learning_rate": 2.3607020547945208e-05,
      "loss": 0.4593,
      "step": 2490
    },
    {
      "epoch": 1.0702054794520548,
      "grad_norm": 15.935379028320312,
      "learning_rate": 2.3581335616438354e-05,
      "loss": 0.5269,
      "step": 2500
    },
    {
      "epoch": 1.0744863013698631,
      "grad_norm": 10.73087215423584,
      "learning_rate": 2.3555650684931507e-05,
      "loss": 0.6115,
      "step": 2510
    },
    {
      "epoch": 1.0787671232876712,
      "grad_norm": 5.429958343505859,
      "learning_rate": 2.3529965753424657e-05,
      "loss": 0.3729,
      "step": 2520
    },
    {
      "epoch": 1.0830479452054795,
      "grad_norm": 21.095849990844727,
      "learning_rate": 2.350428082191781e-05,
      "loss": 0.4417,
      "step": 2530
    },
    {
      "epoch": 1.0873287671232876,
      "grad_norm": 11.610197067260742,
      "learning_rate": 2.347859589041096e-05,
      "loss": 0.7186,
      "step": 2540
    },
    {
      "epoch": 1.091609589041096,
      "grad_norm": 7.565066337585449,
      "learning_rate": 2.345291095890411e-05,
      "loss": 0.5585,
      "step": 2550
    },
    {
      "epoch": 1.095890410958904,
      "grad_norm": 8.311152458190918,
      "learning_rate": 2.342722602739726e-05,
      "loss": 0.6581,
      "step": 2560
    },
    {
      "epoch": 1.1001712328767124,
      "grad_norm": 9.93104076385498,
      "learning_rate": 2.3401541095890413e-05,
      "loss": 0.4014,
      "step": 2570
    },
    {
      "epoch": 1.1044520547945205,
      "grad_norm": 12.65024185180664,
      "learning_rate": 2.3375856164383563e-05,
      "loss": 0.5235,
      "step": 2580
    },
    {
      "epoch": 1.1087328767123288,
      "grad_norm": 60.201515197753906,
      "learning_rate": 2.3350171232876712e-05,
      "loss": 0.91,
      "step": 2590
    },
    {
      "epoch": 1.1130136986301369,
      "grad_norm": 9.550028800964355,
      "learning_rate": 2.3324486301369862e-05,
      "loss": 0.4735,
      "step": 2600
    },
    {
      "epoch": 1.1172945205479452,
      "grad_norm": 9.562759399414062,
      "learning_rate": 2.3298801369863015e-05,
      "loss": 0.4877,
      "step": 2610
    },
    {
      "epoch": 1.1215753424657535,
      "grad_norm": 12.886402130126953,
      "learning_rate": 2.3273116438356165e-05,
      "loss": 0.4259,
      "step": 2620
    },
    {
      "epoch": 1.1258561643835616,
      "grad_norm": 7.7619709968566895,
      "learning_rate": 2.324743150684932e-05,
      "loss": 0.502,
      "step": 2630
    },
    {
      "epoch": 1.13013698630137,
      "grad_norm": 22.827817916870117,
      "learning_rate": 2.3221746575342465e-05,
      "loss": 0.5342,
      "step": 2640
    },
    {
      "epoch": 1.134417808219178,
      "grad_norm": 26.439483642578125,
      "learning_rate": 2.3196061643835618e-05,
      "loss": 0.4635,
      "step": 2650
    },
    {
      "epoch": 1.1386986301369864,
      "grad_norm": 27.487112045288086,
      "learning_rate": 2.3170376712328768e-05,
      "loss": 0.4494,
      "step": 2660
    },
    {
      "epoch": 1.1429794520547945,
      "grad_norm": 8.665365219116211,
      "learning_rate": 2.314469178082192e-05,
      "loss": 0.5481,
      "step": 2670
    },
    {
      "epoch": 1.1472602739726028,
      "grad_norm": 35.2126579284668,
      "learning_rate": 2.3119006849315067e-05,
      "loss": 0.7088,
      "step": 2680
    },
    {
      "epoch": 1.1515410958904109,
      "grad_norm": 19.38114356994629,
      "learning_rate": 2.309332191780822e-05,
      "loss": 0.5704,
      "step": 2690
    },
    {
      "epoch": 1.1558219178082192,
      "grad_norm": 0.7317081093788147,
      "learning_rate": 2.306763698630137e-05,
      "loss": 0.4253,
      "step": 2700
    },
    {
      "epoch": 1.1601027397260273,
      "grad_norm": 8.237403869628906,
      "learning_rate": 2.3041952054794523e-05,
      "loss": 0.4897,
      "step": 2710
    },
    {
      "epoch": 1.1643835616438356,
      "grad_norm": 14.927027702331543,
      "learning_rate": 2.3016267123287673e-05,
      "loss": 0.6706,
      "step": 2720
    },
    {
      "epoch": 1.168664383561644,
      "grad_norm": 28.70943832397461,
      "learning_rate": 2.299058219178082e-05,
      "loss": 0.5062,
      "step": 2730
    },
    {
      "epoch": 1.172945205479452,
      "grad_norm": 20.617576599121094,
      "learning_rate": 2.2964897260273973e-05,
      "loss": 0.5652,
      "step": 2740
    },
    {
      "epoch": 1.1772260273972603,
      "grad_norm": 37.53913879394531,
      "learning_rate": 2.2939212328767123e-05,
      "loss": 0.682,
      "step": 2750
    },
    {
      "epoch": 1.1815068493150684,
      "grad_norm": 125.64456176757812,
      "learning_rate": 2.2913527397260276e-05,
      "loss": 0.6095,
      "step": 2760
    },
    {
      "epoch": 1.1857876712328768,
      "grad_norm": 0.6167072057723999,
      "learning_rate": 2.2887842465753425e-05,
      "loss": 0.3489,
      "step": 2770
    },
    {
      "epoch": 1.1900684931506849,
      "grad_norm": 8.875263214111328,
      "learning_rate": 2.2862157534246575e-05,
      "loss": 0.5724,
      "step": 2780
    },
    {
      "epoch": 1.1943493150684932,
      "grad_norm": 29.087247848510742,
      "learning_rate": 2.2836472602739725e-05,
      "loss": 0.5674,
      "step": 2790
    },
    {
      "epoch": 1.1986301369863013,
      "grad_norm": 108.30213165283203,
      "learning_rate": 2.2810787671232878e-05,
      "loss": 0.4056,
      "step": 2800
    },
    {
      "epoch": 1.2029109589041096,
      "grad_norm": 13.343461990356445,
      "learning_rate": 2.2785102739726028e-05,
      "loss": 0.4987,
      "step": 2810
    },
    {
      "epoch": 1.2071917808219177,
      "grad_norm": 0.6047355532646179,
      "learning_rate": 2.2759417808219178e-05,
      "loss": 0.2861,
      "step": 2820
    },
    {
      "epoch": 1.211472602739726,
      "grad_norm": 12.467401504516602,
      "learning_rate": 2.2733732876712328e-05,
      "loss": 0.7408,
      "step": 2830
    },
    {
      "epoch": 1.2157534246575343,
      "grad_norm": 15.868618965148926,
      "learning_rate": 2.270804794520548e-05,
      "loss": 0.4814,
      "step": 2840
    },
    {
      "epoch": 1.2200342465753424,
      "grad_norm": 24.539642333984375,
      "learning_rate": 2.268236301369863e-05,
      "loss": 0.6094,
      "step": 2850
    },
    {
      "epoch": 1.2243150684931507,
      "grad_norm": 15.609148979187012,
      "learning_rate": 2.2656678082191784e-05,
      "loss": 0.6397,
      "step": 2860
    },
    {
      "epoch": 1.2285958904109588,
      "grad_norm": 15.778641700744629,
      "learning_rate": 2.263099315068493e-05,
      "loss": 0.5055,
      "step": 2870
    },
    {
      "epoch": 1.2328767123287672,
      "grad_norm": 20.7979736328125,
      "learning_rate": 2.2605308219178083e-05,
      "loss": 0.3678,
      "step": 2880
    },
    {
      "epoch": 1.2371575342465753,
      "grad_norm": 39.31053924560547,
      "learning_rate": 2.2579623287671233e-05,
      "loss": 0.3605,
      "step": 2890
    },
    {
      "epoch": 1.2414383561643836,
      "grad_norm": 1.9259212017059326,
      "learning_rate": 2.2553938356164386e-05,
      "loss": 0.4457,
      "step": 2900
    },
    {
      "epoch": 1.245719178082192,
      "grad_norm": 39.82622146606445,
      "learning_rate": 2.2528253424657533e-05,
      "loss": 0.5663,
      "step": 2910
    },
    {
      "epoch": 1.25,
      "grad_norm": 18.27713394165039,
      "learning_rate": 2.2502568493150686e-05,
      "loss": 0.559,
      "step": 2920
    },
    {
      "epoch": 1.254280821917808,
      "grad_norm": 19.46271514892578,
      "learning_rate": 2.2476883561643836e-05,
      "loss": 0.5596,
      "step": 2930
    },
    {
      "epoch": 1.2585616438356164,
      "grad_norm": 15.22482681274414,
      "learning_rate": 2.245119863013699e-05,
      "loss": 0.5677,
      "step": 2940
    },
    {
      "epoch": 1.2628424657534247,
      "grad_norm": 6.7652812004089355,
      "learning_rate": 2.242551369863014e-05,
      "loss": 0.6691,
      "step": 2950
    },
    {
      "epoch": 1.2671232876712328,
      "grad_norm": 15.198921203613281,
      "learning_rate": 2.2399828767123288e-05,
      "loss": 0.4918,
      "step": 2960
    },
    {
      "epoch": 1.2714041095890412,
      "grad_norm": 103.82099151611328,
      "learning_rate": 2.2374143835616438e-05,
      "loss": 0.5306,
      "step": 2970
    },
    {
      "epoch": 1.2756849315068493,
      "grad_norm": 4.987117290496826,
      "learning_rate": 2.234845890410959e-05,
      "loss": 0.5766,
      "step": 2980
    },
    {
      "epoch": 1.2799657534246576,
      "grad_norm": 7.564026355743408,
      "learning_rate": 2.232277397260274e-05,
      "loss": 0.5242,
      "step": 2990
    },
    {
      "epoch": 1.2842465753424657,
      "grad_norm": 3.2099010944366455,
      "learning_rate": 2.229708904109589e-05,
      "loss": 0.6231,
      "step": 3000
    },
    {
      "epoch": 1.288527397260274,
      "grad_norm": 31.249996185302734,
      "learning_rate": 2.227140410958904e-05,
      "loss": 0.582,
      "step": 3010
    },
    {
      "epoch": 1.2928082191780823,
      "grad_norm": 24.8253116607666,
      "learning_rate": 2.2245719178082194e-05,
      "loss": 0.7384,
      "step": 3020
    },
    {
      "epoch": 1.2970890410958904,
      "grad_norm": 9.528766632080078,
      "learning_rate": 2.2220034246575344e-05,
      "loss": 0.3158,
      "step": 3030
    },
    {
      "epoch": 1.3013698630136985,
      "grad_norm": 20.124649047851562,
      "learning_rate": 2.2194349315068497e-05,
      "loss": 0.725,
      "step": 3040
    },
    {
      "epoch": 1.3056506849315068,
      "grad_norm": 6.461442470550537,
      "learning_rate": 2.2168664383561643e-05,
      "loss": 0.523,
      "step": 3050
    },
    {
      "epoch": 1.3099315068493151,
      "grad_norm": 18.374300003051758,
      "learning_rate": 2.2142979452054796e-05,
      "loss": 0.4177,
      "step": 3060
    },
    {
      "epoch": 1.3142123287671232,
      "grad_norm": 15.605560302734375,
      "learning_rate": 2.2117294520547946e-05,
      "loss": 0.6334,
      "step": 3070
    },
    {
      "epoch": 1.3184931506849316,
      "grad_norm": 40.33599090576172,
      "learning_rate": 2.2091609589041096e-05,
      "loss": 0.5582,
      "step": 3080
    },
    {
      "epoch": 1.3227739726027397,
      "grad_norm": 18.125089645385742,
      "learning_rate": 2.2065924657534246e-05,
      "loss": 0.5148,
      "step": 3090
    },
    {
      "epoch": 1.327054794520548,
      "grad_norm": 33.55887222290039,
      "learning_rate": 2.2040239726027395e-05,
      "loss": 0.4119,
      "step": 3100
    },
    {
      "epoch": 1.331335616438356,
      "grad_norm": 27.477209091186523,
      "learning_rate": 2.201455479452055e-05,
      "loss": 0.7335,
      "step": 3110
    },
    {
      "epoch": 1.3356164383561644,
      "grad_norm": 28.373653411865234,
      "learning_rate": 2.19888698630137e-05,
      "loss": 0.4652,
      "step": 3120
    },
    {
      "epoch": 1.3398972602739727,
      "grad_norm": 10.41767692565918,
      "learning_rate": 2.196318493150685e-05,
      "loss": 0.4556,
      "step": 3130
    },
    {
      "epoch": 1.3441780821917808,
      "grad_norm": 15.985031127929688,
      "learning_rate": 2.1937499999999998e-05,
      "loss": 0.5208,
      "step": 3140
    },
    {
      "epoch": 1.348458904109589,
      "grad_norm": 9.439993858337402,
      "learning_rate": 2.191181506849315e-05,
      "loss": 0.4854,
      "step": 3150
    },
    {
      "epoch": 1.3527397260273972,
      "grad_norm": 16.428089141845703,
      "learning_rate": 2.18861301369863e-05,
      "loss": 0.5496,
      "step": 3160
    },
    {
      "epoch": 1.3570205479452055,
      "grad_norm": 13.383630752563477,
      "learning_rate": 2.1860445205479454e-05,
      "loss": 0.4427,
      "step": 3170
    },
    {
      "epoch": 1.3613013698630136,
      "grad_norm": 21.770055770874023,
      "learning_rate": 2.1834760273972604e-05,
      "loss": 0.3362,
      "step": 3180
    },
    {
      "epoch": 1.365582191780822,
      "grad_norm": 2.661846876144409,
      "learning_rate": 2.1809075342465754e-05,
      "loss": 0.4305,
      "step": 3190
    },
    {
      "epoch": 1.36986301369863,
      "grad_norm": 9.57947826385498,
      "learning_rate": 2.1783390410958903e-05,
      "loss": 0.6307,
      "step": 3200
    },
    {
      "epoch": 1.3741438356164384,
      "grad_norm": 7.182124137878418,
      "learning_rate": 2.1757705479452057e-05,
      "loss": 0.7003,
      "step": 3210
    },
    {
      "epoch": 1.3784246575342465,
      "grad_norm": 12.542230606079102,
      "learning_rate": 2.1732020547945206e-05,
      "loss": 0.7924,
      "step": 3220
    },
    {
      "epoch": 1.3827054794520548,
      "grad_norm": 11.852202415466309,
      "learning_rate": 2.1706335616438356e-05,
      "loss": 0.5024,
      "step": 3230
    },
    {
      "epoch": 1.3869863013698631,
      "grad_norm": 22.10004997253418,
      "learning_rate": 2.1680650684931506e-05,
      "loss": 0.5869,
      "step": 3240
    },
    {
      "epoch": 1.3912671232876712,
      "grad_norm": 9.61673641204834,
      "learning_rate": 2.165496575342466e-05,
      "loss": 0.5511,
      "step": 3250
    },
    {
      "epoch": 1.3955479452054795,
      "grad_norm": 3.5772383213043213,
      "learning_rate": 2.162928082191781e-05,
      "loss": 0.5378,
      "step": 3260
    },
    {
      "epoch": 1.3998287671232876,
      "grad_norm": 14.454607009887695,
      "learning_rate": 2.1603595890410962e-05,
      "loss": 0.7021,
      "step": 3270
    },
    {
      "epoch": 1.404109589041096,
      "grad_norm": 12.301724433898926,
      "learning_rate": 2.157791095890411e-05,
      "loss": 0.7471,
      "step": 3280
    },
    {
      "epoch": 1.408390410958904,
      "grad_norm": 4.101442813873291,
      "learning_rate": 2.155222602739726e-05,
      "loss": 0.355,
      "step": 3290
    },
    {
      "epoch": 1.4126712328767124,
      "grad_norm": 39.615867614746094,
      "learning_rate": 2.152654109589041e-05,
      "loss": 0.6048,
      "step": 3300
    },
    {
      "epoch": 1.4169520547945205,
      "grad_norm": 24.081043243408203,
      "learning_rate": 2.1500856164383565e-05,
      "loss": 0.7089,
      "step": 3310
    },
    {
      "epoch": 1.4212328767123288,
      "grad_norm": 6.137239933013916,
      "learning_rate": 2.147517123287671e-05,
      "loss": 0.4565,
      "step": 3320
    },
    {
      "epoch": 1.4255136986301369,
      "grad_norm": 11.220376014709473,
      "learning_rate": 2.1449486301369864e-05,
      "loss": 0.4277,
      "step": 3330
    },
    {
      "epoch": 1.4297945205479452,
      "grad_norm": 49.353233337402344,
      "learning_rate": 2.1423801369863014e-05,
      "loss": 0.3421,
      "step": 3340
    },
    {
      "epoch": 1.4340753424657535,
      "grad_norm": 21.286075592041016,
      "learning_rate": 2.1398116438356167e-05,
      "loss": 0.4536,
      "step": 3350
    },
    {
      "epoch": 1.4383561643835616,
      "grad_norm": 10.80482006072998,
      "learning_rate": 2.1372431506849317e-05,
      "loss": 0.6355,
      "step": 3360
    },
    {
      "epoch": 1.44263698630137,
      "grad_norm": 22.60628318786621,
      "learning_rate": 2.1346746575342467e-05,
      "loss": 0.5357,
      "step": 3370
    },
    {
      "epoch": 1.446917808219178,
      "grad_norm": 7.038317680358887,
      "learning_rate": 2.1321061643835616e-05,
      "loss": 0.5848,
      "step": 3380
    },
    {
      "epoch": 1.4511986301369864,
      "grad_norm": 3.6277172565460205,
      "learning_rate": 2.129537671232877e-05,
      "loss": 0.3811,
      "step": 3390
    },
    {
      "epoch": 1.4554794520547945,
      "grad_norm": 141.23995971679688,
      "learning_rate": 2.126969178082192e-05,
      "loss": 0.3749,
      "step": 3400
    },
    {
      "epoch": 1.4597602739726028,
      "grad_norm": 9.505330085754395,
      "learning_rate": 2.1244006849315066e-05,
      "loss": 0.5169,
      "step": 3410
    },
    {
      "epoch": 1.464041095890411,
      "grad_norm": 31.260398864746094,
      "learning_rate": 2.121832191780822e-05,
      "loss": 0.6798,
      "step": 3420
    },
    {
      "epoch": 1.4683219178082192,
      "grad_norm": 14.576324462890625,
      "learning_rate": 2.119263698630137e-05,
      "loss": 0.3887,
      "step": 3430
    },
    {
      "epoch": 1.4726027397260273,
      "grad_norm": 17.221073150634766,
      "learning_rate": 2.1166952054794522e-05,
      "loss": 0.3907,
      "step": 3440
    },
    {
      "epoch": 1.4768835616438356,
      "grad_norm": 0.8417624235153198,
      "learning_rate": 2.114126712328767e-05,
      "loss": 0.3861,
      "step": 3450
    },
    {
      "epoch": 1.481164383561644,
      "grad_norm": 1.0525720119476318,
      "learning_rate": 2.111558219178082e-05,
      "loss": 0.5549,
      "step": 3460
    },
    {
      "epoch": 1.485445205479452,
      "grad_norm": 8.386690139770508,
      "learning_rate": 2.108989726027397e-05,
      "loss": 0.4114,
      "step": 3470
    },
    {
      "epoch": 1.4897260273972603,
      "grad_norm": 4.5233283042907715,
      "learning_rate": 2.1064212328767124e-05,
      "loss": 0.4464,
      "step": 3480
    },
    {
      "epoch": 1.4940068493150684,
      "grad_norm": 59.0789794921875,
      "learning_rate": 2.1038527397260274e-05,
      "loss": 0.682,
      "step": 3490
    },
    {
      "epoch": 1.4982876712328768,
      "grad_norm": 23.49764633178711,
      "learning_rate": 2.1012842465753427e-05,
      "loss": 0.4015,
      "step": 3500
    },
    {
      "epoch": 1.5025684931506849,
      "grad_norm": 26.084774017333984,
      "learning_rate": 2.0987157534246574e-05,
      "loss": 0.7139,
      "step": 3510
    },
    {
      "epoch": 1.5068493150684932,
      "grad_norm": 7.794407367706299,
      "learning_rate": 2.0961472602739727e-05,
      "loss": 0.4606,
      "step": 3520
    },
    {
      "epoch": 1.5111301369863015,
      "grad_norm": 11.288233757019043,
      "learning_rate": 2.0935787671232877e-05,
      "loss": 0.451,
      "step": 3530
    },
    {
      "epoch": 1.5154109589041096,
      "grad_norm": 2.306042194366455,
      "learning_rate": 2.091010273972603e-05,
      "loss": 0.3553,
      "step": 3540
    },
    {
      "epoch": 1.5196917808219177,
      "grad_norm": 11.157553672790527,
      "learning_rate": 2.0884417808219176e-05,
      "loss": 0.6895,
      "step": 3550
    },
    {
      "epoch": 1.523972602739726,
      "grad_norm": 11.598366737365723,
      "learning_rate": 2.085873287671233e-05,
      "loss": 0.5611,
      "step": 3560
    },
    {
      "epoch": 1.5282534246575343,
      "grad_norm": 16.561546325683594,
      "learning_rate": 2.083304794520548e-05,
      "loss": 0.5075,
      "step": 3570
    },
    {
      "epoch": 1.5325342465753424,
      "grad_norm": 0.8690369129180908,
      "learning_rate": 2.0807363013698632e-05,
      "loss": 0.6442,
      "step": 3580
    },
    {
      "epoch": 1.5368150684931505,
      "grad_norm": 19.73202896118164,
      "learning_rate": 2.0781678082191782e-05,
      "loss": 0.5141,
      "step": 3590
    },
    {
      "epoch": 1.541095890410959,
      "grad_norm": 14.27385425567627,
      "learning_rate": 2.0755993150684932e-05,
      "loss": 0.4149,
      "step": 3600
    },
    {
      "epoch": 1.5453767123287672,
      "grad_norm": 2.2302627563476562,
      "learning_rate": 2.0730308219178082e-05,
      "loss": 0.758,
      "step": 3610
    },
    {
      "epoch": 1.5496575342465753,
      "grad_norm": 4.972026824951172,
      "learning_rate": 2.0704623287671235e-05,
      "loss": 0.4684,
      "step": 3620
    },
    {
      "epoch": 1.5539383561643836,
      "grad_norm": 4.984468460083008,
      "learning_rate": 2.0678938356164385e-05,
      "loss": 0.4058,
      "step": 3630
    },
    {
      "epoch": 1.558219178082192,
      "grad_norm": 58.44465255737305,
      "learning_rate": 2.0653253424657534e-05,
      "loss": 0.6543,
      "step": 3640
    },
    {
      "epoch": 1.5625,
      "grad_norm": 10.453229904174805,
      "learning_rate": 2.0627568493150684e-05,
      "loss": 0.6567,
      "step": 3650
    },
    {
      "epoch": 1.566780821917808,
      "grad_norm": 32.872833251953125,
      "learning_rate": 2.0601883561643837e-05,
      "loss": 0.5757,
      "step": 3660
    },
    {
      "epoch": 1.5710616438356164,
      "grad_norm": 26.98045539855957,
      "learning_rate": 2.0576198630136987e-05,
      "loss": 0.6303,
      "step": 3670
    },
    {
      "epoch": 1.5753424657534247,
      "grad_norm": 19.083444595336914,
      "learning_rate": 2.055051369863014e-05,
      "loss": 0.4122,
      "step": 3680
    },
    {
      "epoch": 1.5796232876712328,
      "grad_norm": 23.704484939575195,
      "learning_rate": 2.0524828767123287e-05,
      "loss": 0.8762,
      "step": 3690
    },
    {
      "epoch": 1.583904109589041,
      "grad_norm": 10.016107559204102,
      "learning_rate": 2.049914383561644e-05,
      "loss": 0.6037,
      "step": 3700
    },
    {
      "epoch": 1.5881849315068495,
      "grad_norm": 11.916234016418457,
      "learning_rate": 2.047345890410959e-05,
      "loss": 0.4509,
      "step": 3710
    },
    {
      "epoch": 1.5924657534246576,
      "grad_norm": 12.279425621032715,
      "learning_rate": 2.0447773972602743e-05,
      "loss": 0.477,
      "step": 3720
    },
    {
      "epoch": 1.5967465753424657,
      "grad_norm": 41.168392181396484,
      "learning_rate": 2.042208904109589e-05,
      "loss": 0.3373,
      "step": 3730
    },
    {
      "epoch": 1.601027397260274,
      "grad_norm": 13.21113395690918,
      "learning_rate": 2.0396404109589042e-05,
      "loss": 0.6568,
      "step": 3740
    },
    {
      "epoch": 1.6053082191780823,
      "grad_norm": 55.25428009033203,
      "learning_rate": 2.0370719178082192e-05,
      "loss": 0.5181,
      "step": 3750
    },
    {
      "epoch": 1.6095890410958904,
      "grad_norm": 4.288461208343506,
      "learning_rate": 2.0345034246575342e-05,
      "loss": 0.4256,
      "step": 3760
    },
    {
      "epoch": 1.6138698630136985,
      "grad_norm": 9.131546020507812,
      "learning_rate": 2.0319349315068495e-05,
      "loss": 0.4068,
      "step": 3770
    },
    {
      "epoch": 1.6181506849315068,
      "grad_norm": 14.117582321166992,
      "learning_rate": 2.029366438356164e-05,
      "loss": 0.5419,
      "step": 3780
    },
    {
      "epoch": 1.6224315068493151,
      "grad_norm": 12.835063934326172,
      "learning_rate": 2.0267979452054795e-05,
      "loss": 0.3837,
      "step": 3790
    },
    {
      "epoch": 1.6267123287671232,
      "grad_norm": 3.802135944366455,
      "learning_rate": 2.0242294520547945e-05,
      "loss": 0.453,
      "step": 3800
    },
    {
      "epoch": 1.6309931506849316,
      "grad_norm": 21.60682487487793,
      "learning_rate": 2.0216609589041098e-05,
      "loss": 0.4163,
      "step": 3810
    },
    {
      "epoch": 1.6352739726027399,
      "grad_norm": 8.55812931060791,
      "learning_rate": 2.0190924657534244e-05,
      "loss": 0.6263,
      "step": 3820
    },
    {
      "epoch": 1.639554794520548,
      "grad_norm": 12.189432144165039,
      "learning_rate": 2.0165239726027397e-05,
      "loss": 0.5857,
      "step": 3830
    },
    {
      "epoch": 1.643835616438356,
      "grad_norm": 21.914569854736328,
      "learning_rate": 2.0139554794520547e-05,
      "loss": 0.6283,
      "step": 3840
    },
    {
      "epoch": 1.6481164383561644,
      "grad_norm": 6.071353912353516,
      "learning_rate": 2.01138698630137e-05,
      "loss": 0.5488,
      "step": 3850
    },
    {
      "epoch": 1.6523972602739727,
      "grad_norm": 2.474181652069092,
      "learning_rate": 2.008818493150685e-05,
      "loss": 0.7891,
      "step": 3860
    },
    {
      "epoch": 1.6566780821917808,
      "grad_norm": 5.496734142303467,
      "learning_rate": 2.00625e-05,
      "loss": 0.4428,
      "step": 3870
    },
    {
      "epoch": 1.660958904109589,
      "grad_norm": 5.611483573913574,
      "learning_rate": 2.003681506849315e-05,
      "loss": 0.5238,
      "step": 3880
    },
    {
      "epoch": 1.6652397260273972,
      "grad_norm": 18.853336334228516,
      "learning_rate": 2.0011130136986303e-05,
      "loss": 0.5035,
      "step": 3890
    },
    {
      "epoch": 1.6695205479452055,
      "grad_norm": 12.679913520812988,
      "learning_rate": 1.9985445205479453e-05,
      "loss": 0.4014,
      "step": 3900
    },
    {
      "epoch": 1.6738013698630136,
      "grad_norm": 7.984349250793457,
      "learning_rate": 1.9959760273972606e-05,
      "loss": 0.2865,
      "step": 3910
    },
    {
      "epoch": 1.678082191780822,
      "grad_norm": 27.35059356689453,
      "learning_rate": 1.9934075342465752e-05,
      "loss": 0.6572,
      "step": 3920
    },
    {
      "epoch": 1.6823630136986303,
      "grad_norm": 17.193315505981445,
      "learning_rate": 1.9908390410958905e-05,
      "loss": 0.4457,
      "step": 3930
    },
    {
      "epoch": 1.6866438356164384,
      "grad_norm": 17.416433334350586,
      "learning_rate": 1.9882705479452055e-05,
      "loss": 0.7941,
      "step": 3940
    },
    {
      "epoch": 1.6909246575342465,
      "grad_norm": 16.681852340698242,
      "learning_rate": 1.9857020547945208e-05,
      "loss": 0.412,
      "step": 3950
    },
    {
      "epoch": 1.6952054794520548,
      "grad_norm": 3.40037202835083,
      "learning_rate": 1.9831335616438355e-05,
      "loss": 0.4159,
      "step": 3960
    },
    {
      "epoch": 1.6994863013698631,
      "grad_norm": 5.397609233856201,
      "learning_rate": 1.9805650684931508e-05,
      "loss": 0.4583,
      "step": 3970
    },
    {
      "epoch": 1.7037671232876712,
      "grad_norm": 28.7585391998291,
      "learning_rate": 1.9779965753424658e-05,
      "loss": 0.6251,
      "step": 3980
    },
    {
      "epoch": 1.7080479452054793,
      "grad_norm": 16.76787567138672,
      "learning_rate": 1.975428082191781e-05,
      "loss": 0.4039,
      "step": 3990
    },
    {
      "epoch": 1.7123287671232876,
      "grad_norm": 18.519248962402344,
      "learning_rate": 1.972859589041096e-05,
      "loss": 0.5611,
      "step": 4000
    },
    {
      "epoch": 1.716609589041096,
      "grad_norm": 3.6796257495880127,
      "learning_rate": 1.970291095890411e-05,
      "loss": 0.61,
      "step": 4010
    },
    {
      "epoch": 1.720890410958904,
      "grad_norm": 2.5599374771118164,
      "learning_rate": 1.967722602739726e-05,
      "loss": 0.51,
      "step": 4020
    },
    {
      "epoch": 1.7251712328767124,
      "grad_norm": 4.009096622467041,
      "learning_rate": 1.9651541095890413e-05,
      "loss": 0.5236,
      "step": 4030
    },
    {
      "epoch": 1.7294520547945207,
      "grad_norm": 61.896385192871094,
      "learning_rate": 1.9625856164383563e-05,
      "loss": 0.4505,
      "step": 4040
    },
    {
      "epoch": 1.7337328767123288,
      "grad_norm": 1.6276532411575317,
      "learning_rate": 1.9600171232876713e-05,
      "loss": 0.3222,
      "step": 4050
    },
    {
      "epoch": 1.7380136986301369,
      "grad_norm": 23.2451114654541,
      "learning_rate": 1.9574486301369863e-05,
      "loss": 0.5861,
      "step": 4060
    },
    {
      "epoch": 1.7422945205479452,
      "grad_norm": 19.793611526489258,
      "learning_rate": 1.9548801369863016e-05,
      "loss": 0.4213,
      "step": 4070
    },
    {
      "epoch": 1.7465753424657535,
      "grad_norm": 24.211828231811523,
      "learning_rate": 1.9523116438356166e-05,
      "loss": 0.6524,
      "step": 4080
    },
    {
      "epoch": 1.7508561643835616,
      "grad_norm": 76.02991485595703,
      "learning_rate": 1.9497431506849315e-05,
      "loss": 0.4516,
      "step": 4090
    },
    {
      "epoch": 1.7551369863013697,
      "grad_norm": 31.41939353942871,
      "learning_rate": 1.9471746575342465e-05,
      "loss": 0.7432,
      "step": 4100
    },
    {
      "epoch": 1.759417808219178,
      "grad_norm": 19.59832000732422,
      "learning_rate": 1.9446061643835615e-05,
      "loss": 0.3654,
      "step": 4110
    },
    {
      "epoch": 1.7636986301369864,
      "grad_norm": 5.274127006530762,
      "learning_rate": 1.9420376712328768e-05,
      "loss": 0.5074,
      "step": 4120
    },
    {
      "epoch": 1.7679794520547945,
      "grad_norm": 14.360557556152344,
      "learning_rate": 1.9394691780821918e-05,
      "loss": 0.3533,
      "step": 4130
    },
    {
      "epoch": 1.7722602739726028,
      "grad_norm": 31.25728416442871,
      "learning_rate": 1.9369006849315068e-05,
      "loss": 0.7591,
      "step": 4140
    },
    {
      "epoch": 1.776541095890411,
      "grad_norm": 13.008493423461914,
      "learning_rate": 1.9343321917808217e-05,
      "loss": 0.6009,
      "step": 4150
    },
    {
      "epoch": 1.7808219178082192,
      "grad_norm": 11.199233055114746,
      "learning_rate": 1.931763698630137e-05,
      "loss": 0.4796,
      "step": 4160
    },
    {
      "epoch": 1.7851027397260273,
      "grad_norm": 7.007798671722412,
      "learning_rate": 1.929195205479452e-05,
      "loss": 0.359,
      "step": 4170
    },
    {
      "epoch": 1.7893835616438356,
      "grad_norm": 7.08510160446167,
      "learning_rate": 1.9266267123287674e-05,
      "loss": 0.3583,
      "step": 4180
    },
    {
      "epoch": 1.793664383561644,
      "grad_norm": 22.454998016357422,
      "learning_rate": 1.924058219178082e-05,
      "loss": 0.663,
      "step": 4190
    },
    {
      "epoch": 1.797945205479452,
      "grad_norm": 1.7827061414718628,
      "learning_rate": 1.9214897260273973e-05,
      "loss": 0.581,
      "step": 4200
    },
    {
      "epoch": 1.8022260273972601,
      "grad_norm": 9.737443923950195,
      "learning_rate": 1.9189212328767123e-05,
      "loss": 0.5445,
      "step": 4210
    },
    {
      "epoch": 1.8065068493150684,
      "grad_norm": 18.056055068969727,
      "learning_rate": 1.9163527397260276e-05,
      "loss": 0.3796,
      "step": 4220
    },
    {
      "epoch": 1.8107876712328768,
      "grad_norm": 22.148479461669922,
      "learning_rate": 1.9137842465753426e-05,
      "loss": 0.4609,
      "step": 4230
    },
    {
      "epoch": 1.8150684931506849,
      "grad_norm": 6.404452323913574,
      "learning_rate": 1.9112157534246576e-05,
      "loss": 0.3963,
      "step": 4240
    },
    {
      "epoch": 1.8193493150684932,
      "grad_norm": 17.864734649658203,
      "learning_rate": 1.9086472602739725e-05,
      "loss": 0.4972,
      "step": 4250
    },
    {
      "epoch": 1.8236301369863015,
      "grad_norm": 14.38877010345459,
      "learning_rate": 1.906078767123288e-05,
      "loss": 0.5134,
      "step": 4260
    },
    {
      "epoch": 1.8279109589041096,
      "grad_norm": 12.86507797241211,
      "learning_rate": 1.903510273972603e-05,
      "loss": 0.267,
      "step": 4270
    },
    {
      "epoch": 1.8321917808219177,
      "grad_norm": 30.7558536529541,
      "learning_rate": 1.9009417808219178e-05,
      "loss": 0.3332,
      "step": 4280
    },
    {
      "epoch": 1.836472602739726,
      "grad_norm": 46.09252166748047,
      "learning_rate": 1.8983732876712328e-05,
      "loss": 0.6937,
      "step": 4290
    },
    {
      "epoch": 1.8407534246575343,
      "grad_norm": 20.406261444091797,
      "learning_rate": 1.895804794520548e-05,
      "loss": 0.3059,
      "step": 4300
    },
    {
      "epoch": 1.8450342465753424,
      "grad_norm": 2.7680084705352783,
      "learning_rate": 1.893236301369863e-05,
      "loss": 0.5121,
      "step": 4310
    },
    {
      "epoch": 1.8493150684931505,
      "grad_norm": 12.978144645690918,
      "learning_rate": 1.8906678082191784e-05,
      "loss": 0.5167,
      "step": 4320
    },
    {
      "epoch": 1.853595890410959,
      "grad_norm": 12.749186515808105,
      "learning_rate": 1.888099315068493e-05,
      "loss": 0.4093,
      "step": 4330
    },
    {
      "epoch": 1.8578767123287672,
      "grad_norm": 4.961120128631592,
      "learning_rate": 1.8855308219178084e-05,
      "loss": 0.6228,
      "step": 4340
    },
    {
      "epoch": 1.8621575342465753,
      "grad_norm": 11.497303009033203,
      "learning_rate": 1.8829623287671233e-05,
      "loss": 0.4085,
      "step": 4350
    },
    {
      "epoch": 1.8664383561643836,
      "grad_norm": 9.498723030090332,
      "learning_rate": 1.8803938356164387e-05,
      "loss": 0.6306,
      "step": 4360
    },
    {
      "epoch": 1.870719178082192,
      "grad_norm": 18.589962005615234,
      "learning_rate": 1.8778253424657533e-05,
      "loss": 0.5089,
      "step": 4370
    },
    {
      "epoch": 1.875,
      "grad_norm": 13.742088317871094,
      "learning_rate": 1.8752568493150686e-05,
      "loss": 0.4835,
      "step": 4380
    },
    {
      "epoch": 1.879280821917808,
      "grad_norm": 13.139420509338379,
      "learning_rate": 1.8726883561643836e-05,
      "loss": 0.5298,
      "step": 4390
    },
    {
      "epoch": 1.8835616438356164,
      "grad_norm": 12.52042007446289,
      "learning_rate": 1.870119863013699e-05,
      "loss": 0.4707,
      "step": 4400
    },
    {
      "epoch": 1.8878424657534247,
      "grad_norm": 46.593074798583984,
      "learning_rate": 1.867551369863014e-05,
      "loss": 0.6026,
      "step": 4410
    },
    {
      "epoch": 1.8921232876712328,
      "grad_norm": 20.09745979309082,
      "learning_rate": 1.864982876712329e-05,
      "loss": 0.4009,
      "step": 4420
    },
    {
      "epoch": 1.896404109589041,
      "grad_norm": 21.251699447631836,
      "learning_rate": 1.862414383561644e-05,
      "loss": 0.5198,
      "step": 4430
    },
    {
      "epoch": 1.9006849315068495,
      "grad_norm": 36.72938537597656,
      "learning_rate": 1.8598458904109588e-05,
      "loss": 0.5082,
      "step": 4440
    },
    {
      "epoch": 1.9049657534246576,
      "grad_norm": 7.994814872741699,
      "learning_rate": 1.857277397260274e-05,
      "loss": 0.421,
      "step": 4450
    },
    {
      "epoch": 1.9092465753424657,
      "grad_norm": 7.301170825958252,
      "learning_rate": 1.8547089041095888e-05,
      "loss": 0.6448,
      "step": 4460
    },
    {
      "epoch": 1.913527397260274,
      "grad_norm": 6.295528888702393,
      "learning_rate": 1.852140410958904e-05,
      "loss": 0.4079,
      "step": 4470
    },
    {
      "epoch": 1.9178082191780823,
      "grad_norm": 25.070934295654297,
      "learning_rate": 1.849571917808219e-05,
      "loss": 0.6458,
      "step": 4480
    },
    {
      "epoch": 1.9220890410958904,
      "grad_norm": 15.986374855041504,
      "learning_rate": 1.8470034246575344e-05,
      "loss": 0.5191,
      "step": 4490
    },
    {
      "epoch": 1.9263698630136985,
      "grad_norm": 10.834296226501465,
      "learning_rate": 1.8444349315068494e-05,
      "loss": 0.4371,
      "step": 4500
    },
    {
      "epoch": 1.9306506849315068,
      "grad_norm": 44.80891036987305,
      "learning_rate": 1.8418664383561643e-05,
      "loss": 0.5873,
      "step": 4510
    },
    {
      "epoch": 1.9349315068493151,
      "grad_norm": 11.328543663024902,
      "learning_rate": 1.8392979452054793e-05,
      "loss": 0.4742,
      "step": 4520
    },
    {
      "epoch": 1.9392123287671232,
      "grad_norm": 2.0611612796783447,
      "learning_rate": 1.8367294520547946e-05,
      "loss": 0.57,
      "step": 4530
    },
    {
      "epoch": 1.9434931506849316,
      "grad_norm": 8.943866729736328,
      "learning_rate": 1.8341609589041096e-05,
      "loss": 0.5446,
      "step": 4540
    },
    {
      "epoch": 1.9477739726027399,
      "grad_norm": 21.342111587524414,
      "learning_rate": 1.8315924657534246e-05,
      "loss": 0.6471,
      "step": 4550
    },
    {
      "epoch": 1.952054794520548,
      "grad_norm": 6.3900604248046875,
      "learning_rate": 1.8290239726027396e-05,
      "loss": 0.6117,
      "step": 4560
    },
    {
      "epoch": 1.956335616438356,
      "grad_norm": 6.169938087463379,
      "learning_rate": 1.826455479452055e-05,
      "loss": 0.5876,
      "step": 4570
    },
    {
      "epoch": 1.9606164383561644,
      "grad_norm": 10.066176414489746,
      "learning_rate": 1.82388698630137e-05,
      "loss": 0.3932,
      "step": 4580
    },
    {
      "epoch": 1.9648972602739727,
      "grad_norm": 41.185943603515625,
      "learning_rate": 1.8213184931506852e-05,
      "loss": 0.518,
      "step": 4590
    },
    {
      "epoch": 1.9691780821917808,
      "grad_norm": 21.49726104736328,
      "learning_rate": 1.8187499999999998e-05,
      "loss": 0.4224,
      "step": 4600
    },
    {
      "epoch": 1.973458904109589,
      "grad_norm": 5.734244346618652,
      "learning_rate": 1.816181506849315e-05,
      "loss": 0.3754,
      "step": 4610
    },
    {
      "epoch": 1.9777397260273972,
      "grad_norm": 37.46115493774414,
      "learning_rate": 1.81361301369863e-05,
      "loss": 0.5325,
      "step": 4620
    },
    {
      "epoch": 1.9820205479452055,
      "grad_norm": 28.129777908325195,
      "learning_rate": 1.8110445205479454e-05,
      "loss": 0.5172,
      "step": 4630
    },
    {
      "epoch": 1.9863013698630136,
      "grad_norm": 13.365239143371582,
      "learning_rate": 1.8084760273972604e-05,
      "loss": 0.3772,
      "step": 4640
    },
    {
      "epoch": 1.990582191780822,
      "grad_norm": 8.639898300170898,
      "learning_rate": 1.8059075342465754e-05,
      "loss": 0.3761,
      "step": 4650
    },
    {
      "epoch": 1.9948630136986303,
      "grad_norm": 13.050405502319336,
      "learning_rate": 1.8033390410958904e-05,
      "loss": 0.3278,
      "step": 4660
    },
    {
      "epoch": 1.9991438356164384,
      "grad_norm": 21.96680450439453,
      "learning_rate": 1.8007705479452057e-05,
      "loss": 0.5551,
      "step": 4670
    },
    {
      "epoch": 2.0,
      "eval_f1": 0.6594364809181281,
      "eval_loss": 0.7407808303833008,
      "eval_precision": 0.690955407939604,
      "eval_recall": 0.6671832623014335,
      "eval_runtime": 539.0773,
      "eval_samples_per_second": 8.292,
      "eval_steps_per_second": 1.037,
      "step": 4672
    }
  ],
  "logging_steps": 10,
  "max_steps": 11680,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 206924744598528.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
