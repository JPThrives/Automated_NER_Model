{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86268ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, AutoConfig\n",
    "from seqeval.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8af207c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- Step 1: Read CoNLL Data -----------------\n",
    "def read_conll(filename):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        words = []\n",
    "        tags = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('*'):\n",
    "                if words:\n",
    "                    sentences.append(words)\n",
    "                    labels.append(tags)\n",
    "                    words = []\n",
    "                    tags = []\n",
    "                continue\n",
    "            splits = line.split('\\t')\n",
    "            if len(splits) >= 2:\n",
    "                words.append(splits[0])\n",
    "                tags.append(splits[1])\n",
    "        if words:\n",
    "            sentences.append(words)\n",
    "            labels.append(tags)\n",
    "    return sentences, labels\n",
    "\n",
    "sentences, tags = read_conll('Test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3349e7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- Step 2: Label Mapping -----------------\n",
    "unique_tags = sorted(set(tag for seq in tags for tag in seq))\n",
    "tag2id = {tag: i for i, tag in enumerate(unique_tags)}\n",
    "id2tag = {i: tag for tag, i in tag2id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "445c1906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- Step 3: Train/Test Split --------------\n",
    "train_sents, test_sents, train_tags, test_tags = train_test_split(\n",
    "    sentences, tags, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c354301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- Step 4: Tokenization & Alignment ------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\n",
    "\n",
    "def tokenize_and_align_labels(sentences, tags, tokenizer, tag2id, max_length=128):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        sentences,\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(tags):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        prev_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != prev_word_idx:\n",
    "                label_ids.append(tag2id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            prev_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    return tokenized_inputs, labels\n",
    "\n",
    "train_inputs, train_labels = tokenize_and_align_labels(train_sents, train_tags, tokenizer, tag2id)\n",
    "test_inputs, test_labels = tokenize_and_align_labels(test_sents, test_tags, tokenizer, tag2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4727fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- Step 5: Dataset Class -----------------\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = NERDataset(train_inputs, train_labels)\n",
    "test_dataset = NERDataset(test_inputs, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3f680e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForTokenClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ------------- Step 6: Model Setup -------------------\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"ai4bharat/indic-bert\",\n",
    "    num_labels=len(tag2id),\n",
    "    id2label=id2tag,\n",
    "    label2id=tag2id\n",
    ")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"ai4bharat/indic-bert\",\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aba427ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1788' max='1788' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1788/1788 2:06:22, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.291400</td>\n",
       "      <td>1.017325</td>\n",
       "      <td>0.608474</td>\n",
       "      <td>0.620598</td>\n",
       "      <td>0.604351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.630900</td>\n",
       "      <td>0.866955</td>\n",
       "      <td>0.652309</td>\n",
       "      <td>0.692359</td>\n",
       "      <td>0.660246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.625100</td>\n",
       "      <td>0.766697</td>\n",
       "      <td>0.687990</td>\n",
       "      <td>0.720266</td>\n",
       "      <td>0.697459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.638000</td>\n",
       "      <td>0.744875</td>\n",
       "      <td>0.702543</td>\n",
       "      <td>0.726246</td>\n",
       "      <td>0.712793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jyoti\\repos\\Automated_NER_Model\\venv\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\jyoti\\repos\\Automated_NER_Model\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\jyoti\\repos\\Automated_NER_Model\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\jyoti\\repos\\Automated_NER_Model\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1788, training_loss=0.8832456456201455, metrics={'train_runtime': 7587.1343, 'train_samples_per_second': 1.885, 'train_steps_per_second': 0.236, 'total_flos': 79140636573696.0, 'train_loss': 0.8832456456201455, 'epoch': 4.0})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------- Step 7: Training Arguments -------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./assamese-ner-model',\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\"\n",
    ")\n",
    "\n",
    "# ------------- Step 8: Compute Metrics ----------------\n",
    "import numpy as np\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    true_labels = []\n",
    "    true_preds = []\n",
    "    \n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        valid_labels = []\n",
    "        valid_preds = []\n",
    "        \n",
    "        for p, l in zip(prediction, label):\n",
    "            if l != -100:\n",
    "                # Handle invalid label IDs\n",
    "                true_tag = id2tag.get(l, 'O')\n",
    "                pred_tag = id2tag.get(p, 'O')\n",
    "                \n",
    "                # Validate IOB format\n",
    "                if len(true_tag) < 2 or true_tag[1] != '-':\n",
    "                    true_tag = 'O'\n",
    "                if len(pred_tag) < 2 or pred_tag[1] != '-':\n",
    "                    pred_tag = 'O'\n",
    "                \n",
    "                valid_labels.append(true_tag)\n",
    "                valid_preds.append(pred_tag)\n",
    "        \n",
    "        # Skip empty sequences\n",
    "        if valid_labels:\n",
    "            true_labels.append(valid_labels)\n",
    "            true_preds.append(valid_preds)\n",
    "    \n",
    "    if not true_labels:  # Handle all-empty case\n",
    "        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "    \n",
    "    report = classification_report(true_labels, true_preds, output_dict=True)\n",
    "    return {\n",
    "        \"precision\": report[\"weighted avg\"][\"precision\"],\n",
    "        \"recall\": report[\"weighted avg\"][\"recall\"],\n",
    "        \"f1\": report[\"weighted avg\"][\"f1-score\"]\n",
    "    }\n",
    "\n",
    "# ------------- Step 9: Trainer -----------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# ------------- Step 10: Train! -----------------------\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d739f7c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./assamese-ner-model\\\\tokenizer_config.json',\n",
       " './assamese-ner-model\\\\special_tokens_map.json',\n",
       " './assamese-ner-model\\\\spiece.model',\n",
       " './assamese-ner-model\\\\added_tokens.json',\n",
       " './assamese-ner-model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------- Step 11: Save Model -------------------\n",
    "model.save_pretrained('./assamese-ner-model')\n",
    "tokenizer.save_pretrained('./assamese-ner-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76c4808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- Step 12: Inference Function -----------\n",
    "def ner_predict(text):\n",
    "    words = text.strip().split()\n",
    "    inputs = tokenizer(\n",
    "        words,\n",
    "        is_split_into_words=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "    preds = torch.argmax(outputs.logits, dim=2).squeeze().tolist()\n",
    "    \n",
    "    # Align predictions with original words\n",
    "    word_ids = inputs.word_ids()\n",
    "    tags = []\n",
    "    for i, word_idx in enumerate(word_ids):\n",
    "        if word_idx is not None and word_idx != word_ids[i-1]:\n",
    "            tags.append(id2tag[preds[i]])\n",
    "    \n",
    "    return list(zip(words, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "243c0412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('কটক', 'B-LOC'), ('চহৰৰ', 'I-LOC'), ('মিছনেৰী', 'B-ORG'), ('স্কুলতেই', 'B-WOA'), ('সুভাষে', 'I-WOA'), ('প্রথম', 'B-NUM'), ('শিক্ষা', 'B-WOA'), ('লাভ', 'I-WOA'), ('কৰে।', 'I-WOA'), ('তীক্ষ্ণ', 'B-WOA'), ('প্রতিভাসম্পন্ন', 'I-WOA'), ('সুভাষে', 'I-WOA'), ('“ৰেভেঞ্চ”', 'B-WOA'), ('কলেজিয়েট', 'B-WOA'), ('স্কুলৰপৰা', 'I-WOA'), ('১৯১৩', 'B-DATE'), ('চনত', 'I-DATE'), ('পাটনা', 'B-ORG'), ('বিশ্ববিদ্যালয়ৰ', 'I-ORG'), ('ভিতৰত', 'B-WOA'), ('দ্বিতীয়', 'B-WOA'), ('স্থান', 'B-WOA'), ('লাভ', 'B-WOA'), ('কৰি', 'I-WOA'), ('পৰীক্ষাত', 'B-WOA'), ('উত্তীর্ণ', 'I-WOA'), ('হয়।', 'I-WOA'), ('তাৰ', 'B-WOA'), ('পাছত', 'I-WOA'), ('কলকাতালৈ', 'B-WOA'), ('আহি', 'I-WOA'), ('প্ৰেছিডেন্সী', 'B-WOA'), ('কলেজত', 'B-WOA'), ('ভৰ্ত্তি', 'I-WOA'), ('হয়।', 'I-WOA'), ('সেই', 'B-WOA'), ('সময়তে', 'I-WOA'), ('শ্ৰীৰামকৃষ্ণ', 'I-WOA'), ('পৰমহংস', 'B-WOA'), ('আৰু', 'I-WOA'), ('স্বামী', 'I-WOA'), ('বিবেকানন্দৰ', 'I-WOA'), ('প্ৰেৰণাত', 'I-WOA'), ('সংসাৰৰ', 'I-WOA'), ('মায়া-মোহ', 'B-WOA'), ('ত্যাগ', 'B-WOA'), ('কৰি', 'I-WOA')]\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "example = \"কটক চহৰৰ মিছনেৰী স্কুলতেই সুভাষে প্রথম শিক্ষা লাভ কৰে। তীক্ষ্ণ প্রতিভাসম্পন্ন সুভাষে “ৰেভেঞ্চ” কলেজিয়েট স্কুলৰপৰা ১৯১৩ চনত পাটনা বিশ্ববিদ্যালয়ৰ ভিতৰত দ্বিতীয় স্থান লাভ কৰি পৰীক্ষাত উত্তীর্ণ হয়। তাৰ পাছত কলকাতালৈ আহি প্ৰেছিডেন্সী কলেজত ভৰ্ত্তি হয়। সেই সময়তে শ্ৰীৰামকৃষ্ণ পৰমহংস আৰু স্বামী বিবেকানন্দৰ প্ৰেৰণাত সংসাৰৰ মায়া-মোহ ত্যাগ কৰি সন্ন্যাসী জীৱন যাপনৰ অভিলাষেৰে হিমালয়লৈ গৈ গভীৰ আত্ম-সাধনাত ৰত হয়। উপযুক্ত গুৰুৰ সন্ধানত তেওঁ কিছুদিন ভাৰতৰ নানা তীৰ্থ ভ্ৰমণ কৰে। কিন্তু মুক্তি-পথৰ সন্ধান থাকিল বহুত দূৰত। সেয়েহে তেওঁ ফিৰি আহি পুনৰ প্ৰেছিডেন্সি কলেজত ভৰ্ত্তি হ’লহি। ক’বলৈ গ’লে এই প্রেছিডেন্সি কলেজেই তেওঁৰ অন্তৰত ৰূপ খাই থকা দেশাত্মবোধ আৰু বিপ্লৱী ভাৱধাৰাৰ আগ্নেয়গিৰিটো উদগীৰণত বৰঙণি যোগায়।\"\n",
    "print(ner_predict(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "07d8eb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('কুমাৰ', 'B-LOC'), ('গাম্বাৰ', 'I-ORG'), ('সঙ্গল', 'I-ORG'), ('মহিধৰ', 'I-ORG')]\n"
     ]
    }
   ],
   "source": [
    "text = \"কুমাৰ গাম্বাৰ সঙ্গল মহিধৰ\"\n",
    "print(ner_predict(text)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
